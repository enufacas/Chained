#!/usr/bin/env python3
"""
Autonomous Experiment Creator for A/B Testing

This module automatically creates and manages A/B testing experiments based
on workflow analysis and system insights.

Author: @workflows-tech-lead
Safety-first approach with rollback mechanisms and monitoring
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone

# Add tools to path
sys.path.insert(0, str(Path(__file__).parent))
from ab_testing_engine import ABTestingEngine
from ab_testing_workflow_analyzer import WorkflowAnalyzer


class AutonomousExperimentCreator:
    """
    Autonomously creates and manages A/B testing experiments.
    
    Features:
    - Intelligent opportunity detection
    - Automatic experiment creation
    - Safety guards and rollback mechanisms
    - Priority-based scheduling
    - Conflict detection
    """
    
    def __init__(
        self, 
        max_concurrent_experiments: int = 5,
        high_priority_limit: int = 2
    ):
        """
        Initialize the autonomous creator.
        
        Args:
            max_concurrent_experiments: Maximum number of active experiments
            high_priority_limit: Maximum number of high-priority experiments
        """
        self.engine = ABTestingEngine()
        self.analyzer = WorkflowAnalyzer()
        self.max_concurrent = max_concurrent_experiments
        self.high_priority_limit = high_priority_limit
    
    def run_autonomous_cycle(self) -> Dict[str, Any]:
        """
        Run a complete autonomous A/B testing cycle.
        
        Steps:
        1. Analyze workflows for opportunities
        2. Check current experiment load
        3. Create new experiments if capacity available
        4. Analyze existing experiments
        5. Auto-complete experiments with clear winners
        
        Returns:
            Summary of actions taken
        """
        results = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "opportunities_found": 0,
            "experiments_created": [],
            "experiments_analyzed": [],
            "winners_detected": [],
            "experiments_completed": []
        }
        
        # Step 1: Analyze workflows for opportunities
        print("üîç Step 1: Analyzing workflows for optimization opportunities...")
        opportunities = self.analyzer.analyze_all_workflows()
        results["opportunities_found"] = len(opportunities)
        print(f"   Found {len(opportunities)} opportunities")
        
        # Step 2: Check current experiment load
        print("\nüìä Step 2: Checking current experiment load...")
        active_experiments = self.engine.list_experiments(status="active")
        active_count = len(active_experiments)
        print(f"   Active experiments: {active_count}/{self.max_concurrent}")
        
        # Step 3: Create new experiments if capacity available
        if active_count < self.max_concurrent:
            print("\nüÜï Step 3: Creating new experiments...")
            available_slots = self.max_concurrent - active_count
            
            # Filter and prioritize opportunities
            viable_opportunities = self._filter_opportunities(
                opportunities, 
                active_experiments
            )
            
            # Create experiments for top opportunities
            created = self._create_experiments(
                viable_opportunities[:available_slots]
            )
            results["experiments_created"] = created
            print(f"   Created {len(created)} new experiments")
        else:
            print("\n‚ö†Ô∏è  Step 3: Skipped - At maximum concurrent experiment limit")
        
        # Step 4: Analyze existing experiments
        print("\nüìà Step 4: Analyzing active experiments...")
        for exp in active_experiments:
            try:
                analysis = self.engine.analyze_experiment(exp["id"])
                results["experiments_analyzed"].append({
                    "id": exp["id"],
                    "name": exp["name"],
                    "status": analysis["status"]
                })
                
                if analysis["status"] == "analyzed" and analysis.get("winner"):
                    results["winners_detected"].append({
                        "id": exp["id"],
                        "name": exp["name"],
                        "winner": analysis["winner"]["variant"],
                        "improvement": analysis["winner"]["improvement"]
                    })
                    print(f"   üèÜ Winner detected: {exp['name']} -> {analysis['winner']['variant']}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error analyzing {exp['id']}: {e}")
        
        # Step 5: Auto-complete experiments with clear winners
        print("\n‚úÖ Step 5: Auto-completing experiments with winners...")
        for winner in results["winners_detected"]:
            try:
                # Apply safety checks before completing
                if self._is_safe_to_complete(winner):
                    self.engine.complete_experiment(
                        experiment_id=winner["id"],
                        winner=winner["winner"],
                        notes=f"Auto-completed by autonomous system. Winner showed {winner['improvement']:.2%} improvement."
                    )
                    results["experiments_completed"].append(winner["id"])
                    print(f"   ‚úÖ Completed: {winner['name']}")
                else:
                    print(f"   ‚ö†Ô∏è  Safety check failed for {winner['name']}, keeping active")
            except Exception as e:
                print(f"   ‚ùå Error completing {winner['id']}: {e}")
        
        # Summary
        print("\n" + "="*60)
        print("üìã Autonomous Cycle Summary")
        print("="*60)
        print(f"Opportunities Found: {results['opportunities_found']}")
        print(f"Experiments Created: {len(results['experiments_created'])}")
        print(f"Experiments Analyzed: {len(results['experiments_analyzed'])}")
        print(f"Winners Detected: {len(results['winners_detected'])}")
        print(f"Experiments Completed: {len(results['experiments_completed'])}")
        
        return results
    
    def _filter_opportunities(
        self, 
        opportunities: List[Dict[str, Any]],
        active_experiments: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Filter opportunities to avoid conflicts with active experiments.
        
        Args:
            opportunities: List of optimization opportunities
            active_experiments: List of currently active experiments
        
        Returns:
            Filtered and prioritized opportunities
        """
        # Get workflows that already have active experiments
        active_workflows = set(
            exp.get("workflow_name") 
            for exp in active_experiments 
            if exp.get("workflow_name")
        )
        
        # Filter out workflows with existing experiments
        viable = [
            opp for opp in opportunities
            if opp["workflow"] not in active_workflows
        ]
        
        # Sort by priority (high first)
        priority_order = {"high": 0, "medium": 1, "low": 2}
        viable.sort(key=lambda x: priority_order.get(x.get("priority", "medium"), 1))
        
        return viable
    
    def _create_experiments(
        self, 
        opportunities: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Create experiments from opportunities.
        
        Args:
            opportunities: List of opportunities to create experiments for
        
        Returns:
            List of created experiment details
        """
        created = []
        
        for opp in opportunities:
            try:
                # Generate experiment proposal
                proposal = self.analyzer.generate_experiment_proposal(opp)
                
                # Create the experiment
                exp_id = self.engine.create_experiment(
                    name=proposal["name"],
                    description=proposal["description"],
                    variants=proposal["variants"],
                    metrics=proposal["metrics"],
                    workflow_name=proposal["workflow_name"]
                )
                
                created.append({
                    "id": exp_id,
                    "name": proposal["name"],
                    "workflow": proposal["workflow_name"],
                    "type": opp["type"],
                    "priority": opp.get("priority", "medium")
                })
                
                print(f"   ‚úÖ Created: {proposal['name']} ({exp_id})")
            
            except ValueError as e:
                print(f"   ‚ö†Ô∏è  Could not create experiment for {opp['workflow']}: {e}")
            except Exception as e:
                print(f"   ‚ùå Unexpected error creating experiment for {opp['workflow']}: {e}")
        
        return created
    
    def _is_safe_to_complete(self, winner: Dict[str, Any]) -> bool:
        """
        Apply safety checks before auto-completing an experiment.
        
        Args:
            winner: Winner information
        
        Returns:
            True if safe to complete, False otherwise
        """
        # Safety checks:
        # 1. Improvement must be positive and significant (> 5%)
        if winner.get("improvement", 0) < 0.05:
            return False
        
        # 2. Experiment must have been running for minimum duration
        # (This would require checking experiment creation time)
        # For now, we trust the analysis
        
        # 3. No critical failures should have occurred
        # (Would need integration with monitoring system)
        
        return True
    
    def generate_rollout_plan(self, experiment_id: str) -> Dict[str, Any]:
        """
        Generate a rollout plan for a completed experiment.
        
        Args:
            experiment_id: Experiment identifier
        
        Returns:
            Rollout plan with recommendations
        """
        details = self.engine.get_experiment_details(experiment_id)
        
        if not details:
            raise ValueError(f"Experiment {experiment_id} not found")
        
        if details["status"] != "completed":
            raise ValueError(f"Experiment {experiment_id} is not completed")
        
        results = details.get("results", {})
        if not results or "winner" not in results:
            raise ValueError(f"No winner found for experiment {experiment_id}")
        
        winner_variant = results["winner"]
        winner_config = details["variants"][winner_variant]["config"]
        
        return {
            "experiment_id": experiment_id,
            "experiment_name": details["name"],
            "workflow": details.get("workflow_name"),
            "winner": winner_variant,
            "config": winner_config,
            "improvement": results.get("improvement", 0),
            "confidence": results.get("confidence", "unknown"),
            "rollout_steps": self._generate_rollout_steps(
                details.get("workflow_name"),
                winner_config
            )
        }
    
    def _generate_rollout_steps(
        self, 
        workflow_name: Optional[str],
        config: Dict[str, Any]
    ) -> List[str]:
        """
        Generate step-by-step rollout instructions.
        
        Args:
            workflow_name: Name of the workflow
            config: Winning configuration
        
        Returns:
            List of rollout steps
        """
        steps = []
        
        if workflow_name:
            steps.append(f"1. Open `.github/workflows/{workflow_name}.yml`")
        else:
            steps.append("1. Identify the target workflow file")
        
        steps.append("2. Review the winning configuration:")
        for key, value in config.items():
            steps.append(f"   - {key}: {value}")
        
        steps.append("3. Update the workflow with the winning configuration")
        steps.append("4. Commit and push the changes")
        steps.append("5. Monitor the workflow for 24 hours after rollout")
        steps.append("6. Verify performance improvements match A/B test results")
        
        return steps


def main():
    """CLI entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Autonomous A/B Testing Experiment Creator"
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=5,
        help="Maximum concurrent experiments (default: 5)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Analyze opportunities without creating experiments"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output results as JSON"
    )
    
    args = parser.parse_args()
    
    creator = AutonomousExperimentCreator(
        max_concurrent_experiments=args.max_concurrent
    )
    
    if args.dry_run:
        print("üîç DRY RUN MODE - No experiments will be created\n")
        opportunities = creator.analyzer.analyze_all_workflows()
        print(f"Found {len(opportunities)} optimization opportunities\n")
        
        for opp in opportunities:
            print(f"‚Ä¢ {opp['workflow']} ({opp['type']})")
            print(f"  Priority: {opp.get('priority', 'medium')}")
            print(f"  Variants: {len(opp['suggested_variants'])}")
            print()
    else:
        results = creator.run_autonomous_cycle()
        
        if args.json:
            print("\n" + json.dumps(results, indent=2))
    
    return 0


if __name__ == "__main__":
    exit(main())
