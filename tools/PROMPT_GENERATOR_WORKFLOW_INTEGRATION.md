# Prompt Generator Workflow Integration Guide

This guide explains how to integrate the self-improving prompt generator into your GitHub Actions workflows for automated Copilot assignment and performance tracking.

**@workflows-tech-lead** has created comprehensive workflow integrations for the prompt generator system.

## Overview

The prompt generator can be integrated into workflows in two main ways:

1. **Issue Assignment Integration**: Generate optimized prompts when assigning issues to Copilot
2. **Performance Tracking**: Monitor and analyze prompt effectiveness over time

## Available Workflows

### 1. Prompt Generator Integration (`prompt-generator-integration.yml`)

**Purpose**: Generate optimized prompts for specific issues on-demand

**Trigger**: Manual dispatch

**Usage**:
```bash
# Via GitHub UI: Actions ‚Üí AI: Prompt Generator Integration ‚Üí Run workflow
# Or via CLI:
gh workflow run prompt-generator-integration.yml \
  -f issue_number=123 \
  -f agent=engineer-master \
  -f enable_learning=true
```

**What it does**:
1. Fetches issue details (title, body, labels)
2. Detects issue category automatically
3. Generates optimized prompt using the prompt generator
4. Posts generated prompt as a comment on the issue
5. Shows template performance statistics

**Example Output**:
```markdown
## ü§ñ Generated Optimized Prompt

**@engineer-master** - This prompt was generated by the self-improving prompt generator...

**Template Used:** bug_fix_systematic
**Category:** bug_fix
**Learning Enhancement:** true

---

**@engineer-master** - Please fix this bug using a systematic approach:
1. Analyze: Reproduce and understand the bug's root cause
...

**Recent Relevant Learnings:**
1. GPT-5.1 ü§ñ, Waymo hits highways üöó, Homebrew 5 üë®‚Äçüíª
...
```

### 2. Prompt Performance Tracker (`prompt-performance-tracker.yml`)

**Purpose**: Track and analyze prompt effectiveness continuously

**Trigger**: Daily schedule (2 AM UTC) or manual dispatch

**What it does**:
1. Refreshes learning insights from TLDR data
2. Generates comprehensive performance report
3. Identifies optimization opportunities
4. Analyzes learning integration effectiveness
5. Creates performance summary with trends
6. Commits performance history
7. Creates PR with updates if changes detected

**Example Report**:
```markdown
# ü§ñ Prompt Generator Performance Report

## üìä Overall Metrics
- Total Prompts Used: 50
- Success Rate: 78%
- Average Resolution Time: 4.5 hours
- Optimization Opportunities: 2 templates

## üéØ Performance Analysis

### Top Performing Templates
- bug_fix_systematic: 0.85 effectiveness (90% success, 20 uses)
- feature_rigorous: 0.82 effectiveness (85% success, 15 uses)
...

### üîß Optimization Opportunities
- refactor_basic: high_failure_rate - Address common error patterns: test_failures, build_errors
```

## Integration into Copilot Assignment Workflow

To integrate the prompt generator into the main Copilot assignment workflow (`copilot-graphql-assign.yml`), you can:

### Option 1: Direct Integration (Recommended)

Modify `tools/assign-copilot-to-issue.sh` to generate prompts during assignment:

```bash
# After agent matching (around line 200)
if [ -n "$matched_agent" ]; then
  echo "üéØ Generating optimized prompt for agent: $matched_agent"
  
  # Detect category from labels
  category="feature"  # default
  if echo "$issue_labels" | grep -qi "bug"; then
    category="bug_fix"
  elif echo "$issue_labels" | grep -qi "documentation"; then
    category="documentation"
  # ... add more category detection
  fi
  
  # Generate prompt
  prompt_output=$(python3 tools/prompt-generator.py generate \
    --issue-body "$issue_body_original" \
    --category "$category" \
    --agent "$matched_agent")
  
  template_id=$(echo "$prompt_output" | grep "Template ID:" | cut -d: -f2 | xargs)
  generated_prompt=$(echo "$prompt_output" | sed -n '/Generated Prompt:/,/^$/p' | tail -n +2)
  
  # Add generated prompt to agent directive
  agent_directive="<!-- COPILOT_AGENT:$matched_agent -->
<!-- PROMPT_TEMPLATE:$template_id -->

> **ü§ñ Agent Assignment**
> 
> This issue has been assigned to GitHub Copilot with the $agent_name_display custom agent profile.
> 
> $generated_prompt
> 
> **IMPORTANT**: Always mention **@$matched_agent** by name in all conversations, comments, and PRs related to this issue.

---

"
fi
```

### Option 2: Post-Assignment Enhancement

Add a workflow step after assignment that enhances the issue with a generated prompt:

```yaml
- name: Generate and add optimized prompt
  if: steps.assign.outputs.assigned == 'true'
  run: |
    # Generate prompt
    prompt=$(python3 tools/prompt-generator.py generate \
      --issue-body "${{ steps.issue.outputs.body }}" \
      --category "${{ steps.category.outputs.category }}" \
      --agent "${{ steps.agent.outputs.matched_agent }}")
    
    # Post as comment
    echo "$prompt" | gh issue comment ${{ github.event.issue.number }} --body-file -
```

## Recording Outcomes for Self-Improvement

To enable the self-improvement loop, record outcomes after issues are resolved:

### In PR Merge Workflows

When a PR is merged that closes an issue:

```yaml
- name: Record prompt outcome
  if: github.event.pull_request.merged == true
  run: |
    # Extract issue numbers from PR body
    issue_numbers=$(echo "${{ github.event.pull_request.body }}" | \
      grep -oP "(?:Closes|Fixes|Resolves) #\K\d+" | \
      head -5)
    
    for issue_num in $issue_numbers; do
      # Get issue details
      issue_data=$(gh issue view $issue_num --json createdAt,closedAt,labels,body)
      
      # Extract template ID from issue body
      template_id=$(echo "$issue_data" | \
        jq -r '.body' | \
        grep -oP '<!-- PROMPT_TEMPLATE:\K[^>]+')
      
      if [ -n "$template_id" ]; then
        # Calculate resolution time
        created=$(echo "$issue_data" | jq -r '.createdAt')
        closed=$(echo "$issue_data" | jq -r '.closedAt')
        resolution_hours=$(python3 -c "
from datetime import datetime
created = datetime.fromisoformat('${created}'.replace('Z', '+00:00'))
closed = datetime.fromisoformat('${closed}'.replace('Z', '+00:00'))
print((closed - created).total_seconds() / 3600)
        ")
        
        # Record successful outcome
        python3 tools/prompt-generator.py record \
          --prompt-id "$template_id" \
          --issue-number "$issue_num" \
          --success \
          --resolution-time "$resolution_hours" \
          --agent "${{ github.event.pull_request.user.login }}"
        
        echo "‚úì Recorded successful outcome for issue #$issue_num"
      fi
    done
```

### Recording Failures

When an issue is closed without a merged PR or marked as wontfix:

```yaml
- name: Record failed outcome
  if: github.event.issue.state_reason == 'not_planned'
  run: |
    # Extract template ID
    template_id=$(echo "${{ github.event.issue.body }}" | \
      grep -oP '<!-- PROMPT_TEMPLATE:\K[^>]+')
    
    if [ -n "$template_id" ]; then
      # Calculate time spent
      created=$(echo "${{ github.event.issue.created_at }}" | date -u -f - +%s)
      closed=$(date -u +%s)
      hours=$(( ($closed - $created) / 3600 ))
      
      # Record failure
      python3 tools/prompt-generator.py record \
        --prompt-id "$template_id" \
        --issue-number "${{ github.event.issue.number }}" \
        --resolution-time "$hours" \
        --error-type "not_planned"
      
      echo "‚úì Recorded outcome for closed issue #${{ github.event.issue.number }}"
    fi
```

## Best Practices

### 1. Category Detection

Always detect category accurately for best template selection:

```bash
detect_category() {
  local labels="$1"
  local title="$2"
  
  # Check labels first (most reliable)
  if echo "$labels" | grep -qi "bug"; then
    echo "bug_fix"
  elif echo "$labels" | grep -qi "security"; then
    echo "security"
  elif echo "$labels" | grep -qi "documentation"; then
    echo "documentation"
  elif echo "$labels" | grep -qi "refactor"; then
    echo "refactor"
  elif echo "$labels" | grep -qi "investigation"; then
    echo "investigation"
  # Fallback to title analysis
  elif echo "$title" | grep -qiE "fix|bug|error|issue"; then
    echo "bug_fix"
  elif echo "$title" | grep -qi "document"; then
    echo "documentation"
  else
    echo "feature"  # default
  fi
}
```

### 2. Learning Enhancement

Enable learning enhancement for all categories except time-sensitive ones:

```bash
# For most issues, enable learning
python3 tools/prompt-generator.py generate \
  --issue-body "$body" \
  --category "$category" \
  --agent "$agent" \
  # Learning enhancement is enabled by default

# For urgent issues, disable to skip learning lookup
python3 tools/prompt-generator.py generate \
  --issue-body "$body" \
  --category "$category" \
  --agent "$agent" \
  --learning-context []  # Explicitly disable
```

### 3. Template Evolution

Periodically evolve successful templates:

```bash
# In a monthly optimization workflow
templates=$(python3 tools/prompt-generator.py report | \
  jq -r '.templates | to_entries[] | 
    select(.value.total_uses >= 10 and .value.effectiveness_score > 0.7) | 
    .key')

for template in $templates; do
  # Evolve with enhancement mutation
  python3 tools/prompt-generator.py evolve \
    --template-id "$template" \
    --mutation enhance
  
  echo "‚úì Created enhanced version of $template"
done
```

### 4. A/B Testing

Set up A/B tests for template variations:

```bash
# Start A/B test between original and evolved template
python3 tools/prompt-generator.py ab-test \
  --template-a "bug_fix_systematic" \
  --template-b "bug_fix_systematic_enhance_v2"

# After 20+ uses each, check results
python3 tools/prompt-generator.py ab-test \
  --test-id "ab_bug_fix_systematic_vs_bug_fix_systematic_enhance_v2" | \
  jq -r 'if .results.winner == "b" then "Use evolved template" else "Keep original" end'
```

## Monitoring and Alerts

Set up alerts for prompt performance issues:

```yaml
- name: Check prompt performance
  run: |
    report=$(python3 tools/prompt-generator.py report)
    
    # Alert on low overall success rate
    success_rate=$(echo "$report" | jq -r '.insights.overall.success_rate // 0')
    if (( $(echo "$success_rate < 0.5" | bc -l) )); then
      echo "::warning::Prompt success rate below 50%: ${success_rate}"
      # Send notification or create issue
    fi
    
    # Alert on templates needing optimization
    suggestions=$(python3 tools/prompt-generator.py optimize)
    count=$(echo "$suggestions" | jq '. | length')
    if [ "$count" -gt 0 ]; then
      echo "::notice::$count templates need optimization"
    fi
```

## Troubleshooting

### Issue: No learning insights found

**Solution**: Run the learning refresh:
```bash
python3 tools/prompt-generator.py refresh-learnings --days 7
```

### Issue: Templates not improving

**Solution**: Ensure outcomes are being recorded consistently:
```bash
# Check outcome data
jq '.outcomes | length' tools/data/prompts/outcomes.json

# If count is low, outcomes aren't being recorded properly
# Review outcome recording workflow steps
```

### Issue: Category detection incorrect

**Solution**: Improve label consistency or add more detection rules:
```bash
# Add more comprehensive category detection
if echo "$labels" | grep -qE "type:bug|kind:bug|bug"; then
  category="bug_fix"
fi
```

## Example: Complete Integration

Here's a complete example of integrating prompt generation into issue assignment:

```yaml
name: "Enhanced Copilot Assignment with Prompts"

on:
  issues:
    types: [opened]

jobs:
  assign-with-prompt:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Match agent
        id: agent
        run: |
          # Use existing agent matching logic
          matched_agent=$(python3 tools/match-issue-to-agent.py \
            "${{ github.event.issue.title }}" \
            "${{ github.event.issue.body }}")
          echo "agent=$matched_agent" >> $GITHUB_OUTPUT
      
      - name: Detect category
        id: category
        run: |
          # Category detection logic
          category=$(detect_category "${{ github.event.issue.labels }}" "${{ github.event.issue.title }}")
          echo "category=$category" >> $GITHUB_OUTPUT
      
      - name: Generate prompt
        id: prompt
        run: |
          prompt_output=$(python3 tools/prompt-generator.py generate \
            --issue-body "${{ github.event.issue.body }}" \
            --category "${{ steps.category.outputs.category }}" \
            --agent "${{ steps.agent.outputs.agent }}")
          
          template_id=$(echo "$prompt_output" | grep "Template ID:" | cut -d: -f2 | xargs)
          echo "template_id=$template_id" >> $GITHUB_OUTPUT
          
          # Save full prompt
          echo "$prompt_output" > /tmp/prompt.txt
      
      - name: Update issue with prompt
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          prompt=$(cat /tmp/prompt.txt)
          
          # Update issue body to include prompt template ID
          current_body="${{ github.event.issue.body }}"
          new_body="<!-- PROMPT_TEMPLATE:${{ steps.prompt.outputs.template_id }} -->

$current_body"
          
          echo "$new_body" | gh issue edit ${{ github.event.issue.number }} --body-file -
          
          # Add prompt as comment
          gh issue comment ${{ github.event.issue.number }} --body "$prompt"
      
      - name: Assign to Copilot
        # ... existing assignment logic
```

---

**@workflows-tech-lead** - This integration enables the autonomous system to continuously improve its prompting effectiveness through real-world feedback and learning.
