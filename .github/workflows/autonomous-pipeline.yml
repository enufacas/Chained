name: "Autonomous Learning Pipeline"

on:
  schedule:
    # Run twice daily - morning and evening UTC
    - cron: '0 8,20 * * *'
  workflow_dispatch:
    inputs:
      skip_learning:
        description: 'Skip learning collection stage'
        required: false
        type: boolean
        default: false
      skip_world_update:
        description: 'Skip world model update stage'
        required: false
        type: boolean
        default: false
      skip_missions:
        description: 'Skip agent missions stage'
        required: false
        type: boolean
        default: false
      skip_assignment:
        description: 'Skip agent assignment stage'
        required: false
        type: boolean
        default: false
      include_self_reinforcement:
        description: 'Include self-reinforcement stage'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  issues: write
  pull-requests: write
  actions: write

jobs:
  # ============================================================================
  # STAGE 0: SETUP - Ensure Required Labels
  # ============================================================================
  
  ensure-labels:
    name: "Stage 0: Ensure Required Labels"
    runs-on: ubuntu-latest
    
    steps:
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Ensure required labels
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'EOF'
          import requests
          import os
          
          GITHUB_TOKEN = os.environ['GH_TOKEN']
          REPO = os.environ['GITHUB_REPOSITORY']
          
          headers = {
              'Authorization': f'token {GITHUB_TOKEN}',
              'Accept': 'application/vnd.github.v3+json'
          }
          
          required_labels = [
              {'name': 'learning', 'color': '0E8A16', 'description': 'Learning-related'},
              {'name': 'agent-mission', 'color': 'D93F0B', 'description': 'Agent mission'},
              {'name': 'ai-generated', 'color': '1D76DB', 'description': 'AI-generated'},
              {'name': 'automated', 'color': 'FBCA04', 'description': 'Automated'},
              {'name': 'pipeline', 'color': '5319E7', 'description': 'Pipeline workflow'},
              {'name': 'auto-merge', 'color': 'BFD4F2', 'description': 'Auto-merge eligible'},
              {'name': 'world-model', 'color': '006B75', 'description': 'World model updates'},
          ]
          
          url = f'https://api.github.com/repos/{REPO}/labels'
          response = requests.get(url, headers=headers)
          existing_labels = {label['name'].lower() for label in response.json()}
          
          for label in required_labels:
              if label['name'].lower() not in existing_labels:
                  requests.post(url, headers=headers, json=label)
                  print(f"‚úì Created label: {label['name']}")
              else:
                  print(f"‚úì Label exists: {label['name']}")
          
          print("‚úÖ All required labels ready")
          EOF

  # ============================================================================
  # STAGE 1: LEARNING COLLECTION (Parallel)
  # ============================================================================
  
  learn-tldr:
    name: "Stage 1a: Learn from TLDR"
    runs-on: ubuntu-latest
    needs: ensure-labels
    if: inputs.skip_learning != true
    outputs:
      learning_count: ${{ steps.fetch.outputs.learning_count }}
      learning_file: ${{ steps.fetch.outputs.learning_file }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests feedparser lxml html5lib

      - name: Fetch TLDR content
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          import re
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup
          import os
          import time
          import feedparser
          
          print("üî• Fetching TLDR Tech content...")
          
          class WebContentFetcher:
              def __init__(self):
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Mozilla/5.0 (compatible; ChainedAI/1.0)'
                  })
              
              def fetch(self, url):
                  try:
                      response = self.session.get(url, timeout=10, allow_redirects=True)
                      if response.status_code == 200:
                          soup = BeautifulSoup(response.text, 'html.parser')
                          for element in soup(['script', 'style', 'nav', 'footer', 'iframe']):
                              element.decompose()
                          content = soup.find('article') or soup.find('main') or soup.find('body')
                          if content:
                              text = content.get_text(separator='\n', strip=True)
                              lines = [line.strip() for line in text.split('\n') if line.strip()]
                              text = '\n'.join(lines)
                              if len(text) > 2000:
                                  text = text[:2000] + '\n\n[Content truncated...]'
                              return text
                  except Exception as e:
                      print(f"  Warning: Could not fetch {url}: {e}")
                  return None
          
          fetcher = WebContentFetcher()
          rss_urls = [
              'https://tldr.tech/api/rss/tech',
              'https://tldr.tech/api/rss/ai',
          ]
          
          all_entries = []
          for rss_url in rss_urls:
              try:
                  print(f"Fetching RSS: {rss_url}")
                  feed = feedparser.parse(rss_url)
                  for entry in feed.entries[:10]:
                      all_entries.append(entry)
                  print(f"  Found {len(feed.entries)} entries")
              except Exception as e:
                  print(f"  Error fetching {rss_url}: {e}")
          
          learnings = []
          for entry in all_entries[:20]:
              title = entry.get('title', '')
              link = entry.get('link', '')
              summary = entry.get('summary', '')[:500]
              
              learning = {
                  'source': 'tldr',
                  'title': title,
                  'url': link,
                  'summary': summary,
                  'collected_at': datetime.now(timezone.utc).isoformat()
              }
              learnings.append(learning)
          
          data = {
              'source': 'tldr',
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'learnings': learnings,
              'count': len(learnings)
          }
          
          os.makedirs('learnings', exist_ok=True)
          filename = f"learnings/tldr_{datetime.now().strftime('%Y%m%d')}.json"
          with open(filename, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"‚úÖ Collected {len(learnings)} TLDR learnings ‚Üí {filename}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"learning_count={len(learnings)}\n")
              f.write(f"learning_file={filename}\n")
          PYTHON_SCRIPT

      - name: Upload TLDR learnings
        uses: actions/upload-artifact@v4
        with:
          name: tldr-learnings
          path: learnings/tldr_*.json
          retention-days: 7

  learn-hackernews:
    name: "Stage 1b: Learn from Hacker News"
    runs-on: ubuntu-latest
    needs: ensure-labels
    if: inputs.skip_learning != true
    outputs:
      story_count: ${{ steps.fetch.outputs.story_count }}
      learning_file: ${{ steps.fetch.outputs.learning_file }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 lxml html5lib

      - name: Fetch Hacker News
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          from datetime import datetime, timezone
          import os
          
          print("üóûÔ∏è Fetching Hacker News top stories...")
          
          try:
              response = requests.get('https://hacker-news.firebaseio.com/v0/topstories.json', timeout=10)
              story_ids = response.json()[:30]
              print(f"Found {len(story_ids)} top story IDs")
          except Exception as e:
              print(f"Error fetching story IDs: {e}")
              story_ids = []
          
          stories = []
          for story_id in story_ids[:20]:
              try:
                  story_response = requests.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json', timeout=5)
                  story = story_response.json()
                  
                  if story and story.get('type') == 'story':
                      stories.append({
                          'source': 'hackernews',
                          'title': story.get('title', ''),
                          'url': story.get('url', ''),
                          'score': story.get('score', 0),
                          'comments': story.get('descendants', 0),
                          'collected_at': datetime.now(timezone.utc).isoformat()
                      })
              except Exception as e:
                  print(f"Error fetching story {story_id}: {e}")
          
          data = {
              'source': 'hackernews',
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'stories': stories,
              'count': len(stories)
          }
          
          os.makedirs('learnings', exist_ok=True)
          filename = f"learnings/hn_{datetime.now().strftime('%Y%m%d')}.json"
          with open(filename, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"‚úÖ Collected {len(stories)} HN stories ‚Üí {filename}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"story_count={len(stories)}\n")
              f.write(f"learning_file={filename}\n")
          PYTHON_SCRIPT

      - name: Upload HN learnings
        uses: actions/upload-artifact@v4
        with:
          name: hn-learnings
          path: learnings/hn_*.json
          retention-days: 7

  learn-github-trending:
    name: "Stage 1c: Learn from GitHub Trending"
    runs-on: ubuntu-latest
    needs: ensure-labels
    if: inputs.skip_learning != true
    outputs:
      repo_count: ${{ steps.fetch.outputs.repo_count }}
      learning_file: ${{ steps.fetch.outputs.learning_file }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests lxml html5lib

      - name: Fetch GitHub Trending
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          from datetime import datetime, timezone
          import sys
          
          print("üî• Fetching GitHub Trending Repositories...")
          
          sys.path.insert(0, 'tools')
          import importlib.util
          spec = importlib.util.spec_from_file_location('fetcher', 'tools/fetch-github-trending.py')
          fetcher_module = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(fetcher_module)
          GitHubTrendingFetcher = fetcher_module.GitHubTrendingFetcher
          
          fetcher = GitHubTrendingFetcher()
          languages = ['python', 'javascript', 'go', 'rust', 'typescript']
          
          all_repos = []
          language_results = fetcher.fetch_multiple_languages(languages, since='daily', max_per_lang=5)
          
          for lang, repos in language_results.items():
              all_repos.extend(repos)
          
          data = {
              'source': 'github_trending',
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'repositories': all_repos,
              'count': len(all_repos)
          }
          
          os.makedirs('learnings', exist_ok=True)
          filename = f"learnings/github_trending_{datetime.now().strftime('%Y%m%d')}.json"
          with open(filename, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"‚úÖ Collected {len(all_repos)} GitHub repos ‚Üí {filename}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"repo_count={len(all_repos)}\n")
              f.write(f"learning_file={filename}\n")
          PYTHON_SCRIPT

      - name: Upload GitHub Trending learnings
        uses: actions/upload-artifact@v4
        with:
          name: github-learnings
          path: learnings/github_trending_*.json
          retention-days: 7

  # ============================================================================
  # STAGE 2: COMBINE LEARNINGS
  # ============================================================================
  
  combine-learnings:
    name: "Stage 2: Combine All Learnings"
    runs-on: ubuntu-latest
    needs: [learn-tldr, learn-hackernews, learn-github-trending]
    if: always() && !cancelled() && inputs.skip_learning != true
    outputs:
      total_learnings: ${{ steps.analyze.outputs.total_learnings }}
      combined_file: ${{ steps.analyze.outputs.combined_file }}
      has_learnings: ${{ steps.analyze.outputs.has_learnings }}
      pr_number: ${{ steps.learning_pr.outputs.pr_number }}
      pr_created: ${{ steps.learning_pr.outputs.pr_created }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download all learning artifacts
        uses: actions/download-artifact@v4
        with:
          path: learnings-artifacts

      - name: Combine learnings
        id: analyze
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          
          print("üß† Combining all learning sources...")
          
          # Collect all learning files
          all_learnings = []
          learnings_dir = Path('learnings-artifacts')
          
          if learnings_dir.exists():
              for subdir in learnings_dir.iterdir():
                  if subdir.is_dir():
                      for file in subdir.glob('*.json'):
                          print(f"Loading: {file}")
                          try:
                              with open(file) as f:
                                  data = json.load(f)
                                  if 'learnings' in data:
                                      all_learnings.extend(data['learnings'])
                                  elif 'stories' in data:
                                      all_learnings.extend(data['stories'])
                                  elif 'repositories' in data:
                                      all_learnings.extend(data['repositories'])
                          except Exception as e:
                              print(f"Error loading {file}: {e}")
          
          print(f"Total learnings combined: {len(all_learnings)}")
          
          # Create combined analysis
          combined = {
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'total_learnings': len(all_learnings),
              'sources': {
                  'tldr': len([l for l in all_learnings if l.get('source') == 'tldr']),
                  'hackernews': len([l for l in all_learnings if l.get('source') == 'hackernews']),
                  'github_trending': len([l for l in all_learnings if l.get('source') == 'github_trending'])
              },
              'learnings': all_learnings
          }
          
          # Save combined file
          os.makedirs('learnings', exist_ok=True)
          combined_file = f"learnings/combined_analysis_{datetime.now().strftime('%Y%m%d')}.json"
          with open(combined_file, 'w') as f:
              json.dump(combined, f, indent=2)
          
          print(f"‚úÖ Combined learnings saved ‚Üí {combined_file}")
          print(f"   TLDR: {combined['sources']['tldr']}")
          print(f"   HN: {combined['sources']['hackernews']}")
          print(f"   GitHub: {combined['sources']['github_trending']}")
          
          has_learnings = 'true' if len(all_learnings) > 0 else 'false'
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"total_learnings={len(all_learnings)}\n")
              f.write(f"combined_file={combined_file}\n")
              f.write(f"has_learnings={has_learnings}\n")
          PYTHON_SCRIPT

      - name: Create PR with learnings
        id: learning_pr
        if: steps.analyze.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add learnings/
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
            echo "pr_created=false" >> $GITHUB_OUTPUT
          else
            TIMESTAMP=$(date +%Y%m%d-%H%M%S)
            BRANCH_NAME="learning-pipeline/${TIMESTAMP}-${{ github.run_id }}"
            
            git checkout -b "$BRANCH_NAME"
            git commit -m "üß† Learning Pipeline - $(date +%Y-%m-%d)"
            git push origin "$BRANCH_NAME"
            
            # Create PR
            PR_URL=$(gh pr create \
              --title "üß† Learning Pipeline - $(date +%Y-%m-%d)" \
              --body "## üß† Autonomous Learning Pipeline

            **Total Learnings:** ${{ steps.analyze.outputs.total_learnings }}
            **Pipeline Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            This PR contains combined learnings from:
            - üì∞ TLDR Tech
            - üóûÔ∏è Hacker News  
            - üî• GitHub Trending

            **Note:** This PR will be auto-merged by the next pipeline stage.

            Next stages: Merge ‚Üí World Update ‚Üí Agent Missions
            " \
              --label "automated,learning,pipeline,auto-merge,copilot" \
              --base main \
              --head "$BRANCH_NAME")
            
            # Extract PR number
            PR_NUMBER=$(echo "$PR_URL" | grep -oE '[0-9]+$')
            echo "Created PR #${PR_NUMBER}: ${PR_URL}"
            echo "pr_number=${PR_NUMBER}" >> $GITHUB_OUTPUT
            echo "pr_created=true" >> $GITHUB_OUTPUT
          fi

  # ============================================================================
  # STAGE 2.5: MERGE LEARNING PR
  # ============================================================================
  
  merge-learning-pr:
    name: "Stage 2.5: Merge Learning PR"
    runs-on: ubuntu-latest
    needs: combine-learnings
    if: always() && !cancelled() && needs.combine-learnings.outputs.pr_created == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Trigger Auto Review and Merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.combine-learnings.outputs.pr_number }}"
          echo "üîÑ Triggering Auto Review & Merge workflow for PR #$PR_NUMBER..."
          
          # Trigger the auto-review-merge workflow with the specific PR number
          gh workflow run auto-review-merge.yml \
            --repo ${{ github.repository }} \
            -f pr_number="$PR_NUMBER"
          
          echo "‚úÖ Auto-review workflow triggered for PR #$PR_NUMBER"

      - name: Wait for PR to be merged
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.combine-learnings.outputs.pr_number }}"
          echo "‚è≥ Waiting for PR #$PR_NUMBER to be merged by auto-review workflow..."
          
          # Give auto-review-merge workflow time to start processing
          echo "   Giving auto-review workflow 8 seconds to start..."
          sleep 8
          
          # Wait up to 3 minutes with exponential backoff for the PR to be merged
          MAX_WAIT=180  # 3 minutes
          ELAPSED=8  # Already waited 8 seconds
          WAIT_INTERVAL=3  # Start with 3 seconds
          CHECK_COUNT=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            CHECK_COUNT=$((CHECK_COUNT + 1))
            
            # Check PR state - use --json to get full state including merged status
            PR_DATA=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt,closed)
            PR_STATE=$(echo "$PR_DATA" | jq -r '.state')
            MERGED_AT=$(echo "$PR_DATA" | jq -r '.mergedAt')
            IS_CLOSED=$(echo "$PR_DATA" | jq -r '.closed')
            
            # Check if merged (mergedAt will be non-null if merged)
            if [ "$MERGED_AT" != "null" ] && [ "$MERGED_AT" != "" ]; then
              echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Learning PR merged - continuing to world model update"
              exit 0
            elif [ "$PR_STATE" = "MERGED" ]; then
              echo "‚úÖ PR #$PR_NUMBER has been merged successfully! (detected in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Learning PR merged - continuing to world model update"
              exit 0
            elif [ "$IS_CLOSED" = "true" ] && [ "$MERGED_AT" = "null" ]; then
              echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
              echo "This may indicate an issue with the auto-review process"
              exit 1
            elif [ "$PR_STATE" = "CLOSED" ]; then
              # Double-check if it was actually merged
              MERGED_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json mergedAt --jq '.mergedAt')
              if [ "$MERGED_CHECK" != "null" ] && [ "$MERGED_CHECK" != "" ]; then
                echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s)"
                echo "üéâ Learning PR merged - continuing to world model update"
                exit 0
              else
                echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
                exit 1
              fi
            fi
            
            # Still waiting - sleep and then update elapsed time
            echo "   Check $CHECK_COUNT: PR #$PR_NUMBER is $PR_STATE, waiting ${WAIT_INTERVAL}s... (${ELAPSED}s elapsed)"
            sleep $WAIT_INTERVAL
            ELAPSED=$((ELAPSED + WAIT_INTERVAL))
            
            # Exponential backoff: double wait time, max 30 seconds
            if [ $WAIT_INTERVAL -lt 30 ]; then
              WAIT_INTERVAL=$((WAIT_INTERVAL * 2))
            fi
          done
          
          # Timeout reached - do final check
          echo "‚è∞ Timeout reached after ${ELAPSED}s"
          FINAL_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt --jq '{state: .state, mergedAt: .mergedAt}')
          echo "Final PR state: $FINAL_CHECK"
          
          FINAL_MERGED=$(echo "$FINAL_CHECK" | jq -r '.mergedAt')
          if [ "$FINAL_MERGED" != "null" ] && [ "$FINAL_MERGED" != "" ]; then
            echo "‚úÖ PR #$PR_NUMBER was actually merged! (confirmed on final check)"
            exit 0
          fi
          
          echo "‚ùå PR #$PR_NUMBER was not merged within ${MAX_WAIT} seconds"
          echo "Check PR status at: https://github.com/${{ github.repository }}/pull/$PR_NUMBER"
          exit 1

  # ============================================================================
  # STAGE 3: WORLD MODEL UPDATE
  # ============================================================================
  
  world-update:
    name: "Stage 3: Update World Model"
    runs-on: ubuntu-latest
    needs: [combine-learnings, merge-learning-pr]
    if: always() && !cancelled() && needs.combine-learnings.outputs.has_learnings == 'true' && inputs.skip_world_update != true
    outputs:
      world_tick: ${{ steps.update.outputs.world_tick }}
      has_changes: ${{ steps.update.outputs.has_changes }}
      pr_number: ${{ steps.world_pr.outputs.pr_number }}
      pr_created: ${{ steps.world_pr.outputs.pr_created }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Update world model
        id: update
        run: |
          echo "üåç Updating world model..."
          
          # Sync agents from registry to world
          python3 world/sync_agents_to_world.py 2>&1 || echo "‚ö†Ô∏è Agent sync warning (continuing)"
          
          # Sync learnings to world ideas
          python3 world/sync_learnings_to_ideas.py 2>&1 || echo "‚ö†Ô∏è Learning sync warning (continuing)"
          
          # Update agent state
          python3 scripts/update_agent.py 2>&1 || echo "‚ö†Ô∏è Agent state update warning (continuing)"
          
          # Sync to docs for GitHub Pages
          mkdir -p docs/world
          cp world/world_state.json docs/world/
          cp world/knowledge.json docs/world/
          
          # Extract world tick
          WORLD_TICK=$(python3 -c "import json; print(json.load(open('world/world_state.json'))['tick'])" 2>/dev/null || echo "unknown")
          
          echo "world_tick=${WORLD_TICK}" >> $GITHUB_OUTPUT
          
          # Check for changes
          git add world/ docs/world/
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No world changes"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "‚úÖ World updated (tick: ${WORLD_TICK})"
          fi

      - name: Create PR with world updates
        id: world_pr
        if: steps.update.outputs.has_changes == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          BRANCH_NAME="world-pipeline/${TIMESTAMP}-${{ github.run_id }}"
          
          git checkout -b "$BRANCH_NAME"
          git commit -m "üåç World Model Update - Pipeline"
          git push origin "$BRANCH_NAME"
          
          # Create PR
          PR_URL=$(gh pr create \
            --title "üåç World Model Update - $(date +%Y-%m-%d)" \
            --body "## üåç World Model Update

          **World Tick:** ${{ steps.update.outputs.world_tick }}
          **Pipeline Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          Updates:
          - ‚úÖ Agent positions synchronized
          - ‚úÖ Learning ideas integrated
          - ‚úÖ World tick incremented

          **Note:** This PR will be auto-merged by the next pipeline stage.

          Next stages: Merge ‚Üí Agent Missions
          " \
            --label "automated,world-model,pipeline,auto-merge,copilot" \
            --base main \
            --head "$BRANCH_NAME")
          
          # Extract PR number
          PR_NUMBER=$(echo "$PR_URL" | grep -oE '[0-9]+$')
          echo "Created PR #${PR_NUMBER}: ${PR_URL}"
          echo "pr_number=${PR_NUMBER}" >> $GITHUB_OUTPUT
          echo "pr_created=true" >> $GITHUB_OUTPUT

  # ============================================================================
  # STAGE 3.5: MERGE WORLD PR
  # ============================================================================
  
  merge-world-pr:
    name: "Stage 3.5: Merge World PR"
    runs-on: ubuntu-latest
    needs: world-update
    if: always() && !cancelled() && needs.world-update.outputs.pr_created == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Trigger Auto Review and Merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.world-update.outputs.pr_number }}"
          echo "üîÑ Triggering Auto Review & Merge workflow for PR #$PR_NUMBER..."
          
          # Trigger the auto-review-merge workflow with the specific PR number
          gh workflow run auto-review-merge.yml \
            --repo ${{ github.repository }} \
            -f pr_number="$PR_NUMBER"
          
          echo "‚úÖ Auto-review workflow triggered for PR #$PR_NUMBER"

      - name: Wait for PR to be merged
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.world-update.outputs.pr_number }}"
          echo "‚è≥ Waiting for PR #$PR_NUMBER to be merged by auto-review workflow..."
          
          # Give auto-review-merge workflow time to start processing
          echo "   Giving auto-review workflow 8 seconds to start..."
          sleep 8
          
          # Wait up to 3 minutes with exponential backoff for the PR to be merged
          MAX_WAIT=180  # 3 minutes
          ELAPSED=8  # Already waited 8 seconds
          WAIT_INTERVAL=3  # Start with 3 seconds
          CHECK_COUNT=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            CHECK_COUNT=$((CHECK_COUNT + 1))
            
            # Check PR state - use --json to get full state including merged status
            PR_DATA=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt,closed)
            PR_STATE=$(echo "$PR_DATA" | jq -r '.state')
            MERGED_AT=$(echo "$PR_DATA" | jq -r '.mergedAt')
            IS_CLOSED=$(echo "$PR_DATA" | jq -r '.closed')
            
            # Check if merged (mergedAt will be non-null if merged)
            if [ "$MERGED_AT" != "null" ] && [ "$MERGED_AT" != "" ]; then
              echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ World model PR merged - continuing to agent missions"
              exit 0
            elif [ "$PR_STATE" = "MERGED" ]; then
              echo "‚úÖ PR #$PR_NUMBER has been merged successfully! (detected in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ World model PR merged - continuing to agent missions"
              exit 0
            elif [ "$IS_CLOSED" = "true" ] && [ "$MERGED_AT" = "null" ]; then
              echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
              echo "This may indicate an issue with the auto-review process"
              exit 1
            elif [ "$PR_STATE" = "CLOSED" ]; then
              # Double-check if it was actually merged
              MERGED_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json mergedAt --jq '.mergedAt')
              if [ "$MERGED_CHECK" != "null" ] && [ "$MERGED_CHECK" != "" ]; then
                echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s)"
                echo "üéâ World model PR merged - continuing to agent missions"
                exit 0
              else
                echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
                exit 1
              fi
            fi
            
            # Still waiting - sleep and then update elapsed time
            echo "   Check $CHECK_COUNT: PR #$PR_NUMBER is $PR_STATE, waiting ${WAIT_INTERVAL}s... (${ELAPSED}s elapsed)"
            sleep $WAIT_INTERVAL
            ELAPSED=$((ELAPSED + WAIT_INTERVAL))
            
            # Exponential backoff: double wait time, max 30 seconds
            if [ $WAIT_INTERVAL -lt 30 ]; then
              WAIT_INTERVAL=$((WAIT_INTERVAL * 2))
            fi
          done
          
          # Timeout reached - do final check
          echo "‚è∞ Timeout reached after ${ELAPSED}s"
          FINAL_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt --jq '{state: .state, mergedAt: .mergedAt}')
          echo "Final PR state: $FINAL_CHECK"
          
          FINAL_MERGED=$(echo "$FINAL_CHECK" | jq -r '.mergedAt')
          if [ "$FINAL_MERGED" != "null" ] && [ "$FINAL_MERGED" != "" ]; then
            echo "‚úÖ PR #$PR_NUMBER was actually merged! (confirmed on final check)"
            exit 0
          fi
          
          echo "‚ùå PR #$PR_NUMBER was not merged within ${MAX_WAIT} seconds"
          echo "Check PR status at: https://github.com/${{ github.repository }}/pull/$PR_NUMBER"
          exit 1

  # ============================================================================
  # STAGE 4: AGENT MISSIONS
  # ============================================================================
  
  agent-missions:
    name: "Stage 4: Create Agent Missions"
    runs-on: ubuntu-latest
    needs: [world-update, merge-world-pr]
    if: always() && !cancelled() && needs.world-update.outputs.has_changes == 'true' && inputs.skip_missions != true
    outputs:
      mission_count: ${{ steps.create.outputs.mission_count }}
      pr_number: ${{ steps.mission_pr.outputs.pr_number }}
      pr_created: ${{ steps.mission_pr.outputs.pr_created }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Create missions
        id: create
        run: |
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime, timezone
          
          print("üéØ Creating agent missions...")
          
          # Load world state
          with open('world/world_state.json', 'r') as f:
              world_state = json.load(f)
          
          # Load knowledge
          with open('world/knowledge.json', 'r') as f:
              knowledge = json.load(f)
          
          agents = world_state.get('agents', [])
          ideas = knowledge.get('ideas', [])
          
          # Find recent learning-based ideas
          recent_ideas = [idea for idea in ideas if idea.get('source') == 'learning_analysis'][:5]
          
          print(f"Processing {len(recent_ideas)} recent ideas")
          
          if not recent_ideas:
              print("No recent ideas for missions")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"mission_count=0\n")
              exit(0)
          
          missions = []
          for idea in recent_ideas:
              idea_id = idea.get('id', 'unknown')
              idea_patterns = idea.get('patterns', [])
              idea_regions = idea.get('inspiration_regions', [])
              
              # Score agents
              agent_scores = []
              for agent in agents:
                  score = 0.5  # Base score
                  agent_scores.append({
                      'agent_id': agent.get('id'),
                      'agent_name': agent.get('label', 'Unknown'),
                      'specialization': agent.get('specialization', ''),
                      'score': score
                  })
              
              # Sort agents by score and select the best one
              agent_scores.sort(key=lambda x: x['score'], reverse=True)
              best_agent = agent_scores[0] if agent_scores else {
                  'agent_id': 'unknown',
                  'agent_name': 'Unknown',
                  'specialization': '',
                  'score': 0.0
              }
              
              missions.append({
                  'idea_id': idea_id,
                  'idea_title': idea.get('title', 'Unknown'),
                  'idea_summary': idea.get('summary', '')[:200],
                  'patterns': idea_patterns,
                  'agent': best_agent,  # Single agent, not list
                  'regions': [r.get('region_id', 'unknown') for r in idea_regions]
              })
          
          # Save missions
          with open('missions_data.json', 'w') as f:
              json.dump(missions, f, indent=2)
          
          print(f"‚úÖ Created {len(missions)} missions")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"mission_count={len(missions)}\n")
          EOF

      - name: Create mission issues
        if: steps.create.outputs.mission_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Use existing tool if available
          if [ -f tools/create_mission_issues.py ]; then
            chmod +x tools/create_mission_issues.py
            python3 tools/create_mission_issues.py || echo "‚ö†Ô∏è Issue creation failed"
          else
            echo "‚ÑπÔ∏è Mission issues tool not found, skipping issue creation"
          fi

      - name: Upload mission assignments artifact
        if: steps.create.outputs.mission_count > 0
        uses: actions/upload-artifact@v4
        with:
          name: mission-assignments
          path: created_missions.json
          retention-days: 1

      - name: Update world state
        if: steps.create.outputs.mission_count > 0
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime, timezone
          
          # Increment world tick
          with open('world/world_state.json', 'r') as f:
              world_state = json.load(f)
          
          world_state['tick'] = world_state.get('tick', 0) + 1
          world_state['time'] = datetime.now(timezone.utc).isoformat()
          
          with open('world/world_state.json', 'w') as f:
              json.dump(world_state, f, indent=2)
          
          print(f"‚úÖ World tick: {world_state['tick']}")
          EOF

      - name: Create PR with mission updates
        id: mission_pr
        if: steps.create.outputs.mission_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add world/
          
          if git diff --staged --quiet; then
            echo "No changes"
            echo "pr_created=false" >> $GITHUB_OUTPUT
          else
            TIMESTAMP=$(date +%Y%m%d-%H%M%S)
            BRANCH_NAME="missions-pipeline/${TIMESTAMP}-${{ github.run_id }}"
            
            git checkout -b "$BRANCH_NAME"
            git commit -m "üéØ Agent Missions - Pipeline"
            git push origin "$BRANCH_NAME"
            
            # Create PR
            PR_URL=$(gh pr create \
              --title "üéØ Agent Missions - $(date +%Y-%m-%d)" \
              --body "## üéØ Agent Missions Created

            **Missions:** ${{ steps.create.outputs.mission_count }}
            **Pipeline Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            Created by **@meta-coordinator** via autonomous pipeline.

            **Note:** This PR will be auto-merged by the next pipeline stage to complete the cycle.
            " \
              --label "automated,agent-mission,pipeline,auto-merge,copilot" \
              --base main \
              --head "$BRANCH_NAME")
            
            # Extract PR number
            PR_NUMBER=$(echo "$PR_URL" | grep -oE '[0-9]+$')
            echo "Created PR #${PR_NUMBER}: ${PR_URL}"
            echo "pr_number=${PR_NUMBER}" >> $GITHUB_OUTPUT
            echo "pr_created=true" >> $GITHUB_OUTPUT
          fi

  # ============================================================================
  # STAGE 4.5: MERGE MISSION PR
  # ============================================================================
  
  merge-mission-pr:
    name: "Stage 4.5: Merge Mission PR"
    runs-on: ubuntu-latest
    needs: agent-missions
    if: always() && !cancelled() && needs.agent-missions.outputs.pr_created == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Trigger Auto Review and Merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.agent-missions.outputs.pr_number }}"
          echo "üîÑ Triggering Auto Review & Merge workflow for PR #$PR_NUMBER..."
          
          # Trigger the auto-review-merge workflow with the specific PR number
          gh workflow run auto-review-merge.yml \
            --repo ${{ github.repository }} \
            -f pr_number="$PR_NUMBER"
          
          echo "‚úÖ Auto-review workflow triggered for PR #$PR_NUMBER"

      - name: Wait for PR to be merged
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.agent-missions.outputs.pr_number }}"
          echo "‚è≥ Waiting for PR #$PR_NUMBER to be merged by auto-review workflow..."
          
          # Give auto-review-merge workflow time to start processing
          echo "   Giving auto-review workflow 8 seconds to start..."
          sleep 8
          
          # Wait up to 3 minutes with exponential backoff for the PR to be merged
          MAX_WAIT=180  # 3 minutes
          ELAPSED=8  # Already waited 8 seconds
          WAIT_INTERVAL=3  # Start with 3 seconds
          CHECK_COUNT=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            CHECK_COUNT=$((CHECK_COUNT + 1))
            
            # Check PR state - use --json to get full state including merged status
            PR_DATA=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt,closed)
            PR_STATE=$(echo "$PR_DATA" | jq -r '.state')
            MERGED_AT=$(echo "$PR_DATA" | jq -r '.mergedAt')
            IS_CLOSED=$(echo "$PR_DATA" | jq -r '.closed')
            
            # Check if merged (mergedAt will be non-null if merged)
            if [ "$MERGED_AT" != "null" ] && [ "$MERGED_AT" != "" ]; then
              echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Pipeline complete - all PRs merged!"
              exit 0
            elif [ "$PR_STATE" = "MERGED" ]; then
              echo "‚úÖ PR #$PR_NUMBER has been merged successfully! (detected in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Pipeline complete - all PRs merged!"
              exit 0
            elif [ "$IS_CLOSED" = "true" ] && [ "$MERGED_AT" = "null" ]; then
              echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
              echo "This may indicate an issue with the auto-review process"
              exit 1
            elif [ "$PR_STATE" = "CLOSED" ]; then
              # Double-check if it was actually merged
              MERGED_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json mergedAt --jq '.mergedAt')
              if [ "$MERGED_CHECK" != "null" ] && [ "$MERGED_CHECK" != "" ]; then
                echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s)"
                echo "üéâ Pipeline complete - all PRs merged!"
                exit 0
              else
                echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
                exit 1
              fi
            fi
            
            # Still waiting - sleep and then update elapsed time
            echo "   Check $CHECK_COUNT: PR #$PR_NUMBER is $PR_STATE, waiting ${WAIT_INTERVAL}s... (${ELAPSED}s elapsed)"
            sleep $WAIT_INTERVAL
            ELAPSED=$((ELAPSED + WAIT_INTERVAL))
            
            # Exponential backoff: double wait time, max 30 seconds
            if [ $WAIT_INTERVAL -lt 30 ]; then
              WAIT_INTERVAL=$((WAIT_INTERVAL * 2))
            fi
          done
          
          # Timeout reached - do final check
          echo "‚è∞ Timeout reached after ${ELAPSED}s"
          FINAL_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt --jq '{state: .state, mergedAt: .mergedAt}')
          echo "Final PR state: $FINAL_CHECK"
          
          FINAL_MERGED=$(echo "$FINAL_CHECK" | jq -r '.mergedAt')
          if [ "$FINAL_MERGED" != "null" ] && [ "$FINAL_MERGED" != "" ]; then
            echo "‚úÖ PR #$PR_NUMBER was actually merged! (confirmed on final check)"
            exit 0
          fi
          
          echo "‚ùå PR #$PR_NUMBER was not merged within ${MAX_WAIT} seconds"
          echo "Check PR status at: https://github.com/${{ github.repository }}/pull/$PR_NUMBER"
          exit 1

  # ============================================================================
  # STAGE 4.75: ASSIGN AGENTS TO MISSIONS
  # ============================================================================
  
  assign-agents-to-missions:
    name: "Stage 4.75: Assign Agents to Missions"
    runs-on: ubuntu-latest
    needs: [agent-missions, merge-mission-pr]
    if: always() && !cancelled() && needs.agent-missions.outputs.mission_count > 0 && inputs.skip_assignment != true
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install requests
      
      - name: Download mission assignments artifact
        uses: actions/download-artifact@v4
        with:
          name: mission-assignments
        continue-on-error: true
      
      - name: Assign agents to mission issues
        env:
          GH_TOKEN: ${{ secrets.COPILOT_PAT || secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REPOSITORY_OWNER: ${{ github.repository_owner }}
          GITHUB_REPOSITORY_NAME: ${{ github.event.repository.name }}
        run: |
          chmod +x tools/assign-agent-directly.sh
          
          echo "üéØ Stage 4.75: Assigning agents to mission issues"
          echo "=================================================="
          echo ""
          
          # Check if created_missions.json exists
          if [ ! -f created_missions.json ]; then
            echo "‚ö†Ô∏è  No created_missions.json found"
            echo "This means either:"
            echo "  1. No missions were created in Stage 4"
            echo "  2. The create_mission_issues.py script didn't run"
            echo ""
            echo "‚ÑπÔ∏è  Checking for mission issues to assign..."
            
            # Try to find recent mission issues without assignment
            RECENT_MISSION_ISSUES=$(gh issue list \
              --repo "$GITHUB_REPOSITORY" \
              --label "agent-mission" \
              --state open \
              --limit 20 \
              --json number,createdAt,labels \
              --jq '[.[] | select(.labels | map(.name) | contains(["agent-mission"]) and (contains(["copilot-assigned"]) | not))] | .[].number')
            
            if [ -z "$RECENT_MISSION_ISSUES" ]; then
              echo "‚ÑπÔ∏è  No unassigned mission issues found, skipping assignment"
              exit 0
            fi
            
            echo "Found unassigned mission issues: $RECENT_MISSION_ISSUES"
            echo "‚ö†Ô∏è  Cannot determine agent specialization without created_missions.json"
            echo "These issues will be picked up by the scheduled copilot-graphql-assign workflow"
            exit 0
          fi
          
          # Read created issues and assign agents
          echo "üìã Reading mission assignments from created_missions.json"
          
          # Count missions
          MISSION_COUNT=$(jq '. | length' created_missions.json)
          echo "Found $MISSION_COUNT missions to assign"
          echo ""
          
          # Track assignment results
          SUCCESS_COUNT=0
          FAILED_COUNT=0
          
          # Use jq to parse the JSON and iterate
          jq -r '.[] | "\(.issue_number) \(.agent_specialization)"' created_missions.json | while read -r issue_number agent_specialization; do
            if [ -n "$issue_number" ] && [ -n "$agent_specialization" ]; then
              echo ""
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
              echo "üéØ Assigning #$issue_number to @$agent_specialization"
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
              
              if ./tools/assign-agent-directly.sh "$issue_number" "$agent_specialization"; then
                echo "‚úÖ Successfully assigned #$issue_number to @$agent_specialization"
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              else
                echo "‚ùå Failed to assign #$issue_number to @$agent_specialization"
                FAILED_COUNT=$((FAILED_COUNT + 1))
                # Continue with other assignments even if one fails
              fi
            fi
          done
          
          echo ""
          echo "=================================================="
          echo "üìä Assignment Summary"
          echo "=================================================="
          echo "Total missions: $MISSION_COUNT"
          echo "Successfully assigned: $SUCCESS_COUNT"
          echo "Failed: $FAILED_COUNT"
          echo ""
          
          if [ $FAILED_COUNT -gt 0 ]; then
            echo "‚ö†Ô∏è  Some assignments failed. Common causes:"
            echo "  1. Missing COPILOT_PAT secret (default GITHUB_TOKEN can't assign Copilot)"
            echo "  2. Copilot not enabled for this repository"
            echo "  3. Network or API issues"
            echo ""
            echo "‚ÑπÔ∏è  Failed assignments will be retried by:"
            echo "  - Scheduled copilot-graphql-assign workflow (runs every 15 minutes)"
            echo "  - Manual workflow dispatch"
            echo ""
          fi
          
          echo "‚úÖ Stage 4.75 complete - Agent assignment process finished"

  # ============================================================================
  # STAGE 5: SELF-REINFORCEMENT (Optional)
  # ============================================================================
  
  self-reinforcement:
    name: "Stage 5: Self-Reinforcement (Optional)"
    runs-on: ubuntu-latest
    needs: [agent-missions, merge-mission-pr]
    if: always() && !cancelled() && inputs.include_self_reinforcement == true
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Collect insights
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üîÑ Collecting insights from completed work..."
          echo "‚ÑπÔ∏è  Self-reinforcement runs separately on daily schedule"
          echo "‚úÖ Pipeline complete"

  # ============================================================================
  # FINAL: PIPELINE SUMMARY
  # ============================================================================
  
  pipeline-summary:
    name: "Pipeline Summary"
    runs-on: ubuntu-latest
    needs: [combine-learnings, merge-learning-pr, world-update, merge-world-pr, agent-missions, merge-mission-pr, assign-agents-to-missions]
    if: always() && !cancelled()
    
    steps:
      - name: Log pipeline results
        run: |
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "üöÄ AUTONOMOUS LEARNING PIPELINE COMPLETE"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo ""
          echo "Stage 1: Learning Collection"
          echo "  Status: ‚úÖ Complete"
          echo ""
          echo "Stage 2: Combine Learnings"
          echo "  Total: ${{ needs.combine-learnings.outputs.total_learnings || 'N/A' }}"
          echo "  Status: ${{ needs.combine-learnings.result }}"
          echo ""
          echo "Stage 3: World Update"
          echo "  Tick: ${{ needs.world-update.outputs.world_tick || 'N/A' }}"
          echo "  Status: ${{ needs.world-update.result }}"
          echo ""
          echo "Stage 4: Agent Missions"
          echo "  Missions: ${{ needs.agent-missions.outputs.mission_count || 'N/A' }}"
          echo "  Status: ${{ needs.agent-missions.result }}"
          echo ""
          echo "Stage 4.75: Assign Agents to Missions"
          echo "  Status: ${{ needs.assign-agents-to-missions.result }}"
          echo ""
          echo "Pipeline Run: ${{ github.run_id }}"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
