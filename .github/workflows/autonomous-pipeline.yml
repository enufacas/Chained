name: "Autonomous Learning Pipeline"

on:
  schedule:
    # Run twice daily - morning and evening UTC
    - cron: '0 8,20 * * *'
  workflow_dispatch:
    inputs:
      skip_learning:
        description: 'Skip learning collection stage'
        required: false
        type: boolean
        default: false
      skip_world_update:
        description: 'Skip world model update stage'
        required: false
        type: boolean
        default: false
      skip_missions:
        description: 'Skip agent missions stage'
        required: false
        type: boolean
        default: false
      skip_assignment:
        description: 'Skip agent assignment stage'
        required: false
        type: boolean
        default: false
      include_self_reinforcement:
        description: 'Include self-reinforcement stage'
        required: false
        type: boolean
        default: false

permissions:
  contents: write
  issues: write
  pull-requests: write
  actions: write

jobs:
  # ============================================================================
  # STAGE 0: SETUP - Ensure Required Labels
  # ============================================================================
  
  ensure-labels:
    name: "Stage 0: Ensure Required Labels"
    runs-on: ubuntu-latest
    
    steps:
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Ensure required labels
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'EOF'
          import requests
          import os
          
          GITHUB_TOKEN = os.environ['GH_TOKEN']
          REPO = os.environ['GITHUB_REPOSITORY']
          
          headers = {
              'Authorization': f'token {GITHUB_TOKEN}',
              'Accept': 'application/vnd.github.v3+json'
          }
          
          required_labels = [
              {'name': 'learning', 'color': '0E8A16', 'description': 'Learning-related'},
              {'name': 'agent-mission', 'color': 'D93F0B', 'description': 'Agent mission'},
              {'name': 'ai-generated', 'color': '1D76DB', 'description': 'AI-generated'},
              {'name': 'automated', 'color': 'FBCA04', 'description': 'Automated'},
              {'name': 'pipeline', 'color': '5319E7', 'description': 'Pipeline workflow'},
              {'name': 'auto-merge', 'color': 'BFD4F2', 'description': 'Auto-merge eligible'},
              {'name': 'world-model', 'color': '006B75', 'description': 'World model updates'},
          ]
          
          url = f'https://api.github.com/repos/{REPO}/labels'
          response = requests.get(url, headers=headers)
          existing_labels = {label['name'].lower() for label in response.json()}
          
          for label in required_labels:
              if label['name'].lower() not in existing_labels:
                  requests.post(url, headers=headers, json=label)
                  print(f"‚úì Created label: {label['name']}")
              else:
                  print(f"‚úì Label exists: {label['name']}")
          
          print("‚úÖ All required labels ready")
          EOF

  # ============================================================================
  # STAGE 1: LEARNING COLLECTION (Parallel)
  # ============================================================================
  
  learn-tldr:
    name: "Stage 1a: Learn from TLDR"
    runs-on: ubuntu-latest
    needs: ensure-labels
    if: inputs.skip_learning != true
    outputs:
      learning_count: ${{ steps.fetch.outputs.learning_count }}
      learning_file: ${{ steps.fetch.outputs.learning_file }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests feedparser lxml html5lib

      - name: Fetch TLDR content
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          import re
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup
          import os
          import time
          import feedparser
          
          print("üî• Fetching TLDR Tech content...")
          
          class WebContentFetcher:
              def __init__(self):
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Mozilla/5.0 (compatible; ChainedAI/1.0)'
                  })
              
              def fetch(self, url):
                  try:
                      response = self.session.get(url, timeout=10, allow_redirects=True)
                      if response.status_code == 200:
                          soup = BeautifulSoup(response.text, 'html.parser')
                          for element in soup(['script', 'style', 'nav', 'footer', 'iframe']):
                              element.decompose()
                          content = soup.find('article') or soup.find('main') or soup.find('body')
                          if content:
                              text = content.get_text(separator='\n', strip=True)
                              lines = [line.strip() for line in text.split('\n') if line.strip()]
                              text = '\n'.join(lines)
                              if len(text) > 2000:
                                  text = text[:2000] + '\n\n[Content truncated...]'
                              return text
                  except Exception as e:
                      print(f"  Warning: Could not fetch {url}: {e}")
                  return None
          
          fetcher = WebContentFetcher()
          rss_urls = [
              'https://tldr.tech/api/rss/tech',
              'https://tldr.tech/api/rss/ai',
          ]
          
          all_entries = []
          for rss_url in rss_urls:
              try:
                  print(f"Fetching RSS: {rss_url}")
                  feed = feedparser.parse(rss_url)
                  for entry in feed.entries[:10]:
                      all_entries.append(entry)
                  print(f"  Found {len(feed.entries)} entries")
              except Exception as e:
                  print(f"  Error fetching {rss_url}: {e}")
          
          learnings = []
          for entry in all_entries[:20]:
              title = entry.get('title', '')
              link = entry.get('link', '')
              summary = entry.get('summary', '')[:500]
              
              learning = {
                  'source': 'tldr',
                  'title': title,
                  'url': link,
                  'summary': summary,
                  'collected_at': datetime.now(timezone.utc).isoformat()
              }
              learnings.append(learning)
          
          data = {
              'source': 'tldr',
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'learnings': learnings,
              'count': len(learnings)
          }
          
          os.makedirs('learnings', exist_ok=True)
          filename = f"learnings/tldr_{datetime.now().strftime('%Y%m%d')}.json"
          with open(filename, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"‚úÖ Collected {len(learnings)} TLDR learnings ‚Üí {filename}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"learning_count={len(learnings)}\n")
              f.write(f"learning_file={filename}\n")
          PYTHON_SCRIPT

      - name: Upload TLDR learnings
        uses: actions/upload-artifact@v4
        with:
          name: tldr-learnings
          path: learnings/tldr_*.json
          retention-days: 7

  learn-hackernews:
    name: "Stage 1b: Learn from Hacker News"
    runs-on: ubuntu-latest
    needs: ensure-labels
    if: inputs.skip_learning != true
    outputs:
      story_count: ${{ steps.fetch.outputs.story_count }}
      learning_file: ${{ steps.fetch.outputs.learning_file }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 lxml html5lib

      - name: Fetch Hacker News
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          from datetime import datetime, timezone
          import os
          
          print("üóûÔ∏è Fetching Hacker News top stories...")
          
          try:
              response = requests.get('https://hacker-news.firebaseio.com/v0/topstories.json', timeout=10)
              story_ids = response.json()[:30]
              print(f"Found {len(story_ids)} top story IDs")
          except Exception as e:
              print(f"Error fetching story IDs: {e}")
              story_ids = []
          
          stories = []
          for story_id in story_ids[:20]:
              try:
                  story_response = requests.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json', timeout=5)
                  story = story_response.json()
                  
                  if story and story.get('type') == 'story':
                      stories.append({
                          'source': 'hackernews',
                          'title': story.get('title', ''),
                          'url': story.get('url', ''),
                          'score': story.get('score', 0),
                          'comments': story.get('descendants', 0),
                          'collected_at': datetime.now(timezone.utc).isoformat()
                      })
              except Exception as e:
                  print(f"Error fetching story {story_id}: {e}")
          
          data = {
              'source': 'hackernews',
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'stories': stories,
              'count': len(stories)
          }
          
          os.makedirs('learnings', exist_ok=True)
          filename = f"learnings/hn_{datetime.now().strftime('%Y%m%d')}.json"
          with open(filename, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"‚úÖ Collected {len(stories)} HN stories ‚Üí {filename}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"story_count={len(stories)}\n")
              f.write(f"learning_file={filename}\n")
          PYTHON_SCRIPT

      - name: Upload HN learnings
        uses: actions/upload-artifact@v4
        with:
          name: hn-learnings
          path: learnings/hn_*.json
          retention-days: 7

  learn-github-trending:
    name: "Stage 1c: Learn from GitHub Trending"
    runs-on: ubuntu-latest
    needs: ensure-labels
    if: inputs.skip_learning != true
    outputs:
      repo_count: ${{ steps.fetch.outputs.repo_count }}
      learning_file: ${{ steps.fetch.outputs.learning_file }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests lxml html5lib

      - name: Fetch GitHub Trending
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          from datetime import datetime, timezone
          import sys
          
          print("üî• Fetching GitHub Trending Repositories...")
          
          sys.path.insert(0, 'tools')
          import importlib.util
          spec = importlib.util.spec_from_file_location('fetcher', 'tools/fetch-github-trending.py')
          fetcher_module = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(fetcher_module)
          GitHubTrendingFetcher = fetcher_module.GitHubTrendingFetcher
          
          fetcher = GitHubTrendingFetcher()
          languages = ['python', 'javascript', 'go', 'rust', 'typescript']
          
          all_repos = []
          language_results = fetcher.fetch_multiple_languages(languages, since='daily', max_per_lang=5)
          
          for lang, repos in language_results.items():
              all_repos.extend(repos)
          
          data = {
              'source': 'github_trending',
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'repositories': all_repos,
              'count': len(all_repos)
          }
          
          os.makedirs('learnings', exist_ok=True)
          filename = f"learnings/github_trending_{datetime.now().strftime('%Y%m%d')}.json"
          with open(filename, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"‚úÖ Collected {len(all_repos)} GitHub repos ‚Üí {filename}")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"repo_count={len(all_repos)}\n")
              f.write(f"learning_file={filename}\n")
          PYTHON_SCRIPT

      - name: Upload GitHub Trending learnings
        uses: actions/upload-artifact@v4
        with:
          name: github-learnings
          path: learnings/github_trending_*.json
          retention-days: 7

  # ============================================================================
  # STAGE 2: COMBINE LEARNINGS
  # ============================================================================
  
  combine-learnings:
    name: "Stage 2: Combine All Learnings"
    runs-on: ubuntu-latest
    needs: [learn-tldr, learn-hackernews, learn-github-trending]
    if: always() && !cancelled() && inputs.skip_learning != true
    outputs:
      total_learnings: ${{ steps.analyze.outputs.total_learnings }}
      combined_file: ${{ steps.analyze.outputs.combined_file }}
      has_learnings: ${{ steps.analyze.outputs.has_learnings }}
      pr_number: ${{ steps.learning_pr.outputs.pr_number }}
      pr_created: ${{ steps.learning_pr.outputs.pr_created }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Download all learning artifacts
        uses: actions/download-artifact@v4
        with:
          path: learnings-artifacts

      - name: Combine learnings
        id: analyze
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          
          print("üß† Combining all learning sources...")
          
          # Collect all learning files
          all_learnings = []
          learnings_dir = Path('learnings-artifacts')
          
          if learnings_dir.exists():
              for subdir in learnings_dir.iterdir():
                  if subdir.is_dir():
                      for file in subdir.glob('*.json'):
                          print(f"Loading: {file}")
                          try:
                              with open(file) as f:
                                  data = json.load(f)
                                  if 'learnings' in data:
                                      all_learnings.extend(data['learnings'])
                                  elif 'stories' in data:
                                      all_learnings.extend(data['stories'])
                                  elif 'repositories' in data:
                                      all_learnings.extend(data['repositories'])
                          except Exception as e:
                              print(f"Error loading {file}: {e}")
          
          print(f"Total learnings combined: {len(all_learnings)}")
          
          # Create combined analysis
          combined = {
              'timestamp': datetime.now(timezone.utc).isoformat(),
              'total_learnings': len(all_learnings),
              'sources': {
                  'tldr': len([l for l in all_learnings if l.get('source') == 'tldr']),
                  'hackernews': len([l for l in all_learnings if l.get('source') == 'hackernews']),
                  'github_trending': len([l for l in all_learnings if l.get('source') == 'github_trending'])
              },
              'learnings': all_learnings
          }
          
          # Save combined file
          os.makedirs('learnings', exist_ok=True)
          combined_file = f"learnings/combined_analysis_{datetime.now().strftime('%Y%m%d')}.json"
          with open(combined_file, 'w') as f:
              json.dump(combined, f, indent=2)
          
          print(f"‚úÖ Combined learnings saved ‚Üí {combined_file}")
          print(f"   TLDR: {combined['sources']['tldr']}")
          print(f"   HN: {combined['sources']['hackernews']}")
          print(f"   GitHub: {combined['sources']['github_trending']}")
          
          has_learnings = 'true' if len(all_learnings) > 0 else 'false'
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"total_learnings={len(all_learnings)}\n")
              f.write(f"combined_file={combined_file}\n")
              f.write(f"has_learnings={has_learnings}\n")
          PYTHON_SCRIPT

      - name: Create PR with learnings
        id: learning_pr
        if: steps.analyze.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add learnings/
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
            echo "pr_created=false" >> $GITHUB_OUTPUT
          else
            TIMESTAMP=$(date +%Y%m%d-%H%M%S)
            BRANCH_NAME="learning-pipeline/${TIMESTAMP}-${{ github.run_id }}"
            
            git checkout -b "$BRANCH_NAME"
            git commit -m "üß† Learning Pipeline - $(date +%Y-%m-%d)"
            git push origin "$BRANCH_NAME"
            
            # Create PR
            PR_URL=$(gh pr create \
              --title "üß† Learning Pipeline - $(date +%Y-%m-%d)" \
              --body "## üß† Autonomous Learning Pipeline

            **Total Learnings:** ${{ steps.analyze.outputs.total_learnings }}
            **Pipeline Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            This PR contains combined learnings from:
            - üì∞ TLDR Tech
            - üóûÔ∏è Hacker News  
            - üî• GitHub Trending

            **Note:** This PR will be auto-merged by the next pipeline stage.

            Next stages: Merge ‚Üí World Update ‚Üí Agent Missions
            " \
              --label "automated,learning,pipeline,auto-merge,copilot" \
              --base main \
              --head "$BRANCH_NAME")
            
            # Extract PR number
            PR_NUMBER=$(echo "$PR_URL" | grep -oE '[0-9]+$')
            echo "Created PR #${PR_NUMBER}: ${PR_URL}"
            echo "pr_number=${PR_NUMBER}" >> $GITHUB_OUTPUT
            echo "pr_created=true" >> $GITHUB_OUTPUT
          fi

  # ============================================================================
  # STAGE 2.5: MERGE LEARNING PR
  # ============================================================================
  
  merge-learning-pr:
    name: "Stage 2.5: Merge Learning PR"
    runs-on: ubuntu-latest
    needs: combine-learnings
    if: always() && !cancelled() && needs.combine-learnings.outputs.pr_created == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Trigger Auto Review and Merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.combine-learnings.outputs.pr_number }}"
          echo "üîÑ Triggering Auto Review & Merge workflow for PR #$PR_NUMBER..."
          
          # Trigger the auto-review-merge workflow with the specific PR number
          gh workflow run auto-review-merge.yml \
            --repo ${{ github.repository }} \
            -f pr_number="$PR_NUMBER"
          
          echo "‚úÖ Auto-review workflow triggered for PR #$PR_NUMBER"

      - name: Wait for PR to be merged
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.combine-learnings.outputs.pr_number }}"
          echo "‚è≥ Waiting for PR #$PR_NUMBER to be merged by auto-review workflow..."
          
          # Give auto-review-merge workflow time to start processing
          echo "   Giving auto-review workflow 8 seconds to start..."
          sleep 8
          
          # Wait up to 3 minutes with exponential backoff for the PR to be merged
          MAX_WAIT=180  # 3 minutes
          ELAPSED=8  # Already waited 8 seconds
          WAIT_INTERVAL=3  # Start with 3 seconds
          CHECK_COUNT=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            CHECK_COUNT=$((CHECK_COUNT + 1))
            
            # Check PR state - use --json to get full state including merged status
            PR_DATA=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt,closed)
            PR_STATE=$(echo "$PR_DATA" | jq -r '.state')
            MERGED_AT=$(echo "$PR_DATA" | jq -r '.mergedAt')
            IS_CLOSED=$(echo "$PR_DATA" | jq -r '.closed')
            
            # Check if merged (mergedAt will be non-null if merged)
            if [ "$MERGED_AT" != "null" ] && [ "$MERGED_AT" != "" ]; then
              echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Learning PR merged - continuing to world model update"
              exit 0
            elif [ "$PR_STATE" = "MERGED" ]; then
              echo "‚úÖ PR #$PR_NUMBER has been merged successfully! (detected in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Learning PR merged - continuing to world model update"
              exit 0
            elif [ "$IS_CLOSED" = "true" ] && [ "$MERGED_AT" = "null" ]; then
              echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
              echo "This may indicate an issue with the auto-review process"
              exit 1
            elif [ "$PR_STATE" = "CLOSED" ]; then
              # Double-check if it was actually merged
              MERGED_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json mergedAt --jq '.mergedAt')
              if [ "$MERGED_CHECK" != "null" ] && [ "$MERGED_CHECK" != "" ]; then
                echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s)"
                echo "üéâ Learning PR merged - continuing to world model update"
                exit 0
              else
                echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
                exit 1
              fi
            fi
            
            # Still waiting - sleep and then update elapsed time
            echo "   Check $CHECK_COUNT: PR #$PR_NUMBER is $PR_STATE, waiting ${WAIT_INTERVAL}s... (${ELAPSED}s elapsed)"
            sleep $WAIT_INTERVAL
            ELAPSED=$((ELAPSED + WAIT_INTERVAL))
            
            # Exponential backoff: double wait time, max 30 seconds
            if [ $WAIT_INTERVAL -lt 30 ]; then
              WAIT_INTERVAL=$((WAIT_INTERVAL * 2))
            fi
          done
          
          # Timeout reached - do final check
          echo "‚è∞ Timeout reached after ${ELAPSED}s"
          FINAL_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt --jq '{state: .state, mergedAt: .mergedAt}')
          echo "Final PR state: $FINAL_CHECK"
          
          FINAL_MERGED=$(echo "$FINAL_CHECK" | jq -r '.mergedAt')
          if [ "$FINAL_MERGED" != "null" ] && [ "$FINAL_MERGED" != "" ]; then
            echo "‚úÖ PR #$PR_NUMBER was actually merged! (confirmed on final check)"
            exit 0
          fi
          
          echo "‚ùå PR #$PR_NUMBER was not merged within ${MAX_WAIT} seconds"
          echo "Check PR status at: https://github.com/${{ github.repository }}/pull/$PR_NUMBER"
          exit 1

  # ============================================================================
  # STAGE 3: WORLD MODEL UPDATE
  # ============================================================================
  
  world-update:
    name: "Stage 3: Update World Model"
    runs-on: ubuntu-latest
    needs: [combine-learnings, merge-learning-pr]
    if: always() && !cancelled() && needs.combine-learnings.outputs.has_learnings == 'true' && inputs.skip_world_update != true
    outputs:
      world_tick: ${{ steps.update.outputs.world_tick }}
      has_changes: ${{ steps.update.outputs.has_changes }}
      pr_number: ${{ steps.world_pr.outputs.pr_number }}
      pr_created: ${{ steps.world_pr.outputs.pr_created }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Update world model
        id: update
        run: |
          echo "üåç Updating world model..."
          
          # Sync agents from registry to world
          python3 world/sync_agents_to_world.py 2>&1 || echo "‚ö†Ô∏è Agent sync warning (continuing)"
          
          # Sync learnings to world ideas
          python3 world/sync_learnings_to_ideas.py 2>&1 || echo "‚ö†Ô∏è Learning sync warning (continuing)"
          
          # Update agent state
          python3 scripts/update_agent.py 2>&1 || echo "‚ö†Ô∏è Agent state update warning (continuing)"
          
          # Sync to docs for GitHub Pages
          mkdir -p docs/world
          cp world/world_state.json docs/world/
          cp world/knowledge.json docs/world/
          
          # Extract world tick
          WORLD_TICK=$(python3 -c "import json; print(json.load(open('world/world_state.json'))['tick'])" 2>/dev/null || echo "unknown")
          
          echo "world_tick=${WORLD_TICK}" >> $GITHUB_OUTPUT
          
          # Check for changes
          git add world/ docs/world/
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No world changes"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "‚úÖ World updated (tick: ${WORLD_TICK})"
          fi

      - name: Create PR with world updates
        id: world_pr
        if: steps.update.outputs.has_changes == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          BRANCH_NAME="world-pipeline/${TIMESTAMP}-${{ github.run_id }}"
          
          git checkout -b "$BRANCH_NAME"
          git commit -m "üåç World Model Update - Pipeline"
          git push origin "$BRANCH_NAME"
          
          # Create PR
          PR_URL=$(gh pr create \
            --title "üåç World Model Update - $(date +%Y-%m-%d)" \
            --body "## üåç World Model Update

          **World Tick:** ${{ steps.update.outputs.world_tick }}
          **Pipeline Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          Updates:
          - ‚úÖ Agent positions synchronized
          - ‚úÖ Learning ideas integrated
          - ‚úÖ World tick incremented

          **Note:** This PR will be auto-merged by the next pipeline stage.

          Next stages: Merge ‚Üí Agent Missions
          " \
            --label "automated,world-model,pipeline,auto-merge,copilot" \
            --base main \
            --head "$BRANCH_NAME")
          
          # Extract PR number
          PR_NUMBER=$(echo "$PR_URL" | grep -oE '[0-9]+$')
          echo "Created PR #${PR_NUMBER}: ${PR_URL}"
          echo "pr_number=${PR_NUMBER}" >> $GITHUB_OUTPUT
          echo "pr_created=true" >> $GITHUB_OUTPUT

  # ============================================================================
  # STAGE 3.5: MERGE WORLD PR
  # ============================================================================
  
  merge-world-pr:
    name: "Stage 3.5: Merge World PR"
    runs-on: ubuntu-latest
    needs: world-update
    if: always() && !cancelled() && needs.world-update.outputs.pr_created == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Trigger Auto Review and Merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.world-update.outputs.pr_number }}"
          echo "üîÑ Triggering Auto Review & Merge workflow for PR #$PR_NUMBER..."
          
          # Trigger the auto-review-merge workflow with the specific PR number
          gh workflow run auto-review-merge.yml \
            --repo ${{ github.repository }} \
            -f pr_number="$PR_NUMBER"
          
          echo "‚úÖ Auto-review workflow triggered for PR #$PR_NUMBER"

      - name: Wait for PR to be merged
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.world-update.outputs.pr_number }}"
          echo "‚è≥ Waiting for PR #$PR_NUMBER to be merged by auto-review workflow..."
          
          # Give auto-review-merge workflow time to start processing
          echo "   Giving auto-review workflow 8 seconds to start..."
          sleep 8
          
          # Wait up to 3 minutes with exponential backoff for the PR to be merged
          MAX_WAIT=180  # 3 minutes
          ELAPSED=8  # Already waited 8 seconds
          WAIT_INTERVAL=3  # Start with 3 seconds
          CHECK_COUNT=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            CHECK_COUNT=$((CHECK_COUNT + 1))
            
            # Check PR state - use --json to get full state including merged status
            PR_DATA=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt,closed)
            PR_STATE=$(echo "$PR_DATA" | jq -r '.state')
            MERGED_AT=$(echo "$PR_DATA" | jq -r '.mergedAt')
            IS_CLOSED=$(echo "$PR_DATA" | jq -r '.closed')
            
            # Check if merged (mergedAt will be non-null if merged)
            if [ "$MERGED_AT" != "null" ] && [ "$MERGED_AT" != "" ]; then
              echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ World model PR merged - continuing to agent missions"
              exit 0
            elif [ "$PR_STATE" = "MERGED" ]; then
              echo "‚úÖ PR #$PR_NUMBER has been merged successfully! (detected in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ World model PR merged - continuing to agent missions"
              exit 0
            elif [ "$IS_CLOSED" = "true" ] && [ "$MERGED_AT" = "null" ]; then
              echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
              echo "This may indicate an issue with the auto-review process"
              exit 1
            elif [ "$PR_STATE" = "CLOSED" ]; then
              # Double-check if it was actually merged
              MERGED_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json mergedAt --jq '.mergedAt')
              if [ "$MERGED_CHECK" != "null" ] && [ "$MERGED_CHECK" != "" ]; then
                echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s)"
                echo "üéâ World model PR merged - continuing to agent missions"
                exit 0
              else
                echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
                exit 1
              fi
            fi
            
            # Still waiting - sleep and then update elapsed time
            echo "   Check $CHECK_COUNT: PR #$PR_NUMBER is $PR_STATE, waiting ${WAIT_INTERVAL}s... (${ELAPSED}s elapsed)"
            sleep $WAIT_INTERVAL
            ELAPSED=$((ELAPSED + WAIT_INTERVAL))
            
            # Exponential backoff: double wait time, max 30 seconds
            if [ $WAIT_INTERVAL -lt 30 ]; then
              WAIT_INTERVAL=$((WAIT_INTERVAL * 2))
            fi
          done
          
          # Timeout reached - do final check
          echo "‚è∞ Timeout reached after ${ELAPSED}s"
          FINAL_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt --jq '{state: .state, mergedAt: .mergedAt}')
          echo "Final PR state: $FINAL_CHECK"
          
          FINAL_MERGED=$(echo "$FINAL_CHECK" | jq -r '.mergedAt')
          if [ "$FINAL_MERGED" != "null" ] && [ "$FINAL_MERGED" != "" ]; then
            echo "‚úÖ PR #$PR_NUMBER was actually merged! (confirmed on final check)"
            exit 0
          fi
          
          echo "‚ùå PR #$PR_NUMBER was not merged within ${MAX_WAIT} seconds"
          echo "Check PR status at: https://github.com/${{ github.repository }}/pull/$PR_NUMBER"
          exit 1

  # ============================================================================
  # STAGE 4: AGENT MISSIONS
  # ============================================================================
  
  agent-missions:
    name: "Stage 4: Create Agent Missions"
    runs-on: ubuntu-latest
    needs: [world-update, merge-world-pr]
    if: always() && !cancelled() && needs.world-update.outputs.has_changes == 'true' && inputs.skip_missions != true
    outputs:
      mission_count: ${{ steps.create.outputs.mission_count }}
      pr_number: ${{ steps.mission_pr.outputs.pr_number }}
      pr_created: ${{ steps.mission_pr.outputs.pr_created }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Create missions
        id: create
        run: |
          python3 << 'EOF'
          import json
          import os
          import subprocess
          import hashlib
          from datetime import datetime, timezone
          
          print("üéØ Creating agent missions...")
          
          # Load world state
          with open('world/world_state.json', 'r') as f:
              world_state = json.load(f)
          
          # Load knowledge
          with open('world/knowledge.json', 'r') as f:
              knowledge = json.load(f)
          
          agents = world_state.get('agents', [])
          ideas = knowledge.get('ideas', [])
          
          # Find recent learning-based ideas that haven't been assigned yet
          recent_ideas = [idea for idea in ideas if idea.get('source') == 'learning_analysis'][:5]
          
          print(f"Processing {len(recent_ideas)} recent ideas")
          
          if not recent_ideas:
              print("No recent ideas for missions")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"mission_count=0\n")
              exit(0)
          
          # Load previous missions to avoid duplicates
          previous_mission_hashes = set()
          try:
              if os.path.exists('.github/agent-system/missions_history.json'):
                  with open('.github/agent-system/missions_history.json', 'r') as f:
                      history = json.load(f)
                      previous_mission_hashes = set(history.get('mission_hashes', []))
                      print(f"Loaded {len(previous_mission_hashes)} previous mission hashes")
          except Exception as e:
              print(f"Note: Could not load mission history: {e}")
          
          missions = []
          agent_assignment_count = {}  # Track how many times each agent has been assigned
          diversity_weight = 0.7  # Strong diversity preference (same as agent_learning_matcher.py)
          
          for idea in recent_ideas:
              idea_id = idea.get('id', 'unknown')
              idea_title = idea.get('title', 'Unknown')
              idea_patterns = idea.get('patterns', [])
              idea_regions = idea.get('inspiration_regions', [])
              idea_summary = idea.get('summary', '')[:200]
              
              # Create a hash of the idea to detect duplicates
              mission_content = f"{idea_id}:{idea_title}:{':'.join(sorted(idea_patterns))}"
              mission_hash = hashlib.md5(mission_content.encode()).hexdigest()
              
              # Skip if we've already created a mission for this idea
              if mission_hash in previous_mission_hashes:
                  print(f"  ‚è≠Ô∏è  Skipping duplicate mission for '{idea_title}' (hash: {mission_hash[:8]})")
                  continue
              
              # Use the existing match-issue-to-agent.py tool for intelligent matching
              # Combine title, summary, and patterns into a description for matching
              match_text = f"{idea_title}. {idea_summary}. Patterns: {', '.join(idea_patterns)}"
              
              try:
                  # Call match-issue-to-agent.py to get the best agent
                  result = subprocess.run(
                      ['python3', 'tools/match-issue-to-agent.py', match_text],
                      capture_output=True,
                      text=True,
                      check=True
                  )
                  match_data = json.loads(result.stdout)
                  matched_specialization = match_data.get('agent')
                  match_score = match_data.get('score', 0)
                  match_confidence = match_data.get('confidence', 'unknown')
                  all_scores = match_data.get('all_scores', {})
                  
                  # Apply diversity penalty (like agent_learning_matcher.py)
                  # This prevents one agent from monopolizing all missions
                  adjusted_scores = []
                  for agent_spec, base_score in all_scores.items():
                      # Penalty increases with each assignment (0%, 70%, 90%)
                      assignment_count = agent_assignment_count.get(agent_spec, 0)
                      penalty = assignment_count * diversity_weight
                      adjusted_score = base_score * (1.0 - min(penalty, 0.9))  # Cap at 90%
                      adjusted_scores.append((agent_spec, adjusted_score, base_score))
                  
                  # Sort by adjusted score to find best agent after diversity penalty
                  adjusted_scores.sort(key=lambda x: x[1], reverse=True)
                  
                  if adjusted_scores:
                      matched_specialization = adjusted_scores[0][0]
                      adjusted_score = adjusted_scores[0][1]
                      match_score = adjusted_scores[0][2]  # Original score for logging
                      
                      print(f"  ‚úÖ Matched '{idea_title}' to @{matched_specialization} (score: {match_score}, adjusted: {adjusted_score:.3f}, confidence: {match_confidence})")
                      
                      # Show if diversity penalty was applied
                      if agent_assignment_count.get(matched_specialization, 0) > 0:
                          rank = agent_assignment_count[matched_specialization] + 1
                          print(f"     ‚ÑπÔ∏è  Assignment #{rank} for @{matched_specialization} (diversity penalty applied)")
                  else:
                      print(f"  ‚ö†Ô∏è  No valid agents found for '{idea_title}'")
                      continue
                  
                  # Find the agent details from world state or create from specialization
                  agent_details = None
                  for agent in agents:
                      if agent.get('specialization') == matched_specialization:
                          agent_details = agent
                          break
                  
                  # If not in world_state.json, check if agent file exists
                  if not agent_details:
                      agent_file = f'.github/agents/{matched_specialization}.md'
                      if os.path.exists(agent_file):
                          # Agent file exists, use specialization as fallback
                          agent_details = {
                              'id': matched_specialization,
                              'label': matched_specialization.replace('-', ' ').title(),
                              'specialization': matched_specialization
                          }
                          print(f"  ‚ÑπÔ∏è  Using agent file for @{matched_specialization} (not in world_state.json)")
                      else:
                          print(f"  ‚ö†Ô∏è  Matched agent @{matched_specialization} not found in world state or agent files, skipping")
                          continue
                  
                  best_agent = {
                      'agent_id': agent_details.get('id'),
                      'agent_name': agent_details.get('label', 'Unknown'),
                      'specialization': matched_specialization,
                      'score': match_score
                  }
                  
                  # Update assignment count for diversity penalty tracking
                  agent_assignment_count[matched_specialization] = agent_assignment_count.get(matched_specialization, 0) + 1
                  
              except subprocess.CalledProcessError as e:
                  print(f"  ‚ö†Ô∏è  Error matching agent for '{idea_title}': {e.stderr}")
                  continue
              except json.JSONDecodeError as e:
                  print(f"  ‚ö†Ô∏è  Error parsing match result for '{idea_title}': {e}")
                  continue
              except Exception as e:
                  print(f"  ‚ö†Ô∏è  Unexpected error for '{idea_title}': {e}")
                  continue
              
              missions.append({
                  'idea_id': idea_id,
                  'idea_title': idea_title,
                  'idea_summary': idea_summary,
                  'patterns': idea_patterns,
                  'agent': best_agent,  # Single agent, not list
                  'regions': [r.get('region_id', 'unknown') for r in idea_regions],
                  'mission_hash': mission_hash
              })
          
          # Save missions
          with open('missions_data.json', 'w') as f:
              json.dump(missions, f, indent=2)
          
          # Update mission history to prevent duplicates in future runs
          try:
              os.makedirs('.github/agent-system', exist_ok=True)
              new_hashes = [m['mission_hash'] for m in missions]
              all_hashes = list(previous_mission_hashes) + new_hashes
              # Keep only the last 100 mission hashes to prevent unbounded growth
              all_hashes = all_hashes[-100:]
              
              with open('.github/agent-system/missions_history.json', 'w') as f:
                  json.dump({
                      'last_updated': datetime.now(timezone.utc).isoformat(),
                      'mission_hashes': all_hashes
                  }, f, indent=2)
              print(f"üìù Updated mission history with {len(new_hashes)} new hashes")
          except Exception as e:
              print(f"‚ö†Ô∏è  Could not update mission history: {e}")
          
          # Print agent distribution summary
          if missions:
              print()
              print("üìä Agent Distribution Summary:")
              agent_counts = {}
              for mission in missions:
                  agent_spec = mission['agent']['specialization']
                  agent_counts[agent_spec] = agent_counts.get(agent_spec, 0) + 1
              
              for agent_spec, count in sorted(agent_counts.items(), key=lambda x: x[1], reverse=True):
                  print(f"   ‚Ä¢ @{agent_spec}: {count} assignment(s)")
              
              unique_agents = len(agent_counts)
              total_missions = len(missions)
              diversity_pct = (unique_agents / total_missions * 100) if total_missions > 0 else 0
              print(f"\n   Diversity: {unique_agents}/{total_missions} unique agents ({diversity_pct:.0f}%)")
              
              if diversity_pct >= 80:
                  print(f"   ‚úÖ Excellent diversity!")
              elif diversity_pct >= 60:
                  print(f"   ‚úì Good diversity")
              else:
                  print(f"   ‚ö†Ô∏è  Low diversity (consider lowering min_score threshold)")
          
          print(f"‚úÖ Created {len(missions)} new missions (skipped duplicates)")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"mission_count={len(missions)}\n")
          EOF

      - name: Create mission issues
        if: steps.create.outputs.mission_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Use existing tool if available
          if [ -f tools/create_mission_issues.py ]; then
            chmod +x tools/create_mission_issues.py
            python3 tools/create_mission_issues.py || echo "‚ö†Ô∏è Issue creation failed"
          else
            echo "‚ÑπÔ∏è Mission issues tool not found, skipping issue creation"
          fi

      - name: Upload mission assignments artifact
        if: steps.create.outputs.mission_count > 0
        uses: actions/upload-artifact@v4
        with:
          name: mission-assignments
          path: created_missions.json
          retention-days: 1

      - name: Update world state
        if: steps.create.outputs.mission_count > 0
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime, timezone
          
          # Increment world tick
          with open('world/world_state.json', 'r') as f:
              world_state = json.load(f)
          
          world_state['tick'] = world_state.get('tick', 0) + 1
          world_state['time'] = datetime.now(timezone.utc).isoformat()
          
          with open('world/world_state.json', 'w') as f:
              json.dump(world_state, f, indent=2)
          
          print(f"‚úÖ World tick: {world_state['tick']}")
          EOF

      - name: Create PR with mission updates
        id: mission_pr
        if: steps.create.outputs.mission_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add world/ .github/agent-system/missions_history.json
          
          if git diff --staged --quiet; then
            echo "No changes"
            echo "pr_created=false" >> $GITHUB_OUTPUT
          else
            TIMESTAMP=$(date +%Y%m%d-%H%M%S)
            BRANCH_NAME="missions-pipeline/${TIMESTAMP}-${{ github.run_id }}"
            
            git checkout -b "$BRANCH_NAME"
            git commit -m "üéØ Agent Missions - Pipeline"
            git push origin "$BRANCH_NAME"
            
            # Create PR
            PR_URL=$(gh pr create \
              --title "üéØ Agent Missions - $(date +%Y-%m-%d)" \
              --body "## üéØ Agent Missions Created

            **Missions:** ${{ steps.create.outputs.mission_count }}
            **Pipeline Run:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            Created by **@meta-coordinator** via autonomous pipeline.

            **Note:** This PR will be auto-merged by the next pipeline stage to complete the cycle.
            " \
              --label "automated,agent-mission,pipeline,auto-merge,copilot" \
              --base main \
              --head "$BRANCH_NAME")
            
            # Extract PR number
            PR_NUMBER=$(echo "$PR_URL" | grep -oE '[0-9]+$')
            echo "Created PR #${PR_NUMBER}: ${PR_URL}"
            echo "pr_number=${PR_NUMBER}" >> $GITHUB_OUTPUT
            echo "pr_created=true" >> $GITHUB_OUTPUT
          fi

  # ============================================================================
  # STAGE 4.5: MERGE MISSION PR
  # ============================================================================
  
  merge-mission-pr:
    name: "Stage 4.5: Merge Mission PR"
    runs-on: ubuntu-latest
    needs: agent-missions
    if: always() && !cancelled() && needs.agent-missions.outputs.pr_created == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Trigger Auto Review and Merge
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.agent-missions.outputs.pr_number }}"
          echo "üîÑ Triggering Auto Review & Merge workflow for PR #$PR_NUMBER..."
          
          # Trigger the auto-review-merge workflow with the specific PR number
          gh workflow run auto-review-merge.yml \
            --repo ${{ github.repository }} \
            -f pr_number="$PR_NUMBER"
          
          echo "‚úÖ Auto-review workflow triggered for PR #$PR_NUMBER"

      - name: Wait for PR to be merged
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PR_NUMBER="${{ needs.agent-missions.outputs.pr_number }}"
          echo "‚è≥ Waiting for PR #$PR_NUMBER to be merged by auto-review workflow..."
          
          # Give auto-review-merge workflow time to start processing
          echo "   Giving auto-review workflow 8 seconds to start..."
          sleep 8
          
          # Wait up to 3 minutes with exponential backoff for the PR to be merged
          MAX_WAIT=180  # 3 minutes
          ELAPSED=8  # Already waited 8 seconds
          WAIT_INTERVAL=3  # Start with 3 seconds
          CHECK_COUNT=0
          
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            CHECK_COUNT=$((CHECK_COUNT + 1))
            
            # Check PR state - use --json to get full state including merged status
            PR_DATA=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt,closed)
            PR_STATE=$(echo "$PR_DATA" | jq -r '.state')
            MERGED_AT=$(echo "$PR_DATA" | jq -r '.mergedAt')
            IS_CLOSED=$(echo "$PR_DATA" | jq -r '.closed')
            
            # Check if merged (mergedAt will be non-null if merged)
            if [ "$MERGED_AT" != "null" ] && [ "$MERGED_AT" != "" ]; then
              echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Pipeline complete - all PRs merged!"
              exit 0
            elif [ "$PR_STATE" = "MERGED" ]; then
              echo "‚úÖ PR #$PR_NUMBER has been merged successfully! (detected in ${ELAPSED}s after $CHECK_COUNT checks)"
              echo "üéâ Pipeline complete - all PRs merged!"
              exit 0
            elif [ "$IS_CLOSED" = "true" ] && [ "$MERGED_AT" = "null" ]; then
              echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
              echo "This may indicate an issue with the auto-review process"
              exit 1
            elif [ "$PR_STATE" = "CLOSED" ]; then
              # Double-check if it was actually merged
              MERGED_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json mergedAt --jq '.mergedAt')
              if [ "$MERGED_CHECK" != "null" ] && [ "$MERGED_CHECK" != "" ]; then
                echo "‚úÖ PR #$PR_NUMBER was merged successfully! (verified in ${ELAPSED}s)"
                echo "üéâ Pipeline complete - all PRs merged!"
                exit 0
              else
                echo "‚ö†Ô∏è PR #$PR_NUMBER was closed without merging"
                exit 1
              fi
            fi
            
            # Still waiting - sleep and then update elapsed time
            echo "   Check $CHECK_COUNT: PR #$PR_NUMBER is $PR_STATE, waiting ${WAIT_INTERVAL}s... (${ELAPSED}s elapsed)"
            sleep $WAIT_INTERVAL
            ELAPSED=$((ELAPSED + WAIT_INTERVAL))
            
            # Exponential backoff: double wait time, max 30 seconds
            if [ $WAIT_INTERVAL -lt 30 ]; then
              WAIT_INTERVAL=$((WAIT_INTERVAL * 2))
            fi
          done
          
          # Timeout reached - do final check
          echo "‚è∞ Timeout reached after ${ELAPSED}s"
          FINAL_CHECK=$(gh pr view "$PR_NUMBER" --repo ${{ github.repository }} --json state,mergedAt --jq '{state: .state, mergedAt: .mergedAt}')
          echo "Final PR state: $FINAL_CHECK"
          
          FINAL_MERGED=$(echo "$FINAL_CHECK" | jq -r '.mergedAt')
          if [ "$FINAL_MERGED" != "null" ] && [ "$FINAL_MERGED" != "" ]; then
            echo "‚úÖ PR #$PR_NUMBER was actually merged! (confirmed on final check)"
            exit 0
          fi
          
          echo "‚ùå PR #$PR_NUMBER was not merged within ${MAX_WAIT} seconds"
          echo "Check PR status at: https://github.com/${{ github.repository }}/pull/$PR_NUMBER"
          exit 1

  # ============================================================================
  # STAGE 4.75: ASSIGN AGENTS TO MISSIONS
  # ============================================================================
  
  assign-agents-to-missions:
    name: "Stage 4.75: Assign Agents to Missions"
    runs-on: ubuntu-latest
    needs: [agent-missions, merge-mission-pr]
    if: always() && !cancelled() && needs.agent-missions.outputs.mission_count > 0 && inputs.skip_assignment != true
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install requests
      
      - name: Download mission assignments artifact
        uses: actions/download-artifact@v4
        with:
          name: mission-assignments
        continue-on-error: true
      
      - name: Assign agents to mission issues
        env:
          GH_TOKEN: ${{ secrets.COPILOT_PAT || secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REPOSITORY_OWNER: ${{ github.repository_owner }}
          GITHUB_REPOSITORY_NAME: ${{ github.event.repository.name }}
        run: |
          chmod +x tools/assign-agent-directly.sh
          
          echo "üéØ Stage 4.75: Assigning agents to mission issues"
          echo "=================================================="
          echo ""
          
          # Check if created_missions.json exists
          if [ ! -f created_missions.json ]; then
            echo "‚ö†Ô∏è  No created_missions.json found"
            echo "This means either:"
            echo "  1. No missions were created in Stage 4"
            echo "  2. The create_mission_issues.py script didn't run"
            echo ""
            echo "‚ÑπÔ∏è  Checking for mission issues to assign..."
            
            # Try to find recent mission issues without assignment
            RECENT_MISSION_ISSUES=$(gh issue list \
              --repo "$GITHUB_REPOSITORY" \
              --label "agent-mission" \
              --state open \
              --limit 20 \
              --json number,createdAt,labels \
              --jq '[.[] | select(.labels | map(.name) | contains(["agent-mission"]) and (contains(["copilot-assigned"]) | not))] | .[].number')
            
            if [ -z "$RECENT_MISSION_ISSUES" ]; then
              echo "‚ÑπÔ∏è  No unassigned mission issues found, skipping assignment"
              exit 0
            fi
            
            echo "Found unassigned mission issues: $RECENT_MISSION_ISSUES"
            echo "‚ö†Ô∏è  Cannot determine agent specialization without created_missions.json"
            echo "These issues will be picked up by the scheduled copilot-graphql-assign workflow"
            exit 0
          fi
          
          # Read created issues and assign agents
          echo "üìã Reading mission assignments from created_missions.json"
          
          # Count missions
          MISSION_COUNT=$(jq '. | length' created_missions.json)
          echo "Found $MISSION_COUNT missions to assign"
          echo ""
          
          # Track assignment results
          SUCCESS_COUNT=0
          FAILED_COUNT=0
          
          # Use jq to parse the JSON and iterate
          jq -r '.[] | "\(.issue_number) \(.agent_specialization)"' created_missions.json | while read -r issue_number agent_specialization; do
            if [ -n "$issue_number" ] && [ -n "$agent_specialization" ]; then
              echo ""
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
              echo "üéØ Assigning #$issue_number to @$agent_specialization"
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
              
              if ./tools/assign-agent-directly.sh "$issue_number" "$agent_specialization"; then
                echo "‚úÖ Successfully assigned #$issue_number to @$agent_specialization"
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              else
                echo "‚ùå Failed to assign #$issue_number to @$agent_specialization"
                FAILED_COUNT=$((FAILED_COUNT + 1))
                # Continue with other assignments even if one fails
              fi
            fi
          done
          
          echo ""
          echo "=================================================="
          echo "üìä Assignment Summary"
          echo "=================================================="
          echo "Total missions: $MISSION_COUNT"
          echo "Successfully assigned: $SUCCESS_COUNT"
          echo "Failed: $FAILED_COUNT"
          echo ""
          
          if [ $FAILED_COUNT -gt 0 ]; then
            echo "‚ö†Ô∏è  Some assignments failed. Common causes:"
            echo "  1. Missing COPILOT_PAT secret (default GITHUB_TOKEN can't assign Copilot)"
            echo "  2. Copilot not enabled for this repository"
            echo "  3. Network or API issues"
            echo ""
            echo "‚ÑπÔ∏è  Failed assignments will be retried by:"
            echo "  - Scheduled copilot-graphql-assign workflow (runs every 15 minutes)"
            echo "  - Manual workflow dispatch"
            echo ""
          fi
          
          echo "‚úÖ Stage 4.75 complete - Agent assignment process finished"

  # ============================================================================
  # STAGE 5: SELF-REINFORCEMENT (Optional)
  # ============================================================================
  
  self-reinforcement:
    name: "Stage 5: Self-Reinforcement (Optional)"
    runs-on: ubuntu-latest
    needs: [agent-missions, merge-mission-pr]
    if: always() && !cancelled() && inputs.include_self_reinforcement == true
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Collect insights
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üîÑ Collecting insights from completed work..."
          echo "‚ÑπÔ∏è  Self-reinforcement runs separately on daily schedule"
          echo "‚úÖ Pipeline complete"

  # ============================================================================
  # FINAL: PIPELINE SUMMARY
  # ============================================================================
  
  pipeline-summary:
    name: "Pipeline Summary"
    runs-on: ubuntu-latest
    needs: [combine-learnings, merge-learning-pr, world-update, merge-world-pr, agent-missions, merge-mission-pr, assign-agents-to-missions]
    if: always() && !cancelled()
    
    steps:
      - name: Log pipeline results
        run: |
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "üöÄ AUTONOMOUS LEARNING PIPELINE COMPLETE"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo ""
          echo "Stage 1: Learning Collection"
          echo "  Status: ‚úÖ Complete"
          echo ""
          echo "Stage 2: Combine Learnings"
          echo "  Total: ${{ needs.combine-learnings.outputs.total_learnings || 'N/A' }}"
          echo "  Status: ${{ needs.combine-learnings.result }}"
          echo ""
          echo "Stage 3: World Update"
          echo "  Tick: ${{ needs.world-update.outputs.world_tick || 'N/A' }}"
          echo "  Status: ${{ needs.world-update.result }}"
          echo ""
          echo "Stage 4: Agent Missions"
          echo "  Missions: ${{ needs.agent-missions.outputs.mission_count || 'N/A' }}"
          echo "  Status: ${{ needs.agent-missions.result }}"
          echo ""
          echo "Stage 4.75: Assign Agents to Missions"
          echo "  Status: ${{ needs.assign-agents-to-missions.result }}"
          echo ""
          echo "Pipeline Run: ${{ github.run_id }}"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
