name: "Learning: GitHub Copilot"

# This workflow fetches learning content about GitHub Copilot from multiple sources:
# 1. GitHub Docs (official documentation)
# 2. Reddit r/GithubCopilot (community discussions) - may be blocked by network
# 3. GitHub Discussions (community forum) - requires API token
on:
  schedule:
    # Run twice daily - morning and evening UTC
    - cron: '0 9,21 * * *'
  workflow_dispatch:
    inputs:
      docs_count:
        description: 'Number of documentation topics to fetch'
        required: false
        default: '5'
        type: string
      reddit_count:
        description: 'Number of Reddit posts to fetch'
        required: false
        default: '5'
        type: string
      discussions_count:
        description: 'Number of GitHub discussions to fetch'
        required: false
        default: '5'
        type: string

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  learn-from-copilot:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests lxml html5lib

      - name: Fetch GitHub Copilot content
        id: fetch
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DOCS_COUNT: ${{ inputs.docs_count || '5' }}
          REDDIT_COUNT: ${{ inputs.reddit_count || '5' }}
          DISCUSSIONS_COUNT: ${{ inputs.discussions_count || '5' }}
        run: |
          python3 tools/fetch-github-copilot.py \
            --docs ${DOCS_COUNT} \
            --reddit ${REDDIT_COUNT} \
            --discussions ${DISCUSSIONS_COUNT} \
            --output /tmp/copilot_raw.json
          
          # Extract metadata for GitHub Actions outputs
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          
          with open('/tmp/copilot_raw.json', 'r') as f:
              data = json.load(f)
          
          learnings = data.get('learnings', [])
          source_counts = data.get('source_counts', {})
          
          print(f"Fetched {len(learnings)} learnings")
          print(f"Source breakdown: {source_counts}")
          
          # Move to learnings directory
          from datetime import datetime
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          learning_file = f'learnings/copilot_{timestamp}.json'
          
          os.makedirs('learnings', exist_ok=True)
          with open(learning_file, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"Saved to: {learning_file}")
          
          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"has_learnings={'true' if learnings else 'false'}\n")
              f.write(f"learning_count={len(learnings)}\n")
              f.write(f"learning_file={learning_file}\n")
              f.write(f"docs_count={source_counts.get('GitHub Copilot Docs', 0)}\n")
              f.write(f"reddit_count={source_counts.get('Reddit r/GithubCopilot', 0)}\n")
              f.write(f"discussions_count={source_counts.get('GitHub Community Discussions', 0)}\n")
              if learnings:
                  # Get first learning as sample
                  sample = learnings[0].get('title', 'No title')
                  f.write(f"sample_learning={sample}\n")
          
          PYTHON_SCRIPT

      - name: Parse and clean learnings
        if: steps.fetch.outputs.has_learnings == 'true'
        id: parse
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import sys
          sys.path.insert(0, 'tools')
          
          # Import the intelligent parser
          import importlib.util
          spec = importlib.util.spec_from_file_location('parser', 'tools/intelligent-content-parser.py')
          parser_module = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(parser_module)
          
          parser = parser_module.IntelligentContentParser()
          
          # Load the learning file
          learning_file = os.environ.get('LEARNING_FILE', '')
          if not learning_file or not os.path.exists(learning_file):
              print("No learning file found")
              sys.exit(0)
          
          print(f"Parsing: {learning_file}")
          
          # Load and parse
          with open(learning_file, 'r') as f:
              data = json.load(f)
          
          learnings = data.get('learnings', [])
          print(f"Input learnings: {len(learnings)}")
          
          # Parse and clean
          cleaned, stats = parser.parse_learnings(learnings)
          
          print(f"Cleaned learnings: {len(cleaned)}")
          print(f"Acceptance rate: {stats['acceptance_rate']:.1%}")
          
          # Update data
          data['learnings'] = cleaned
          data['parsing_stats'] = stats
          data['parsed_version'] = '1.0'
          
          # Save cleaned version
          with open(learning_file, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"âœ“ Saved cleaned learnings to {learning_file}")
          
          # Output stats
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"cleaned_count={len(cleaned)}\n")
              f.write(f"rejected_count={stats['rejected']}\n")
              f.write(f"acceptance_rate={stats['acceptance_rate']:.1%}\n")
          
          PYTHON_SCRIPT
        env:
          LEARNING_FILE: ${{ steps.fetch.outputs.learning_file }}

      - name: Analyze trends
        if: steps.fetch.outputs.has_learnings == 'true'
        id: analyze
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import sys
          sys.path.insert(0, 'tools')
          
          # Import the thematic analyzer
          import importlib.util
          spec = importlib.util.spec_from_file_location('analyzer', 'tools/thematic-analyzer.py')
          analyzer_module = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(analyzer_module)
          
          analyzer = analyzer_module.ThematicAnalyzer(lookback_days=7)
          
          # Load learnings from directory
          learnings = analyzer.load_learnings_from_files('learnings')
          
          print(f"Analyzing {len(learnings)} learnings from last 7 days")
          
          # Perform analysis
          analysis = analyzer.analyze_learnings(learnings)
          
          # Save analysis
          os.makedirs('learnings', exist_ok=True)
          from datetime import datetime
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          analysis_file = f'learnings/analysis_{timestamp}.json'
          
          # Convert to dict
          from dataclasses import asdict
          analysis_data = asdict(analysis)
          
          with open(analysis_file, 'w') as f:
              json.dump(analysis_data, f, indent=2)
          
          print(f"âœ“ Saved analysis to {analysis_file}")
          
          # Output summary
          print(f"\nTop technologies:")
          for tech in analysis.top_technologies[:5]:
              print(f"  - {tech.name} (score: {tech.score:.1f}, mentions: {tech.mention_count})")
          
          print(f"\nHot themes for agent spawning:")
          for theme in analysis.hot_themes:
              print(f"  - {theme}")
          
          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"analysis_file={analysis_file}\n")
              f.write(f"hot_themes={','.join(analysis.hot_themes)}\n")
              f.write(f"theme_count={len(analysis.hot_themes)}\n")
          
          PYTHON_SCRIPT

      - name: Create learning issue
        if: steps.fetch.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create an issue documenting the learnings
          gh issue create \
            --title "ðŸ§  Learn from GitHub Copilot Sources - $(date +%Y-%m-%d)" \
            --body "## New Insights About GitHub Copilot

          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Total Learnings:** ${{ steps.fetch.outputs.learning_count }}
          **After Filtering:** ${{ steps.parse.outputs.cleaned_count }}
          **Acceptance Rate:** ${{ steps.parse.outputs.acceptance_rate }}
          **Hot Themes:** ${{ steps.analyze.outputs.theme_count }}

          ### ðŸ“Š Source Breakdown
          
          - **GitHub Copilot Docs:** ${{ steps.fetch.outputs.docs_count }} topics
          - **Reddit r/GithubCopilot:** ${{ steps.fetch.outputs.reddit_count }} posts
          - **GitHub Discussions:** ${{ steps.fetch.outputs.discussions_count }} threads

          ### ðŸ“š What Was Learned

          **Sample Learning:**
          ${{ steps.fetch.outputs.sample_learning }}

          This learning session collected insights from multiple GitHub Copilot sources:
          - ðŸ“– Official GitHub Copilot documentation
          - ðŸ’¬ Community discussions from Reddit
          - ðŸ—£ï¸ GitHub Community forum posts
          - ðŸŽ¯ Best practices and use cases
          - âš¡ Feature updates and announcements

          ### ðŸŽ¯ Hot Themes Identified

          ${{ steps.analyze.outputs.hot_themes }}

          These themes are being monitored for potential new agent creation and mission assignment.

          ### ðŸ”— Learning Resources

          - **Learning File:** [\`${{ steps.fetch.outputs.learning_file }}\`](https://github.com/enufacas/Chained/blob/main/${{ steps.fetch.outputs.learning_file }})
          - **Analysis File:** [\`${{ steps.analyze.outputs.analysis_file }}\`](https://github.com/enufacas/Chained/blob/main/${{ steps.analyze.outputs.analysis_file }})
          - **Learnings Directory:** [Browse all learnings](https://github.com/enufacas/Chained/tree/main/learnings)

          ### ðŸ“Š Key Insights

          The collected learnings include:
          - GitHub Copilot features and capabilities
          - Best practices for AI-assisted coding
          - Integration patterns and workflows
          - Community feedback and experiences
          - Tips and tricks from power users
          - Security and privacy considerations

          ### âœ¨ Quality Improvements

          - âœ… Fetched from multiple authoritative sources
          - âœ… Intelligently parsed and cleaned content
          - âœ… Validated content quality with ${{ steps.parse.outputs.acceptance_rate }} acceptance
          - âœ… Analyzed thematic trends across all sources

          ### ðŸŽ¯ Actions Taken
          - âœ… Fetched GitHub Copilot documentation
          - âœ… Collected Reddit community discussions
          - âœ… Gathered GitHub Discussions threads
          - âœ… Intelligently parsed and filtered content
          - âœ… Performed thematic analysis
          - âœ… Saved to learnings database
          - â³ Will generate missions for agents

          ### ðŸ”„ Next Steps
          These learnings will influence:
          - Agent mission creation focused on GitHub Copilot
          - Implementation of Copilot best practices
          - Exploration of advanced Copilot features
          - Integration patterns in the autonomous system
          - Documentation improvements

          ---

          *This learning was automatically collected by **@coordinate-wizard** from multiple GitHub Copilot sources. The AI continuously learns to improve its understanding of GitHub Copilot capabilities.*" \
            --label "learning,automated,copilot,github-copilot" || echo "Issue creation skipped or failed"

      - name: Create PR for learnings
        if: steps.fetch.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Rebuild learnings book
          echo "Rebuilding learnings book..."
          python3 tools/build-learnings-book.py || echo "Learnings book rebuild skipped"
          
          git add learnings/
          
          if git diff --staged --quiet; then
            echo "No new learnings to commit"
          else
            # Create a branch for this learning update
            TIMESTAMP=$(date +%Y%m%d-%H%M%S)
            BRANCH_NAME="learning/copilot-${TIMESTAMP}-${{ github.run_id }}"
            git checkout -b "${BRANCH_NAME}"
            
            git commit -m "ðŸ§  Learn from GitHub Copilot sources - $(date -u +%Y-%m-%d)

            Collected learnings from:
            - GitHub Copilot Docs: ${{ steps.fetch.outputs.docs_count }}
            - Reddit: ${{ steps.fetch.outputs.reddit_count }}
            - GitHub Discussions: ${{ steps.fetch.outputs.discussions_count }}

            Total: ${{ steps.fetch.outputs.learning_count }} learnings
            After filtering: ${{ steps.parse.outputs.cleaned_count }}

            Automated by @coordinate-wizard"
            
            git push origin "${BRANCH_NAME}"
            
            # Create PR
            gh pr create \
              --title "ðŸ§  Learning Update: GitHub Copilot - $(date -u +%Y-%m-%d)" \
              --body "## Automated Learning Update from GitHub Copilot Sources (@coordinate-wizard)
            
            **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
            **Total Learnings:** ${{ steps.fetch.outputs.learning_count }}
            **After Filtering:** ${{ steps.parse.outputs.cleaned_count }}
            **Acceptance Rate:** ${{ steps.parse.outputs.acceptance_rate }}
            
            ### ðŸ“Š Source Breakdown
            
            - **GitHub Copilot Docs:** ${{ steps.fetch.outputs.docs_count }} topics
            - **Reddit r/GithubCopilot:** ${{ steps.fetch.outputs.reddit_count }} posts  
            - **GitHub Discussions:** ${{ steps.fetch.outputs.discussions_count }} threads
            
            ### Summary
            
            **@coordinate-wizard** has orchestrated the collection of new learnings about GitHub Copilot from multiple authoritative sources including official documentation, community discussions, and user experiences.
            
            **Sample Learning:**
            ${{ steps.fetch.outputs.sample_learning }}
            
            ### Changes
            - âœ… Fetched latest GitHub Copilot documentation topics
            - âœ… Collected community discussions from Reddit
            - âœ… Gathered GitHub Discussions threads
            - âœ… Intelligently parsed and cleaned content
            - âœ… Performed thematic analysis
            - âœ… Updated learnings database
            - âœ… Rebuilt learnings book
            
            ### ðŸŽ¯ Mission Generation
            
            These learnings will automatically spawn agent missions through the autonomous pipeline, ensuring at least one mission related to GitHub Copilot best practices, features, or integration patterns.
            
            ---
            *This PR was automatically created by **@coordinate-wizard** via the GitHub Copilot Learning workflow.*" \
              --label "automated,learning,copilot,github-copilot" \
              --base main \
              --head "${BRANCH_NAME}"
            
            echo "âœ… PR created successfully for GitHub Copilot learnings"
          fi

      - name: Log learning activity
        if: always()
        run: |
          echo "=========================================="
          echo "GitHub Copilot Learning Complete"
          echo "=========================================="
          echo "Learnings collected: ${{ steps.fetch.outputs.learning_count || '0' }}"
          echo "Docs: ${{ steps.fetch.outputs.docs_count || '0' }}"
          echo "Reddit: ${{ steps.fetch.outputs.reddit_count || '0' }}"
          echo "Discussions: ${{ steps.fetch.outputs.discussions_count || '0' }}"
          echo "After filtering: ${{ steps.parse.outputs.cleaned_count || 'N/A' }}"
          echo "Hot themes: ${{ steps.analyze.outputs.theme_count || 'N/A' }}"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          echo "=========================================="
