name: "A/B Testing System"

# Consolidated A/B testing workflow
# Created by @workflows-tech-lead
#
# This workflow consolidates:
# - ab-testing-demo.yml: Integration demo for A/B testing
# - ab-testing-manager.yml: Experiment manager and analyzer
# - autonomous-ab-testing.yml: Autonomous experiment orchestrator
#
# Each stage can be triggered independently via workflow_dispatch

on:
  schedule:
    # Run demo every 6 hours (optimized based on A/B test)
    - cron: '0 */6 * * *'
    # Run manager daily for analysis
    - cron: '0 8 * * *'
    # Run autonomous orchestrator twice daily
    - cron: '0 6,18 * * *'
  
  workflow_dispatch:
    inputs:
      stage:
        description: 'Which A/B testing stage to run'
        required: true
        type: choice
        default: 'demo'
        options:
          - demo
          - manager
          - autonomous
      action:
        description: 'Action for manager/autonomous stages'
        required: false
        type: choice
        options:
          - analyze_all
          - list_experiments
          - create_example
          - auto_create_experiments
          - analyze_and_optimize
          - rollout_winners
        default: 'analyze_all'
      experiment_id:
        description: 'Experiment ID (for specific operations)'
        required: false
        type: string

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  # Stage 1: A/B Testing Integration Demo
  ab-testing-demo:
    name: "A/B Testing Demo"
    runs-on: ubuntu-latest
    # Run on schedule at */6 hours, or manual trigger with 'demo' stage
    if: |
      (github.event_name == 'schedule' && github.event.schedule == '0 */6 * * *') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.stage == 'demo')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Check for Active Experiment
        id: check_experiment
        run: |
          # Check if there's an active experiment for this workflow
          result=$(python3 tools/ab_testing_helper.py get-variant "ab-testing-demo" || echo '{"has_experiment": false}')
          
          echo "Experiment check result:"
          echo "$result" | jq '.'
          
          # Extract values for use in subsequent steps
          has_experiment=$(echo "$result" | jq -r '.has_experiment')
          experiment_id=$(echo "$result" | jq -r '.experiment_id // ""')
          variant=$(echo "$result" | jq -r '.variant // "control"')
          
          echo "has_experiment=${has_experiment}" >> $GITHUB_OUTPUT
          echo "experiment_id=${experiment_id}" >> $GITHUB_OUTPUT
          echo "variant=${variant}" >> $GITHUB_OUTPUT
          
          # Get variant configuration
          if [ "$has_experiment" == "true" ]; then
            config=$(echo "$result" | jq -c '.variant_config')
            echo "variant_config=${config}" >> $GITHUB_OUTPUT
            echo "âœ… Active experiment found, using variant: ${variant}"
          else
            echo "â„¹ï¸ No active experiment, using default configuration"
          fi

      - name: Apply Variant Configuration
        id: apply_config
        env:
          VARIANT: ${{ steps.check_experiment.outputs.variant }}
          VARIANT_CONFIG: ${{ steps.check_experiment.outputs.variant_config }}
        run: |
          echo "ðŸ”§ Applying configuration for variant: ${VARIANT}"
          
          if [ -n "${VARIANT_CONFIG}" ]; then
            echo "Configuration: ${VARIANT_CONFIG}"
            
            # Example: Extract configuration values
            timeout=$(echo "${VARIANT_CONFIG}" | jq -r '.timeout // 300')
            retries=$(echo "${VARIANT_CONFIG}" | jq -r '.max_retries // 3')
            
            echo "  Timeout: ${timeout}s"
            echo "  Max Retries: ${retries}"
            
            # Export for use in job
            echo "timeout=${timeout}" >> $GITHUB_OUTPUT
            echo "retries=${retries}" >> $GITHUB_OUTPUT
          else
            # Default configuration
            echo "  Using default configuration"
            echo "timeout=300" >> $GITHUB_OUTPUT
            echo "retries=3" >> $GITHUB_OUTPUT
          fi

      - name: Execute Workflow Task (Demo)
        id: task
        env:
          TIMEOUT: ${{ steps.apply_config.outputs.timeout }}
          RETRIES: ${{ steps.apply_config.outputs.retries }}
        run: |
          echo "ðŸš€ Executing task with configuration:"
          echo "  Timeout: ${TIMEOUT}s"
          echo "  Max Retries: ${RETRIES}"
          
          # Record start time
          start_time=$(date +%s)
          
          # Simulate task execution
          sleep 2
          
          # Calculate execution time
          end_time=$(date +%s)
          execution_time=$((end_time - start_time))
          
          # Simulate success rate (for demo, always 100%)
          success_rate=1.0
          
          # Record metrics for use in next step
          echo "execution_time=${execution_time}" >> $GITHUB_OUTPUT
          echo "success_rate=${success_rate}" >> $GITHUB_OUTPUT
          
          echo "âœ… Task completed in ${execution_time}s"

      - name: Record A/B Test Sample
        if: steps.check_experiment.outputs.has_experiment == 'true'
        env:
          EXPERIMENT_ID: ${{ steps.check_experiment.outputs.experiment_id }}
          VARIANT: ${{ steps.check_experiment.outputs.variant }}
          EXECUTION_TIME: ${{ steps.task.outputs.execution_time }}
          SUCCESS_RATE: ${{ steps.task.outputs.success_rate }}
        run: |
          echo "ðŸ“Š Recording A/B test sample"
          echo "  Experiment: ${EXPERIMENT_ID}"
          echo "  Variant: ${VARIANT}"
          echo "  Execution Time: ${EXECUTION_TIME}s"
          echo "  Success Rate: ${SUCCESS_RATE}"
          
          # Record the sample
          python3 tools/ab_testing_helper.py record \
            "${EXPERIMENT_ID}" \
            "${VARIANT}" \
            --metric execution_time="${EXECUTION_TIME}" \
            --metric success_rate="${SUCCESS_RATE}" \
            --metadata run_id="${{ github.run_id }}" \
            --metadata workflow="${{ github.workflow }}" \
            --metadata run_number="${{ github.run_number }}"
          
          echo "âœ… Sample recorded successfully"

      - name: Summary
        if: always()
        run: |
          echo "### ðŸ”¬ A/B Testing Demo Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.check_experiment.outputs.has_experiment }}" == "true" ]; then
            echo "**Status**: Active experiment detected" >> $GITHUB_STEP_SUMMARY
            echo "**Experiment ID**: ${{ steps.check_experiment.outputs.experiment_id }}" >> $GITHUB_STEP_SUMMARY
            echo "**Variant Used**: ${{ steps.check_experiment.outputs.variant }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Performance Metrics**:" >> $GITHUB_STEP_SUMMARY
            echo "- Execution Time: ${{ steps.task.outputs.execution_time }}s" >> $GITHUB_STEP_SUMMARY
            echo "- Success Rate: ${{ steps.task.outputs.success_rate }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Status**: No active experiment" >> $GITHUB_STEP_SUMMARY
            echo "Using default configuration" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "*Consolidated by @workflows-tech-lead - Stage: demo*" >> $GITHUB_STEP_SUMMARY

  # Stage 2: A/B Testing Manager
  experiment-manager:
    name: "Experiment Manager"
    runs-on: ubuntu-latest
    # Run on schedule at 8am daily, or manual trigger with 'manager' stage
    if: |
      (github.event_name == 'schedule' && github.event.schedule == '0 8 * * *') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.stage == 'manager')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: List Active Experiments
        id: list
        run: |
          python3 tools/ab_testing_engine.py list active > /tmp/active_experiments.json
          echo "Active Experiments:"
          cat /tmp/active_experiments.json
          
          # Count active experiments
          count=$(python3 -c "import json; data = json.load(open('/tmp/active_experiments.json')); print(len(data))")
          echo "experiment_count=${count}" >> $GITHUB_OUTPUT

      - name: Analyze Experiments
        if: github.event.inputs.action == 'analyze_all' || github.event.inputs.action == '' || github.event_name == 'schedule'
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          
          # Add tools to path
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Get all active experiments
          active_experiments = engine.list_experiments(status="active")
          
          print(f"ðŸ”¬ Analyzing {len(active_experiments)} active experiments...")
          
          results = []
          for exp_summary in active_experiments:
              exp_id = exp_summary["id"]
              exp_name = exp_summary["name"]
              
              print(f"\nðŸ“Š Analyzing: {exp_name} ({exp_id})")
              
              try:
                  analysis = engine.analyze_experiment(exp_id)
                  
                  if analysis["status"] == "insufficient_data":
                      print(f"  â³ Status: Insufficient data")
                      print(f"  ðŸ“ˆ Samples: {analysis['current_samples']}")
                  elif analysis["status"] == "analyzed":
                      print(f"  âœ… Status: Analysis complete")
                      
                      if analysis["winner"]:
                          winner_info = analysis["winner"]
                          print(f"  ðŸ† Winner: {winner_info['variant']}")
                          print(f"  ðŸ“ˆ Improvement: {winner_info['improvement']:.2%}")
                          print(f"  ðŸŽ¯ Confidence: {winner_info['confidence']}")
                          
                          results.append({
                              "experiment_id": exp_id,
                              "experiment_name": exp_name,
                              "winner": winner_info["variant"],
                              "improvement": winner_info["improvement"]
                          })
                      else:
                          print(f"  âš–ï¸  No clear winner yet")
              except Exception as e:
                  print(f"  âŒ Error analyzing experiment: {e}")
          
          # Save results
          if results:
              with open('/tmp/winning_experiments.json', 'w') as f:
                  json.dump(results, f, indent=2)
              print(f"\nâœ… Found {len(results)} experiments with clear winners")
          else:
              print(f"\nðŸ“Š No experiments have clear winners yet")
          PYTHON_SCRIPT

      - name: Create Example Experiment
        if: github.event.inputs.action == 'create_example'
        run: |
          python3 << 'PYTHON_SCRIPT'
          import sys
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Create an example experiment
          variants = {
              "control": {
                  "schedule": "*/15 * * * *",
                  "description": "Current 15-minute schedule"
              },
              "variant_a": {
                  "schedule": "*/10 * * * *",
                  "description": "More frequent 10-minute schedule"
              },
              "variant_b": {
                  "schedule": "*/20 * * * *",
                  "description": "Less frequent 20-minute schedule"
              }
          }
          
          exp_id = engine.create_experiment(
              name="Auto-Review Schedule Optimization",
              description="Testing different schedule frequencies for auto-review-merge workflow to optimize for resource usage vs responsiveness",
              variants=variants,
              metrics=["execution_time", "success_rate", "resource_usage"],
              workflow_name="auto-review-merge"
          )
          
          print(f"âœ… Created example experiment: {exp_id}")
          print(f"ðŸ“‹ Experiment: Auto-Review Schedule Optimization")
          print(f"ðŸ”¬ Variants: {len(variants)}")
          print(f"ðŸ“Š Metrics: execution_time, success_rate, resource_usage")
          PYTHON_SCRIPT

      - name: Report Results
        if: steps.list.outputs.experiment_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check if we have winning experiments
          if [ -f /tmp/winning_experiments.json ]; then
            winners=$(cat /tmp/winning_experiments.json)
            winner_count=$(echo "$winners" | python3 -c "import json, sys; print(len(json.load(sys.stdin)))")
            
            if [ "$winner_count" -gt 0 ]; then
              # Create issue to report winners
              issue_body=$(cat << 'EOF'
          ## ðŸ† A/B Testing Results Available
          
          **@engineer-master** has completed analysis of active A/B testing experiments.
          
          ### Winning Experiments
          
          The following experiments have clear winners based on statistical analysis:
          
          EOF
          )
              
              # Add winner details
              echo "$winners" | python3 << 'PYTHON_SCRIPT' >> /tmp/issue_body.md
          import json
          import sys
          
          winners = json.load(sys.stdin)
          
          for result in winners:
              exp_name = result["experiment_name"]
              winner = result["winner"]
              improvement = result["improvement"]
              
              print(f"#### {exp_name}")
              print(f"- **Winner**: `{winner}`")
              print(f"- **Improvement**: {improvement:.2%}")
              print(f"- **Recommendation**: Consider rolling out the winning configuration")
              print()
          PYTHON_SCRIPT
              
              cat /tmp/issue_body.md >> /tmp/full_issue_body.md
              echo "" >> /tmp/full_issue_body.md
              echo "**IMPORTANT**: Always mention **@engineer-master** by name when discussing these results." >> /tmp/full_issue_body.md
              echo "" >> /tmp/full_issue_body.md
              echo "*Generated by @workflows-tech-lead consolidation (stage: manager)*" >> /tmp/full_issue_body.md
              
              # Create the issue
              gh issue create \
                --title "ðŸ† A/B Testing Winners Detected" \
                --body-file /tmp/full_issue_body.md \
                --label "automated,ab-testing,ai-generated" \
                --repo ${{ github.repository }}
              
              echo "âœ… Created issue with A/B testing results"
            fi
          fi

      - name: Summary
        run: |
          echo "### ðŸ”¬ A/B Testing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Active Experiments**: ${{ steps.list.outputs.experiment_count }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f /tmp/active_experiments.json ]; then
            echo "#### Active Experiments" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/active_experiments.json', 'r') as f:
              experiments = json.load(f)
          
          if experiments:
              print("| Experiment | Status | Variants | Total Samples |")
              print("|------------|--------|----------|---------------|")
              for exp in experiments:
                  print(f"| {exp['name']} | {exp['status']} | {exp['variant_count']} | {exp['total_samples']} |")
          else:
              print("*No active experiments*")
          PYTHON_SCRIPT
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "*Consolidated by @workflows-tech-lead - Stage: manager*" >> $GITHUB_STEP_SUMMARY

  # Stage 3: Autonomous A/B Testing Orchestrator
  autonomous-orchestrator:
    name: "Autonomous Orchestrator"
    runs-on: ubuntu-latest
    # Run on schedule twice daily, or manual trigger with 'autonomous' stage
    if: |
      (github.event_name == 'schedule' && github.event.schedule == '0 6,18 * * *') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.stage == 'autonomous')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Identify Optimization Opportunities
        id: identify
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          import os
          from pathlib import Path
          
          # Add tools to path
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Analyze workflow performance history to identify opportunities
          opportunities = []
          
          # Check learning data for performance insights
          learnings_dir = Path("learnings")
          if learnings_dir.exists():
              recent_learnings = sorted(learnings_dir.glob("*.json"))[-5:]
              
              for learning_file in recent_learnings:
                  try:
                      with open(learning_file) as f:
                          data = json.load(f)
                          # Look for performance-related learnings
                          if "learnings" in data:
                              for item in data.get("learnings", []):
                                  title = item.get("title", "").lower()
                                  if any(keyword in title for keyword in ["performance", "optimization", "faster", "efficient"]):
                                      opportunities.append({
                                          "source": learning_file.name,
                                          "title": item.get("title"),
                                          "type": "performance_insight"
                                      })
                  except Exception as e:
                      print(f"Warning: Could not parse {learning_file}: {e}")
          
          # Check existing workflows for optimization candidates
          workflows_dir = Path(".github/workflows")
          scheduled_workflows = []
          
          for workflow_file in workflows_dir.glob("*.yml"):
              try:
                  with open(workflow_file) as f:
                      content = f.read()
                      # Check if workflow has schedule and could benefit from optimization
                      if "schedule:" in content and "cron:" in content:
                          workflow_name = workflow_file.stem
                          # Skip ab-testing workflows themselves
                          if "ab-testing" not in workflow_name and "test" not in workflow_name:
                              scheduled_workflows.append({
                                  "workflow": workflow_name,
                                  "file": str(workflow_file),
                                  "type": "schedule_optimization"
                              })
              except Exception as e:
                  print(f"Warning: Could not parse {workflow_file}: {e}")
          
          # Limit to top 3 candidates
          top_candidates = scheduled_workflows[:3]
          
          # Check for existing experiments to avoid duplicates
          active_experiments = engine.list_experiments(status="active")
          active_workflows = [exp.get("workflow_name") for exp in active_experiments]
          
          # Filter out workflows that already have active experiments
          new_candidates = [
              c for c in top_candidates 
              if c["workflow"] not in active_workflows
          ]
          
          print(f"ðŸ” Found {len(new_candidates)} optimization opportunities")
          for candidate in new_candidates:
              print(f"  - {candidate['workflow']}: {candidate['type']}")
          
          # Save for next step
          with open('/tmp/opportunities.json', 'w') as f:
              json.dump(new_candidates, f, indent=2)
          
          print(f"\nâœ… Identified {len(new_candidates)} new optimization opportunities")
          PYTHON_SCRIPT
          
          # Check if we have opportunities
          if [ -f /tmp/opportunities.json ]; then
            count=$(python3 -c "import json; print(len(json.load(open('/tmp/opportunities.json'))))")
            echo "opportunity_count=${count}" >> $GITHUB_OUTPUT
          else
            echo "opportunity_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Auto-Create Experiments
        if: |
          github.event.inputs.action == 'auto_create_experiments' ||
          github.event.inputs.action == '' ||
          github.event_name == 'schedule'
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path
          
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Load opportunities
          if not Path('/tmp/opportunities.json').exists():
              print("No opportunities to create experiments for")
              sys.exit(0)
          
          with open('/tmp/opportunities.json') as f:
              opportunities = json.load(f)
          
          if not opportunities:
              print("âœ… No new experiments to create")
              sys.exit(0)
          
          created_experiments = []
          
          for opp in opportunities[:2]:  # Limit to 2 new experiments at a time
              workflow = opp["workflow"]
              exp_type = opp["type"]
              
              print(f"\nðŸ”¬ Creating experiment for {workflow}")
              
              if exp_type == "schedule_optimization":
                  # Create schedule optimization experiment
                  variants = {
                      "control": {
                          "description": "Current schedule",
                          "schedule_factor": 1.0
                      },
                      "more_frequent": {
                          "description": "33% more frequent",
                          "schedule_factor": 0.75
                      },
                      "less_frequent": {
                          "description": "25% less frequent", 
                          "schedule_factor": 1.25
                      }
                  }
                  
                  try:
                      exp_id = engine.create_experiment(
                          name=f"Schedule Optimization: {workflow}",
                          description=f"Testing different schedule frequencies for {workflow} to optimize resource usage vs responsiveness",
                          variants=variants,
                          metrics=["execution_time", "success_rate", "resource_usage"],
                          workflow_name=workflow
                      )
                      
                      created_experiments.append({
                          "id": exp_id,
                          "workflow": workflow,
                          "type": exp_type
                      })
                      
                      print(f"âœ… Created experiment: {exp_id}")
                  except ValueError as e:
                      print(f"âš ï¸  Could not create experiment for {workflow}: {e}")
          
          if created_experiments:
              with open('/tmp/created_experiments.json', 'w') as f:
                  json.dump(created_experiments, f, indent=2)
              print(f"\nðŸŽ‰ Successfully created {len(created_experiments)} new experiments")
          PYTHON_SCRIPT

      - name: Analyze Active Experiments
        id: analyze
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Get all active experiments
          active_experiments = engine.list_experiments(status="active")
          
          print(f"ðŸ“Š Analyzing {len(active_experiments)} active experiments\n")
          
          winners = []
          insufficient_data = []
          no_winner = []
          
          for exp_summary in active_experiments:
              exp_id = exp_summary["id"]
              exp_name = exp_summary["name"]
              
              print(f"ðŸ”¬ {exp_name}")
              
              try:
                  analysis = engine.analyze_experiment(exp_id)
                  
                  if analysis["status"] == "insufficient_data":
                      current_samples = analysis['current_samples']
                      print(f"  â³ Insufficient data: {current_samples}")
                      insufficient_data.append({
                          "id": exp_id,
                          "name": exp_name,
                          "samples": current_samples
                      })
                  
                  elif analysis["status"] == "analyzed":
                      if analysis["winner"]:
                          winner_info = analysis["winner"]
                          print(f"  ðŸ† Winner: {winner_info['variant']}")
                          print(f"  ðŸ“ˆ Improvement: {winner_info['improvement']:.2%}")
                          
                          winners.append({
                              "id": exp_id,
                              "name": exp_name,
                              "winner": winner_info["variant"],
                              "improvement": winner_info["improvement"],
                              "workflow": exp_summary.get("workflow_name")
                          })
                      else:
                          print(f"  âš–ï¸  No clear winner yet")
                          no_winner.append({
                              "id": exp_id,
                              "name": exp_name
                          })
              except Exception as e:
                  print(f"  âŒ Error: {e}")
          
          # Save results
          results = {
              "winners": winners,
              "insufficient_data": insufficient_data,
              "no_winner": no_winner
          }
          
          with open('/tmp/analysis_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print(f"\nâœ… Analysis complete:")
          print(f"  ðŸ† Winners: {len(winners)}")
          print(f"  â³ Need more data: {len(insufficient_data)}")
          print(f"  âš–ï¸  No clear winner: {len(no_winner)}")
          PYTHON_SCRIPT
          
          # Set outputs
          if [ -f /tmp/analysis_results.json ]; then
            winners_count=$(python3 -c "import json; print(len(json.load(open('/tmp/analysis_results.json'))['winners']))")
            echo "winners_count=${winners_count}" >> $GITHUB_OUTPUT
          else
            echo "winners_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Auto-Rollout Winners
        if: steps.analyze.outputs.winners_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path
          
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          # Load analysis results
          with open('/tmp/analysis_results.json') as f:
              results = json.load(f)
          
          winners = results['winners']
          
          if not winners:
              print("No winners to roll out")
              sys.exit(0)
          
          print(f"ðŸš€ Processing {len(winners)} winning experiments\n")
          
          engine = ABTestingEngine()
          rollout_plans = []
          
          for winner in winners:
              exp_id = winner['id']
              exp_name = winner['name']
              winning_variant = winner['winner']
              improvement = winner['improvement']
              workflow = winner.get('workflow')
              
              print(f"ðŸ“‹ {exp_name}")
              print(f"  Winner: {winning_variant}")
              print(f"  Improvement: {improvement:.2%}")
              
              # Get full experiment details
              details = engine.get_experiment_details(exp_id)
              winner_config = details['variants'][winning_variant]['config']
              
              rollout_plans.append({
                  "experiment_id": exp_id,
                  "experiment_name": exp_name,
                  "workflow": workflow,
                  "winning_variant": winning_variant,
                  "config": winner_config,
                  "improvement": improvement
              })
              
              # Mark experiment as completed
              engine.complete_experiment(
                  experiment_id=exp_id,
                  winner=winning_variant,
                  notes=f"Auto-completed by autonomous A/B testing system. Winner showed {improvement:.2%} improvement."
              )
              
              print(f"  âœ… Marked as complete\n")
          
          # Save rollout plans
          with open('/tmp/rollout_plans.json', 'w') as f:
              json.dump(rollout_plans, f, indent=2)
          
          print(f"âœ… Prepared {len(rollout_plans)} rollout plans")
          PYTHON_SCRIPT

      - name: Create Rollout Issues
        if: steps.analyze.outputs.winners_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ ! -f /tmp/rollout_plans.json ]; then
            echo "No rollout plans found"
            exit 0
          fi
          
          # Create issues for each winner
          python3 << 'PYTHON_SCRIPT'
          import json
          import subprocess
          
          with open('/tmp/rollout_plans.json') as f:
              plans = json.load(f)
          
          for plan in plans:
              exp_name = plan['experiment_name']
              workflow = plan['workflow']
              winner = plan['winning_variant']
              improvement = plan['improvement']
              config = plan['config']
              
              # Create issue body
              issue_body = f"""## ðŸ† A/B Test Winner Ready for Rollout
          
          **@accelerate-specialist** has identified a winning configuration for automatic rollout.
          
          ### Experiment Details
          
          - **Experiment**: {exp_name}
          - **Workflow**: `{workflow}`
          - **Winner**: `{winner}`
          - **Improvement**: {improvement:.2%}
          
          ### Winning Configuration
          
          ```json
          {json.dumps(config, indent=2)}
          ```
          
          ### Recommended Actions
          
          1. **Review the winning configuration** to ensure it's appropriate
          2. **Update the workflow** with the winning configuration
          3. **Monitor performance** after rollout
          4. **Close this issue** once rollout is complete
          
          ### Auto-Rollout
          
          This experiment has been automatically completed by the autonomous A/B testing system. The winning configuration has been validated through statistical analysis and is ready for production deployment.
          
          ---
          
          *Generated by @workflows-tech-lead consolidation (stage: autonomous)*
          *Original system by @accelerate-specialist*
          """
              
              # Create the issue with label fallback
              title = f"ðŸš€ Rollout A/B Test Winner: {workflow} ({winner})"
              
              # Try with labels first
              result = subprocess.run([
                  'gh', 'issue', 'create',
                  '--title', title,
                  '--body', issue_body,
                  '--label', 'automated,ab-testing,optimization,accelerate-specialist'
              ], capture_output=True, text=True)
              
              if result.returncode == 0:
                  print(f"âœ… Created rollout issue for {workflow}")
              else:
                  # Retry without labels if label creation fails
                  print(f"âš ï¸  Issue creation with labels failed, retrying without labels...")
                  result_no_labels = subprocess.run([
                      'gh', 'issue', 'create',
                      '--title', title,
                      '--body', issue_body
                  ], capture_output=True, text=True)
                  
                  if result_no_labels.returncode == 0:
                      print(f"âœ… Created rollout issue for {workflow} (without labels)")
                  else:
                      print(f"âŒ Failed to create issue for {workflow}: {result_no_labels.stderr}")
          PYTHON_SCRIPT

      - name: Generate Summary
        if: always()
        run: |
          echo "## ðŸ”¬ Autonomous A/B Testing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Opportunities
          if [ -f /tmp/opportunities.json ]; then
            opp_count=$(python3 -c "import json; print(len(json.load(open('/tmp/opportunities.json'))))")
            echo "**Optimization Opportunities**: ${opp_count}" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Created experiments
          if [ -f /tmp/created_experiments.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ†• New Experiments Created" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/created_experiments.json') as f:
              experiments = json.load(f)
          
          if experiments:
              for exp in experiments:
                  print(f"- **{exp['workflow']}**: {exp['id']}")
          else:
              print("*No new experiments created*")
          PYTHON_SCRIPT
          fi
          
          # Analysis results
          if [ -f /tmp/analysis_results.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“Š Analysis Results" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/analysis_results.json') as f:
              results = json.load(f)
          
          print(f"- ðŸ† **Winners**: {len(results['winners'])}")
          print(f"- â³ **Need More Data**: {len(results['insufficient_data'])}")
          print(f"- âš–ï¸  **No Clear Winner**: {len(results['no_winner'])}")
          
          if results['winners']:
              print("\n#### Winners Ready for Rollout")
              for winner in results['winners']:
                  print(f"- **{winner['name']}**: {winner['winner']} ({winner['improvement']:.2%} improvement)")
          PYTHON_SCRIPT
          fi
          
          # Rollout plans
          if [ -f /tmp/rollout_plans.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸš€ Rollout Plans Created" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/rollout_plans.json') as f:
              plans = json.load(f)
          
          if plans:
              print(f"{len(plans)} experiments ready for rollout")
              for plan in plans:
                  print(f"- **{plan['workflow']}**: Using {plan['winning_variant']}")
          PYTHON_SCRIPT
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Consolidated by @workflows-tech-lead - Stage: autonomous*" >> $GITHUB_STEP_SUMMARY
