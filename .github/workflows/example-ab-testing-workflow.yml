name: Example Workflow with A/B Testing

# This workflow demonstrates how to integrate A/B testing
# using the new API and integration helpers.
#
# Author: @APIs-architect

on:
  workflow_dispatch:
    inputs:
      config_override:
        description: 'Optional config override (JSON)'
        required: false
        default: '{}'

permissions:
  contents: read

jobs:
  run_with_ab_testing:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Participate in A/B test
        id: ab_test
        run: |
          # Define default configuration
          DEFAULT_CONFIG='{"timeout": 300, "max_retries": 3, "batch_size": 100}'
          
          # Override with input if provided
          if [ "${{ github.event.inputs.config_override }}" != "{}" ]; then
            DEFAULT_CONFIG="${{ github.event.inputs.config_override }}"
          fi
          
          echo "ðŸ§ª Checking for active A/B test..."
          
          # Run participation script and capture JSON output with error handling
          set +e  # Don't exit on error, we'll handle it
          RESULT_JSON=$(python3 -c "
import sys
import json
sys.path.insert(0, 'tools')
try:
    from ab_testing_integration import WorkflowIntegration
    
    integration = WorkflowIntegration('example-workflow')
    config = integration.participate(json.loads('$DEFAULT_CONFIG'))
    
    result = {
        'config': config,
        'is_participating': integration._is_participating,
        'experiment_id': integration.experiment_id or '',
        'variant': integration.variant_name or 'default'
    }
    
    print(json.dumps(result))
except Exception as e:
    # Return error in JSON format for graceful handling
    error_result = {
        'config': json.loads('$DEFAULT_CONFIG'),
        'is_participating': false,
        'experiment_id': '',
        'variant': 'default',
        'error': str(e)
    }
    print(json.dumps(error_result), file=sys.stderr)
    sys.exit(1)
" 2>&1)
          PYTHON_EXIT_CODE=$?
          set -e  # Re-enable exit on error
          
          echo "$RESULT_JSON"
          
          # Check if Python script succeeded
          if [ $PYTHON_EXIT_CODE -ne 0 ]; then
            echo "âš ï¸  A/B testing integration failed, using default config"
            CONFIG='$DEFAULT_CONFIG'
            IS_PARTICIPATING="false"
            EXPERIMENT_ID=""
            VARIANT="default"
          else
            # Parse JSON result safely with error checking
            if echo "$RESULT_JSON" | jq -e . >/dev/null 2>&1; then
              CONFIG=$(echo "$RESULT_JSON" | jq -r '.config | @json')
              IS_PARTICIPATING=$(echo "$RESULT_JSON" | jq -r '.is_participating')
              EXPERIMENT_ID=$(echo "$RESULT_JSON" | jq -r '.experiment_id')
              VARIANT=$(echo "$RESULT_JSON" | jq -r '.variant')
            else
              echo "âš ï¸  Invalid JSON response, using default config"
              CONFIG='$DEFAULT_CONFIG'
              IS_PARTICIPATING="false"
              EXPERIMENT_ID=""
              VARIANT="default"
            fi
          fi
          
          echo "config=$CONFIG" >> $GITHUB_OUTPUT
          echo "is_participating=$IS_PARTICIPATING" >> $GITHUB_OUTPUT
          echo "experiment_id=$EXPERIMENT_ID" >> $GITHUB_OUTPUT
          echo "variant=$VARIANT" >> $GITHUB_OUTPUT
          
          if [ "$IS_PARTICIPATING" == "true" ]; then
            echo "âœ… Participating in experiment $EXPERIMENT_ID as variant $VARIANT"
          else
            echo "ðŸ“Œ Not participating in any experiment - using default config"
          fi
      
      - name: Run workflow task
        id: run_task
        continue-on-error: true  # Don't fail entire workflow if task fails
        run: |
          START_TIME=$(date +%s)
          
          echo "ðŸš€ Running workflow task..."
          
          # Run the example workflow with error handling
          if python3 examples/ab_testing_workflow_example.py; then
            EXIT_CODE=0
          else
            EXIT_CODE=$?
            echo "âš ï¸  Example workflow failed with exit code ${EXIT_CODE}"
          fi
          
          END_TIME=$(date +%s)
          EXECUTION_TIME=$((END_TIME - START_TIME))
          
          echo "execution_time=$EXECUTION_TIME" >> $GITHUB_OUTPUT
          echo "success=$([[ $EXIT_CODE -eq 0 ]] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… Task completed successfully in ${EXECUTION_TIME}s"
          else
            echo "âŒ Task failed after ${EXECUTION_TIME}s"
          fi
          
          # Don't exit with error code - let continue-on-error handle it
          exit 0
      
      - name: Record metrics (success)
        if: steps.ab_test.outputs.is_participating == 'true' && steps.run_task.outputs.success == 'true'
        continue-on-error: true  # Don't fail workflow if metrics recording fails
        run: |
          echo "ðŸ“Š Recording successful run metrics..."
          
          python3 tools/ab_testing_integration.py record example-workflow \
            --experiment-id "${{ steps.ab_test.outputs.experiment_id }}" \
            --variant "${{ steps.ab_test.outputs.variant }}" \
            --execution-time ${{ steps.run_task.outputs.execution_time }} \
            --success \
            --metrics '{"workflow_run": "${{ github.run_id }}"}'
          
          echo "âœ… Metrics recorded"
      
      - name: Record metrics (failure)
        if: steps.ab_test.outputs.is_participating == 'true' && steps.run_task.outputs.success != 'true'
        continue-on-error: true  # Don't fail workflow if metrics recording fails
        run: |
          echo "ðŸ“Š Recording failed run metrics..."
          
          python3 tools/ab_testing_integration.py record example-workflow \
            --experiment-id "${{ steps.ab_test.outputs.experiment_id }}" \
            --variant "${{ steps.ab_test.outputs.variant }}" \
            --execution-time ${{ steps.run_task.outputs.execution_time || 0 }} \
            --error "Workflow task failed" \
            --metrics '{"workflow_run": "${{ github.run_id }}"}'
          
          echo "ðŸ“Š Metrics recorded"
      
      - name: Summary
        if: always()
        run: |
          echo "## A/B Testing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.ab_test.outputs.is_participating }}" == "true" ]; then
            echo "ðŸ§ª **Experiment**: ${{ steps.ab_test.outputs.experiment_id }}" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ“Š **Variant**: ${{ steps.ab_test.outputs.variant }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "ðŸ“Œ **No active experiment** - using default configuration" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "â±ï¸ **Execution Time**: ${{ steps.run_task.outputs.execution_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Status**: ${{ steps.run_task.outputs.success == 'true' && 'Success' || 'Failed' }}" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.ab_test.outputs.is_participating }}" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ“ˆ Metrics have been recorded for A/B test analysis" >> $GITHUB_STEP_SUMMARY
          fi
