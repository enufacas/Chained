name: "Autonomous A/B Testing Orchestrator"

# This workflow autonomously creates, manages, and learns from A/B testing experiments
# Author: @accelerate-specialist

on:
  schedule:
    # Run twice daily to manage experiments
    - cron: '0 6,18 * * *'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - auto_create_experiments
          - analyze_and_optimize
          - rollout_winners
        default: 'analyze_and_optimize'

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  orchestrate-ab-testing:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Identify Optimization Opportunities
        id: identify
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          import os
          from pathlib import Path
          
          # Add tools to path
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Analyze workflow performance history to identify opportunities
          opportunities = []
          
          # Check learning data for performance insights
          learnings_dir = Path("learnings")
          if learnings_dir.exists():
              recent_learnings = sorted(learnings_dir.glob("*.json"))[-5:]
              
              for learning_file in recent_learnings:
                  try:
                      with open(learning_file) as f:
                          data = json.load(f)
                          # Look for performance-related learnings
                          if "learnings" in data:
                              for item in data.get("learnings", []):
                                  title = item.get("title", "").lower()
                                  if any(keyword in title for keyword in ["performance", "optimization", "faster", "efficient"]):
                                      opportunities.append({
                                          "source": learning_file.name,
                                          "title": item.get("title"),
                                          "type": "performance_insight"
                                      })
                  except Exception as e:
                      print(f"Warning: Could not parse {learning_file}: {e}")
          
          # Check existing workflows for optimization candidates
          workflows_dir = Path(".github/workflows")
          scheduled_workflows = []
          
          for workflow_file in workflows_dir.glob("*.yml"):
              try:
                  with open(workflow_file) as f:
                      content = f.read()
                      # Check if workflow has schedule and could benefit from optimization
                      if "schedule:" in content and "cron:" in content:
                          workflow_name = workflow_file.stem
                          # Skip ab-testing workflows themselves
                          if "ab-testing" not in workflow_name and "test" not in workflow_name:
                              scheduled_workflows.append({
                                  "workflow": workflow_name,
                                  "file": str(workflow_file),
                                  "type": "schedule_optimization"
                              })
              except Exception as e:
                  print(f"Warning: Could not parse {workflow_file}: {e}")
          
          # Limit to top 3 candidates
          top_candidates = scheduled_workflows[:3]
          
          # Check for existing experiments to avoid duplicates
          active_experiments = engine.list_experiments(status="active")
          active_workflows = [exp.get("workflow_name") for exp in active_experiments]
          
          # Filter out workflows that already have active experiments
          new_candidates = [
              c for c in top_candidates 
              if c["workflow"] not in active_workflows
          ]
          
          print(f"ðŸ” Found {len(new_candidates)} optimization opportunities")
          for candidate in new_candidates:
              print(f"  - {candidate['workflow']}: {candidate['type']}")
          
          # Save for next step
          with open('/tmp/opportunities.json', 'w') as f:
              json.dump(new_candidates, f, indent=2)
          
          print(f"\nâœ… Identified {len(new_candidates)} new optimization opportunities")
          PYTHON_SCRIPT
          
          # Check if we have opportunities
          if [ -f /tmp/opportunities.json ]; then
            count=$(python3 -c "import json; print(len(json.load(open('/tmp/opportunities.json'))))")
            echo "opportunity_count=${count}" >> $GITHUB_OUTPUT
          else
            echo "opportunity_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Auto-Create Experiments
        if: github.event.inputs.action == 'auto_create_experiments' || github.event_name == 'schedule'
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path
          
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Load opportunities
          if not Path('/tmp/opportunities.json').exists():
              print("No opportunities to create experiments for")
              sys.exit(0)
          
          with open('/tmp/opportunities.json') as f:
              opportunities = json.load(f)
          
          if not opportunities:
              print("âœ… No new experiments to create")
              sys.exit(0)
          
          created_experiments = []
          
          for opp in opportunities[:2]:  # Limit to 2 new experiments at a time
              workflow = opp["workflow"]
              exp_type = opp["type"]
              
              print(f"\nðŸ”¬ Creating experiment for {workflow}")
              
              if exp_type == "schedule_optimization":
                  # Create schedule optimization experiment
                  variants = {
                      "control": {
                          "description": "Current schedule",
                          "schedule_factor": 1.0
                      },
                      "more_frequent": {
                          "description": "33% more frequent",
                          "schedule_factor": 0.75
                      },
                      "less_frequent": {
                          "description": "25% less frequent", 
                          "schedule_factor": 1.25
                      }
                  }
                  
                  try:
                      exp_id = engine.create_experiment(
                          name=f"Schedule Optimization: {workflow}",
                          description=f"Testing different schedule frequencies for {workflow} to optimize resource usage vs responsiveness",
                          variants=variants,
                          metrics=["execution_time", "success_rate", "resource_usage"],
                          workflow_name=workflow
                      )
                      
                      created_experiments.append({
                          "id": exp_id,
                          "workflow": workflow,
                          "type": exp_type
                      })
                      
                      print(f"âœ… Created experiment: {exp_id}")
                  except ValueError as e:
                      print(f"âš ï¸  Could not create experiment for {workflow}: {e}")
          
          if created_experiments:
              with open('/tmp/created_experiments.json', 'w') as f:
                  json.dump(created_experiments, f, indent=2)
              print(f"\nðŸŽ‰ Successfully created {len(created_experiments)} new experiments")
          PYTHON_SCRIPT

      - name: Analyze Active Experiments
        id: analyze
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          engine = ABTestingEngine()
          
          # Get all active experiments
          active_experiments = engine.list_experiments(status="active")
          
          print(f"ðŸ“Š Analyzing {len(active_experiments)} active experiments\n")
          
          winners = []
          insufficient_data = []
          no_winner = []
          
          for exp_summary in active_experiments:
              exp_id = exp_summary["id"]
              exp_name = exp_summary["name"]
              
              print(f"ðŸ”¬ {exp_name}")
              
              try:
                  analysis = engine.analyze_experiment(exp_id)
                  
                  if analysis["status"] == "insufficient_data":
                      current_samples = analysis['current_samples']
                      print(f"  â³ Insufficient data: {current_samples}")
                      insufficient_data.append({
                          "id": exp_id,
                          "name": exp_name,
                          "samples": current_samples
                      })
                  
                  elif analysis["status"] == "analyzed":
                      if analysis["winner"]:
                          winner_info = analysis["winner"]
                          print(f"  ðŸ† Winner: {winner_info['variant']}")
                          print(f"  ðŸ“ˆ Improvement: {winner_info['improvement']:.2%}")
                          
                          winners.append({
                              "id": exp_id,
                              "name": exp_name,
                              "winner": winner_info["variant"],
                              "improvement": winner_info["improvement"],
                              "workflow": exp_summary.get("workflow_name")
                          })
                      else:
                          print(f"  âš–ï¸  No clear winner yet")
                          no_winner.append({
                              "id": exp_id,
                              "name": exp_name
                          })
              except Exception as e:
                  print(f"  âŒ Error: {e}")
          
          # Save results
          results = {
              "winners": winners,
              "insufficient_data": insufficient_data,
              "no_winner": no_winner
          }
          
          with open('/tmp/analysis_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print(f"\nâœ… Analysis complete:")
          print(f"  ðŸ† Winners: {len(winners)}")
          print(f"  â³ Need more data: {len(insufficient_data)}")
          print(f"  âš–ï¸  No clear winner: {len(no_winner)}")
          PYTHON_SCRIPT
          
          # Set outputs
          if [ -f /tmp/analysis_results.json ]; then
            winners_count=$(python3 -c "import json; print(len(json.load(open('/tmp/analysis_results.json'))['winners']))")
            echo "winners_count=${winners_count}" >> $GITHUB_OUTPUT
          else
            echo "winners_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Auto-Rollout Winners
        if: steps.analyze.outputs.winners_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path
          
          sys.path.insert(0, 'tools')
          from ab_testing_engine import ABTestingEngine
          
          # Load analysis results
          with open('/tmp/analysis_results.json') as f:
              results = json.load(f)
          
          winners = results['winners']
          
          if not winners:
              print("No winners to roll out")
              sys.exit(0)
          
          print(f"ðŸš€ Processing {len(winners)} winning experiments\n")
          
          engine = ABTestingEngine()
          rollout_plans = []
          
          for winner in winners:
              exp_id = winner['id']
              exp_name = winner['name']
              winning_variant = winner['winner']
              improvement = winner['improvement']
              workflow = winner.get('workflow')
              
              print(f"ðŸ“‹ {exp_name}")
              print(f"  Winner: {winning_variant}")
              print(f"  Improvement: {improvement:.2%}")
              
              # Get full experiment details
              details = engine.get_experiment_details(exp_id)
              winner_config = details['variants'][winning_variant]['config']
              
              rollout_plans.append({
                  "experiment_id": exp_id,
                  "experiment_name": exp_name,
                  "workflow": workflow,
                  "winning_variant": winning_variant,
                  "config": winner_config,
                  "improvement": improvement
              })
              
              # Mark experiment as completed
              engine.complete_experiment(
                  experiment_id=exp_id,
                  winner=winning_variant,
                  notes=f"Auto-completed by autonomous A/B testing system. Winner showed {improvement:.2%} improvement."
              )
              
              print(f"  âœ… Marked as complete\n")
          
          # Save rollout plans
          with open('/tmp/rollout_plans.json', 'w') as f:
              json.dump(rollout_plans, f, indent=2)
          
          print(f"âœ… Prepared {len(rollout_plans)} rollout plans")
          PYTHON_SCRIPT

      - name: Create Rollout Issues
        if: steps.analyze.outputs.winners_count > 0
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ ! -f /tmp/rollout_plans.json ]; then
            echo "No rollout plans found"
            exit 0
          fi
          
          # Create issues for each winner
          python3 << 'PYTHON_SCRIPT'
          import json
          import subprocess
          
          with open('/tmp/rollout_plans.json') as f:
              plans = json.load(f)
          
          for plan in plans:
              exp_name = plan['experiment_name']
              workflow = plan['workflow']
              winner = plan['winning_variant']
              improvement = plan['improvement']
              config = plan['config']
              
              # Create issue body
              issue_body = f"""## ðŸ† A/B Test Winner Ready for Rollout

**@accelerate-specialist** has identified a winning configuration for automatic rollout.

### Experiment Details

- **Experiment**: {exp_name}
- **Workflow**: `{workflow}`
- **Winner**: `{winner}`
- **Improvement**: {improvement:.2%}

### Winning Configuration

```json
{json.dumps(config, indent=2)}
```

### Recommended Actions

1. **Review the winning configuration** to ensure it's appropriate
2. **Update the workflow** with the winning configuration
3. **Monitor performance** after rollout
4. **Close this issue** once rollout is complete

### Auto-Rollout

This experiment has been automatically completed by the autonomous A/B testing system. The winning configuration has been validated through statistical analysis and is ready for production deployment.

---

*Generated by **@accelerate-specialist** Autonomous A/B Testing System*
"""
              
              # Create the issue
              title = f"ðŸš€ Rollout A/B Test Winner: {workflow} ({winner})"
              
              result = subprocess.run([
                  'gh', 'issue', 'create',
                  '--title', title,
                  '--body', issue_body,
                  '--label', 'automated,ab-testing,optimization,accelerate-specialist'
              ], capture_output=True, text=True)
              
              if result.returncode == 0:
                  print(f"âœ… Created rollout issue for {workflow}")
              else:
                  print(f"âš ï¸  Failed to create issue for {workflow}: {result.stderr}")
          PYTHON_SCRIPT

      - name: Generate Summary
        if: always()
        run: |
          echo "## ðŸ”¬ Autonomous A/B Testing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Opportunities
          if [ -f /tmp/opportunities.json ]; then
            opp_count=$(python3 -c "import json; print(len(json.load(open('/tmp/opportunities.json'))))")
            echo "**Optimization Opportunities**: ${opp_count}" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Created experiments
          if [ -f /tmp/created_experiments.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ†• New Experiments Created" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/created_experiments.json') as f:
              experiments = json.load(f)
          
          if experiments:
              for exp in experiments:
                  print(f"- **{exp['workflow']}**: {exp['id']}")
          else:
              print("*No new experiments created*")
          PYTHON_SCRIPT
          fi
          
          # Analysis results
          if [ -f /tmp/analysis_results.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“Š Analysis Results" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/analysis_results.json') as f:
              results = json.load(f)
          
          print(f"- ðŸ† **Winners**: {len(results['winners'])}")
          print(f"- â³ **Need More Data**: {len(results['insufficient_data'])}")
          print(f"- âš–ï¸  **No Clear Winner**: {len(results['no_winner'])}")
          
          if results['winners']:
              print("\n#### Winners Ready for Rollout")
              for winner in results['winners']:
                  print(f"- **{winner['name']}**: {winner['winner']} ({winner['improvement']:.2%} improvement)")
          PYTHON_SCRIPT
          fi
          
          # Rollout plans
          if [ -f /tmp/rollout_plans.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸš€ Rollout Plans Created" >> $GITHUB_STEP_SUMMARY
            python3 << 'PYTHON_SCRIPT' >> $GITHUB_STEP_SUMMARY
          import json
          
          with open('/tmp/rollout_plans.json') as f:
              plans = json.load(f)
          
          if plans:
              print(f"{len(plans)} experiments ready for rollout")
              for plan in plans:
                  print(f"- **{plan['workflow']}**: Using {plan['winning_variant']}")
          PYTHON_SCRIPT
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Autonomous optimization by **@accelerate-specialist***" >> $GITHUB_STEP_SUMMARY
