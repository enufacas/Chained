name: "Learning: TLDR Tech"

on:
  schedule:
    # Run twice daily - morning and evening
    - cron: '0 8,20 * * *'
  workflow_dispatch:

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  learn-from-tldr:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests feedparser lxml html5lib

      - name: Fetch and analyze TLDR content
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          import re
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup
          import os
          import time
          
          print("Fetching TLDR Tech content...")
          
          # Web content fetcher (embedded)
          class WebContentFetcher:
              def __init__(self):
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Mozilla/5.0 (compatible; ChainedAI/1.0)'
                  })
              
              def fetch(self, url):
                  try:
                      response = self.session.get(url, timeout=10, allow_redirects=True)
                      if response.status_code == 200:
                          soup = BeautifulSoup(response.text, 'html.parser')
                          
                          # Remove unwanted elements
                          for element in soup(['script', 'style', 'nav', 'footer', 'iframe']):
                              element.decompose()
                          
                          # Try to find main content
                          content = soup.find('article') or soup.find('main') or soup.find('body')
                          if content:
                              text = content.get_text(separator='\n', strip=True)
                              lines = [line.strip() for line in text.split('\n') if line.strip()]
                              text = '\n'.join(lines)
                              if len(text) > 2000:
                                  text = text[:2000] + '\n\n[Content truncated...]'
                              return text
                  except Exception as e:
                      print(f"  Warning: Could not fetch content from {url}: {e}")
                  return None
          
          fetcher = WebContentFetcher()
          
          # TLDR has an RSS feed we can use
          rss_urls = [
              'https://tldr.tech/api/rss/tech',
              'https://tldr.tech/api/rss/ai',
              'https://tldr.tech/api/rss/devops'
          ]
          
          learnings = []
          tech_trends = []
          
          for rss_url in rss_urls:
              try:
                  response = requests.get(rss_url, timeout=10)
                  if response.status_code == 200:
                      # Parse RSS content
                      from xml.etree import ElementTree as ET
                      root = ET.fromstring(response.content)
                      
                      for item in root.findall('.//item')[:5]:  # Top 5 items
                          title = item.find('title')
                          desc = item.find('description')
                          link = item.find('link')
                          
                          if title is not None and title.text:
                              learning = {
                                  'title': title.text,
                                  'description': desc.text if desc is not None else '',
                                  'source': 'TLDR'
                              }
                              
                              # Try to fetch actual content from link
                              if link is not None and link.text:
                                  learning['url'] = link.text
                                  print(f"  Fetching content from: {link.text[:60]}...")
                                  content = fetcher.fetch(link.text)
                                  if content:
                                      learning['content'] = content
                                      print(f"  âœ“ Fetched content ({len(content)} chars)")
                                  time.sleep(0.5)  # Rate limiting
                              
                              learnings.append(learning)
                              
                              # Extract tech trends
                              title_lower = title.text.lower()
                              if any(word in title_lower for word in ['ai', 'ml', 'llm', 'gpt', 'copilot']):
                                  tech_trends.append(f"AI/ML: {title.text}")
                              elif any(word in title_lower for word in ['github', 'git', 'devops', 'ci/cd']):
                                  tech_trends.append(f"DevOps: {title.text}")
                              elif any(word in title_lower for word in ['python', 'javascript', 'rust', 'go']):
                                  tech_trends.append(f"Programming: {title.text}")
                      
                      print(f"âœ“ Fetched from {rss_url}")
              except Exception as e:
                  print(f"âœ— Error fetching {rss_url}: {e}")
          
          # Save learnings
          os.makedirs('learnings', exist_ok=True)
          now = datetime.now(timezone.utc)
          timestamp = now.strftime('%Y%m%d_%H%M%S')
          
          with open(f'learnings/tldr_{timestamp}.json', 'w') as f:
              json.dump({
                  'timestamp': now.isoformat(),
                  'source': 'TLDR Tech',
                  'learnings': learnings,
                  'trends': tech_trends
              }, f, indent=2)
          
          print(f"Saved {len(learnings)} learnings and {len(tech_trends)} trends")
          
          # Output for GitHub Actions
          if learnings:
              first_learning = learnings[0]['title']
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"has_learnings=true\n")
                  f.write(f"learning_count={len(learnings)}\n")
                  f.write(f"trend_count={len(tech_trends)}\n")
                  f.write(f"sample_learning={first_learning}\n")
                  f.write(f"learning_file=learnings/tldr_{timestamp}.json\n")
          else:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"has_learnings=false\n")
          
          PYTHON_SCRIPT

      - name: Parse and clean learnings
        id: parse
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import sys
          sys.path.insert(0, 'tools')
          
          # Import the intelligent parser
          import importlib.util
          spec = importlib.util.spec_from_file_location('parser', 'tools/intelligent-content-parser.py')
          parser_module = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(parser_module)
          
          parser = parser_module.IntelligentContentParser()
          
          # Find the latest learning file
          learning_file = os.environ.get('LEARNING_FILE', '')
          if not learning_file or not os.path.exists(learning_file):
              # Find most recent tldr file
              import glob
              files = glob.glob('learnings/tldr_*.json')
              if files:
                  learning_file = max(files, key=os.path.getctime)
              else:
                  print("No learning file found")
                  sys.exit(0)
          
          print(f"Parsing: {learning_file}")
          
          # Load and parse
          with open(learning_file, 'r') as f:
              data = json.load(f)
          
          learnings = data.get('learnings', [])
          print(f"Input learnings: {len(learnings)}")
          
          # Parse and clean
          cleaned, stats = parser.parse_learnings(learnings)
          
          print(f"Cleaned learnings: {len(cleaned)}")
          print(f"Acceptance rate: {stats['acceptance_rate']:.1%}")
          
          # Update data
          data['learnings'] = cleaned
          data['parsing_stats'] = stats
          data['parsed_version'] = '1.0'
          
          # Save cleaned version
          with open(learning_file, 'w') as f:
              json.dump(data, f, indent=2)
          
          print(f"âœ“ Saved cleaned learnings to {learning_file}")
          
          # Output stats
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"cleaned_count={len(cleaned)}\n")
              f.write(f"rejected_count={stats['rejected']}\n")
              f.write(f"acceptance_rate={stats['acceptance_rate']:.1%}\n")
          
          PYTHON_SCRIPT

      - name: Analyze trends
        id: analyze
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          import sys
          sys.path.insert(0, 'tools')
          
          # Import the thematic analyzer
          import importlib.util
          spec = importlib.util.spec_from_file_location('analyzer', 'tools/thematic-analyzer.py')
          analyzer_module = importlib.util.module_from_spec(spec)
          spec.loader.exec_module(analyzer_module)
          
          analyzer = analyzer_module.ThematicAnalyzer(lookback_days=7)
          
          # Load learnings from directory
          learnings = analyzer.load_learnings_from_files('learnings')
          
          print(f"Analyzing {len(learnings)} learnings from last 7 days")
          
          # Perform analysis
          analysis = analyzer.analyze_learnings(learnings)
          
          # Save analysis
          os.makedirs('learnings', exist_ok=True)
          from datetime import datetime
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          analysis_file = f'learnings/analysis_{timestamp}.json'
          
          # Convert to dict
          from dataclasses import asdict
          analysis_data = asdict(analysis)
          
          with open(analysis_file, 'w') as f:
              json.dump(analysis_data, f, indent=2)
          
          print(f"âœ“ Saved analysis to {analysis_file}")
          
          # Output summary
          print(f"\nTop technologies:")
          for tech in analysis.top_technologies[:5]:
              print(f"  - {tech.name} (score: {tech.score:.1f}, mentions: {tech.mention_count})")
          
          print(f"\nHot themes for agent spawning:")
          for theme in analysis.hot_themes:
              print(f"  - {theme}")
          
          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"analysis_file={analysis_file}\n")
              f.write(f"hot_themes={','.join(analysis.hot_themes)}\n")
              f.write(f"theme_count={len(analysis.hot_themes)}\n")
          
          PYTHON_SCRIPT

      - name: Create learning issue
        if: steps.fetch.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create an issue documenting the learnings
          gh issue create \
            --title "ðŸ§  Learn from TLDR Tech - $(date +%Y-%m-%d)" \
            --body "## New Tech Insights from TLDR Tech

          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Learnings Collected:** ${{ steps.fetch.outputs.learning_count }}
          **After Filtering:** ${{ steps.parse.outputs.cleaned_count }}
          **Acceptance Rate:** ${{ steps.parse.outputs.acceptance_rate }}
          **Trends Identified:** ${{ steps.fetch.outputs.trend_count }}
          **Hot Themes:** ${{ steps.analyze.outputs.theme_count }}

          ### ðŸ“š What Was Learned

          **Sample Learning:**
          ${{ steps.fetch.outputs.sample_learning }}

          This learning session collected insights from TLDR Tech newsletters covering:
          - ðŸ¤– AI/ML developments and LLM advancements
          - ðŸš€ DevOps practices and CI/CD innovations
          - ðŸ’» Programming languages and framework updates
          - ðŸ”§ Developer tools and productivity enhancements

          ### ðŸŽ¯ Hot Themes Identified

          ${{ steps.analyze.outputs.hot_themes }}

          These themes are being monitored for potential new agent creation.

          ### ðŸ”— Learning Resources

          - **Learning File:** [\`${{ steps.fetch.outputs.learning_file }}\`](https://github.com/enufacas/Chained/blob/main/${{ steps.fetch.outputs.learning_file }})
          - **Analysis File:** [\`${{ steps.analyze.outputs.analysis_file }}\`](https://github.com/enufacas/Chained/blob/main/${{ steps.analyze.outputs.analysis_file }})
          - **Learnings Directory:** [Browse all learnings](https://github.com/enufacas/Chained/tree/main/learnings)
          - **Learnings Documentation:** [README](https://github.com/enufacas/Chained/blob/main/learnings/README.md)

          ### ðŸ“Š Key Insights

          The collected learnings include:
          - Technology trends and emerging patterns
          - Best practices from industry leaders
          - Security updates and vulnerability alerts
          - Performance optimization techniques
          - New tool and framework releases

          ### âœ¨ Quality Improvements

          - âœ… Filtered out sponsor and promotional content
          - âœ… Cleaned malformed emojis and special characters
          - âœ… Validated content quality with ${{ steps.parse.outputs.acceptance_rate }} acceptance
          - âœ… Analyzed thematic trends across all sources

          ### ðŸŽ¯ Actions Taken
          - âœ… Fetched latest tech news from TLDR Tech
          - âœ… Intelligently parsed and filtered content
          - âœ… Extracted key learnings and trends
          - âœ… Performed thematic analysis
          - âœ… Saved to learnings database
          - âœ… Categorized by topic (AI/ML, DevOps, Programming)
          - â³ Will incorporate into next idea generation cycle

          ### ðŸ”„ Next Steps
          These learnings will influence:
          - Future idea generation and prioritization
          - Technology choices in implementations
          - Best practices adoption
          - Feature development approaches
          - Potential spawning of specialized agents

          ---

          *This learning was automatically collected by the TLDR Learning workflow with intelligent content parsing. The AI continuously learns from external sources to stay current with technology trends.*" \
            --label "learning,automated,copilot" || echo "Issue creation skipped or failed"

      - name: Create PR for learnings
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Rebuild learnings book
          echo "Rebuilding learnings book..."
          python3 tools/build-learnings-book.py
          
          git add learnings/
          
          if git diff --staged --quiet; then
            echo "No new learnings to commit"
          else
            # Create a branch for this learning update
            branch_name="learning/tldr-$(date +%Y%m%d-%H%M%S)"
            git checkout -b "${branch_name}"
            
            git commit -m "ðŸ§  Learn from TLDR Tech - $(date -u +%Y-%m-%d)"
            git push origin "${branch_name}"
            
            # Create PR
            gh pr create \
              --title "ðŸ§  Learning Update: TLDR Tech - $(date -u +%Y-%m-%d)" \
              --body "## Automated Learning Update from TLDR Tech
            
            **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
            **Learnings Collected:** ${{ steps.fetch.outputs.learning_count }}
            **Trends Identified:** ${{ steps.fetch.outputs.trend_count }}
            
            ### Summary
            This PR adds new learnings collected from TLDR Tech newsletters with full article content.
            
            **Sample Learning:**
            ${{ steps.fetch.outputs.sample_learning }}
            
            ### Changes
            - Updated learnings database with latest tech insights
            - Extracted key trends in AI/ML, DevOps, and Programming
            - Fetched and summarized full article content
            - Rebuilt learnings book with new insights
            
            ---
            *This PR was automatically created by the TLDR Learning workflow and will be auto-merged.*" \
              --label "automated,learning,copilot" \
              --base main \
              --head "${branch_name}"
            
            echo "âœ… PR created successfully for TLDR learnings"
          fi

      - name: Log learning activity
        run: |
          echo "TLDR learning completed"
          echo "Learnings collected: ${{ steps.fetch.outputs.learning_count }}"
          echo "Trends identified: ${{ steps.fetch.outputs.trend_count }}"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
