name: "Learning: TLDR Tech"

on:
  schedule:
    # Run twice daily - morning and evening
    - cron: '0 8,20 * * *'
  workflow_dispatch:

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  learn-from-tldr:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install beautifulsoup4 requests feedparser lxml html5lib

      - name: Fetch and analyze TLDR content
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          import re
          from datetime import datetime, timezone
          from bs4 import BeautifulSoup
          import os
          import time
          
          print("Fetching TLDR Tech content...")
          
          # Web content fetcher (embedded)
          class WebContentFetcher:
              def __init__(self):
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Mozilla/5.0 (compatible; ChainedAI/1.0)'
                  })
              
              def fetch(self, url):
                  try:
                      response = self.session.get(url, timeout=10, allow_redirects=True)
                      if response.status_code == 200:
                          soup = BeautifulSoup(response.text, 'html.parser')
                          
                          # Remove unwanted elements
                          for element in soup(['script', 'style', 'nav', 'footer', 'iframe']):
                              element.decompose()
                          
                          # Try to find main content
                          content = soup.find('article') or soup.find('main') or soup.find('body')
                          if content:
                              text = content.get_text(separator='\n', strip=True)
                              lines = [line.strip() for line in text.split('\n') if line.strip()]
                              text = '\n'.join(lines)
                              if len(text) > 2000:
                                  text = text[:2000] + '\n\n[Content truncated...]'
                              return text
                  except Exception as e:
                      print(f"  Warning: Could not fetch content from {url}: {e}")
                  return None
          
          fetcher = WebContentFetcher()
          
          # TLDR has an RSS feed we can use
          rss_urls = [
              'https://tldr.tech/api/rss/tech',
              'https://tldr.tech/api/rss/ai',
              'https://tldr.tech/api/rss/devops'
          ]
          
          learnings = []
          tech_trends = []
          
          for rss_url in rss_urls:
              try:
                  response = requests.get(rss_url, timeout=10)
                  if response.status_code == 200:
                      # Parse RSS content
                      from xml.etree import ElementTree as ET
                      root = ET.fromstring(response.content)
                      
                      for item in root.findall('.//item')[:5]:  # Top 5 items
                          title = item.find('title')
                          desc = item.find('description')
                          link = item.find('link')
                          
                          if title is not None and title.text:
                              learning = {
                                  'title': title.text,
                                  'description': desc.text if desc is not None else '',
                                  'source': 'TLDR'
                              }
                              
                              # Try to fetch actual content from link
                              if link is not None and link.text:
                                  learning['url'] = link.text
                                  print(f"  Fetching content from: {link.text[:60]}...")
                                  content = fetcher.fetch(link.text)
                                  if content:
                                      learning['content'] = content
                                      print(f"  ‚úì Fetched content ({len(content)} chars)")
                                  time.sleep(0.5)  # Rate limiting
                              
                              learnings.append(learning)
                              
                              # Extract tech trends
                              title_lower = title.text.lower()
                              if any(word in title_lower for word in ['ai', 'ml', 'llm', 'gpt', 'copilot']):
                                  tech_trends.append(f"AI/ML: {title.text}")
                              elif any(word in title_lower for word in ['github', 'git', 'devops', 'ci/cd']):
                                  tech_trends.append(f"DevOps: {title.text}")
                              elif any(word in title_lower for word in ['python', 'javascript', 'rust', 'go']):
                                  tech_trends.append(f"Programming: {title.text}")
                      
                      print(f"‚úì Fetched from {rss_url}")
              except Exception as e:
                  print(f"‚úó Error fetching {rss_url}: {e}")
          
          # Save learnings
          os.makedirs('learnings', exist_ok=True)
          now = datetime.now(timezone.utc)
          timestamp = now.strftime('%Y%m%d_%H%M%S')
          
          with open(f'learnings/tldr_{timestamp}.json', 'w') as f:
              json.dump({
                  'timestamp': now.isoformat(),
                  'source': 'TLDR Tech',
                  'learnings': learnings,
                  'trends': tech_trends
              }, f, indent=2)
          
          print(f"Saved {len(learnings)} learnings and {len(tech_trends)} trends")
          
          # Output for GitHub Actions
          if learnings:
              first_learning = learnings[0]['title']
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"has_learnings=true\n")
                  f.write(f"learning_count={len(learnings)}\n")
                  f.write(f"trend_count={len(tech_trends)}\n")
                  f.write(f"sample_learning={first_learning}\n")
                  f.write(f"learning_file=learnings/tldr_{timestamp}.json\n")
          else:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"has_learnings=false\n")
          
          PYTHON_SCRIPT

      - name: Create learning issue
        if: steps.fetch.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Create an issue documenting the learnings
          gh issue create \
            --title "üß† Learn from TLDR Tech - $(date +%Y-%m-%d)" \
            --body "## New Tech Insights from TLDR Tech

          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Learnings Collected:** ${{ steps.fetch.outputs.learning_count }}
          **Trends Identified:** ${{ steps.fetch.outputs.trend_count }}

          ### üìö What Was Learned

          **Sample Learning:**
          ${{ steps.fetch.outputs.sample_learning }}

          This learning session collected insights from TLDR Tech newsletters covering:
          - ü§ñ AI/ML developments and LLM advancements
          - üöÄ DevOps practices and CI/CD innovations
          - üíª Programming languages and framework updates
          - üîß Developer tools and productivity enhancements

          ### üîó Learning Resources

          - **Learning File:** [\`${{ steps.fetch.outputs.learning_file }}\`](https://github.com/enufacas/Chained/blob/main/${{ steps.fetch.outputs.learning_file }})
          - **Learnings Directory:** [Browse all learnings](https://github.com/enufacas/Chained/tree/main/learnings)
          - **Learnings Documentation:** [README](https://github.com/enufacas/Chained/blob/main/learnings/README.md)

          ### üìä Key Insights

          The collected learnings include:
          - Technology trends and emerging patterns
          - Best practices from industry leaders
          - Security updates and vulnerability alerts
          - Performance optimization techniques
          - New tool and framework releases

          ### üéØ Actions Taken
          - ‚úÖ Fetched latest tech news from TLDR Tech
          - ‚úÖ Extracted key learnings and trends
          - ‚úÖ Saved to learnings database
          - ‚úÖ Categorized by topic (AI/ML, DevOps, Programming)
          - ‚è≥ Will incorporate into next idea generation cycle

          ### üîÑ Next Steps
          These learnings will influence:
          - Future idea generation and prioritization
          - Technology choices in implementations
          - Best practices adoption
          - Feature development approaches

          ---

          *This learning was automatically collected by the TLDR Learning workflow. The AI continuously learns from external sources to stay current with technology trends.*" \
            --label "learning,automated,copilot" || echo "Issue creation skipped or failed"

      - name: Create PR for learnings
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Rebuild learnings book
          echo "Rebuilding learnings book..."
          python3 tools/build-learnings-book.py
          
          git add learnings/
          
          if git diff --staged --quiet; then
            echo "No new learnings to commit"
          else
            # Create a branch for this learning update
            branch_name="learning/tldr-$(date +%Y%m%d-%H%M%S)"
            git checkout -b "${branch_name}"
            
            git commit -m "üß† Learn from TLDR Tech - $(date -u +%Y-%m-%d)"
            git push origin "${branch_name}"
            
            # Create PR
            gh pr create \
              --title "üß† Learning Update: TLDR Tech - $(date -u +%Y-%m-%d)" \
              --body "## Automated Learning Update from TLDR Tech
            
            **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
            **Learnings Collected:** ${{ steps.fetch.outputs.learning_count }}
            **Trends Identified:** ${{ steps.fetch.outputs.trend_count }}
            
            ### Summary
            This PR adds new learnings collected from TLDR Tech newsletters with full article content.
            
            **Sample Learning:**
            ${{ steps.fetch.outputs.sample_learning }}
            
            ### Changes
            - Updated learnings database with latest tech insights
            - Extracted key trends in AI/ML, DevOps, and Programming
            - Fetched and summarized full article content
            - Rebuilt learnings book with new insights
            
            ---
            *This PR was automatically created by the TLDR Learning workflow and will be auto-merged.*" \
              --label "automated,learning,copilot" \
              --base main \
              --head "${branch_name}"
            
            echo "‚úÖ PR created successfully for TLDR learnings"
          fi

      - name: Log learning activity
        run: |
          echo "TLDR learning completed"
          echo "Learnings collected: ${{ steps.fetch.outputs.learning_count }}"
          echo "Trends identified: ${{ steps.fetch.outputs.trend_count }}"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
