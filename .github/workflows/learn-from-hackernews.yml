name: "Learning: Hacker News"

on:
  schedule:
    # Run three times daily - morning, afternoon, evening
    - cron: '0 7,13,19 * * *'
  workflow_dispatch:

permissions:
  contents: write
  issues: write
  pull-requests: write

jobs:
  learn-from-hn:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 lxml html5lib

      - name: Fetch and analyze Hacker News
        id: fetch
        run: |
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          from datetime import datetime, timezone
          import os
          import re
          import time
          from bs4 import BeautifulSoup
          
          print("Fetching Hacker News top stories...")
          
          # Web content fetcher (embedded)
          class WebContentFetcher:
              def __init__(self):
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Mozilla/5.0 (compatible; ChainedAI/1.0)'
                  })
              
              def fetch(self, url):
                  try:
                      response = self.session.get(url, timeout=10, allow_redirects=True)
                      if response.status_code == 200:
                          soup = BeautifulSoup(response.text, 'html.parser')
                          
                          # Remove unwanted elements
                          for element in soup(['script', 'style', 'nav', 'footer', 'iframe']):
                              element.decompose()
                          
                          # Try to find main content
                          content = soup.find('article') or soup.find('main') or soup.find('body')
                          if content:
                              text = content.get_text(separator='\n', strip=True)
                              lines = [line.strip() for line in text.split('\n') if line.strip()]
                              text = '\n'.join(lines)
                              if len(text) > 2000:
                                  text = text[:2000] + '\n\n[Content truncated...]'
                              return text
                  except Exception as e:
                      print(f"  Warning: Could not fetch content from {url}: {e}")
                  return None
          
          fetcher = WebContentFetcher()
          
          # Hacker News API
          base_url = 'https://hacker-news.firebaseio.com/v0'
          
          learnings = []
          tech_topics = {}
          
          try:
              # Get top stories
              response = requests.get(f'{base_url}/topstories.json', timeout=10)
              top_stories = response.json()[:30]  # Top 30 stories
              
              for story_id in top_stories:
                  try:
                      story_response = requests.get(f'{base_url}/item/{story_id}.json', timeout=5)
                      story = story_response.json()
                      
                      if story and 'title' in story:
                          title = story['title']
                          url = story.get('url', '')
                          score = story.get('score', 0)
                          
                          # Only include high-quality stories (score > 100)
                          if score > 100:
                              learning = {
                                  'title': title,
                                  'url': url,
                                  'score': score,
                                  'source': 'Hacker News'
                              }
                              
                              # Try to fetch actual content
                              if url:
                                  print(f"  Fetching content from: {url[:60]}...")
                                  content = fetcher.fetch(url)
                                  if content:
                                      learning['content'] = content
                                      print(f"  ‚úì Fetched content ({len(content)} chars)")
                                  time.sleep(0.5)  # Rate limiting
                              
                              learnings.append(learning)
                              
                              # Categorize by keywords
                              title_lower = title.lower()
                              
                              keywords = {
                                  'AI/ML': ['ai', 'ml', 'machine learning', 'neural', 'gpt', 'llm', 'copilot'],
                                  'Programming': ['python', 'rust', 'go', 'javascript', 'typescript', 'code'],
                                  'DevOps': ['docker', 'kubernetes', 'ci/cd', 'github', 'actions'],
                                  'Database': ['postgres', 'sql', 'database', 'mongodb', 'redis'],
                                  'Web': ['web', 'browser', 'http', 'api', 'react', 'vue'],
                                  'Security': ['security', 'vulnerability', 'encryption', 'auth'],
                                  'Performance': ['performance', 'optimization', 'speed', 'benchmark'],
                                  'Open Source': ['open source', 'oss', 'github', 'git']
                              }
                              
                              for category, terms in keywords.items():
                                  if any(term in title_lower for term in terms):
                                      if category not in tech_topics:
                                          tech_topics[category] = []
                                      tech_topics[category].append(title)
                                      break
                  except Exception as e:
                      print(f"Error fetching story {story_id}: {e}")
                      continue
              
              print(f"‚úì Fetched {len(learnings)} high-quality stories")
              
          except Exception as e:
              print(f"‚úó Error fetching Hacker News: {e}")
          
          # Save learnings
          os.makedirs('learnings', exist_ok=True)
          now = datetime.now(timezone.utc)
          timestamp = now.strftime('%Y%m%d_%H%M%S')
          
          with open(f'learnings/hn_{timestamp}.json', 'w') as f:
              json.dump({
                  'timestamp': now.isoformat(),
                  'source': 'Hacker News',
                  'learnings': learnings,
                  'topics': tech_topics,
                  'total_stories': len(learnings)
              }, f, indent=2)
          
          print(f"Saved {len(learnings)} learnings across {len(tech_topics)} topics")
          
          # Generate ideas based on trends
          ai_ideas = []
          if 'AI/ML' in tech_topics:
              ai_ideas.append("Implement trending AI technique from Hacker News discussions")
          if 'Programming' in tech_topics:
              ai_ideas.append("Adopt new programming pattern trending on HN")
          if 'Performance' in tech_topics:
              ai_ideas.append("Apply performance optimization inspired by HN")
          
          # Output for GitHub Actions
          if learnings:
              # Sort by score and get the highest scoring story
              top_story = max(learnings, key=lambda x: x['score'])
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"has_learnings=true\n")
                  f.write(f"learning_count={len(learnings)}\n")
                  f.write(f"topic_count={len(tech_topics)}\n")
                  f.write(f"top_story_title={top_story['title']}\n")
                  f.write(f"top_story_score={top_story['score']}\n")
                  f.write(f"idea_count={len(ai_ideas)}\n")
                  f.write(f"learning_file=learnings/hn_{timestamp}.json\n")
          else:
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write(f"has_learnings=false\n")
          
          PYTHON_SCRIPT

      - name: Create learning issue
        if: steps.fetch.outputs.has_learnings == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh issue create \
            --title "üî• Learn from Hacker News - $(date +%Y-%m-%d)" \
            --body "## Hot Topics from Hacker News

          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **High-Quality Stories:** ${{ steps.fetch.outputs.learning_count }}
          **Topics Covered:** ${{ steps.fetch.outputs.topic_count }}
          **New Ideas Generated:** ${{ steps.fetch.outputs.idea_count }}

          ### üèÜ Top Story

          **${{ steps.fetch.outputs.top_story_title }}**
          - ‚¨ÜÔ∏è Community Score: **${{ steps.fetch.outputs.top_story_score }}**

          This highly-upvoted story represents what the tech community finds most valuable right now.

          ### üìö What Was Learned

          This learning session analyzed trending Hacker News discussions (stories with 100+ upvotes) across multiple categories:

          - ü§ñ **AI/ML**: Latest developments in machine learning, LLMs, and AI applications
          - üíª **Programming**: Language features, frameworks, and coding practices
          - üöÄ **DevOps**: CI/CD, Docker, Kubernetes, and infrastructure topics
          - üóÑÔ∏è **Database**: PostgreSQL, SQL, NoSQL, and data management
          - üåê **Web**: Browser technologies, HTTP, APIs, and web frameworks
          - üîí **Security**: Vulnerabilities, encryption, and authentication
          - ‚ö° **Performance**: Optimization techniques and benchmarking
          - üåü **Open Source**: GitHub projects and OSS community insights

          ### üîó Learning Resources

          - **Learning File:** [\`${{ steps.fetch.outputs.learning_file }}\`](https://github.com/enufacas/Chained/blob/main/${{ steps.fetch.outputs.learning_file }})
          - **Learnings Directory:** [Browse all learnings](https://github.com/enufacas/Chained/tree/main/learnings)
          - **Learnings Documentation:** [README](https://github.com/enufacas/Chained/blob/main/learnings/README.md)

          ### üí° Key Insights

          From this learning session:
          - Identified trending technologies and patterns
          - Extracted community wisdom and best practices
          - Discovered emerging tools and frameworks
          - Found discussions on performance and security
          - Generated actionable ideas based on trends

          ### üéØ Actions Taken
          - ‚úÖ Analyzed top 30 trending Hacker News stories
          - ‚úÖ Filtered for high-quality content (100+ upvotes)
          - ‚úÖ Categorized by technical topics
          - ‚úÖ Extracted insights and patterns
          - ‚úÖ Generated ${{ steps.fetch.outputs.idea_count }} new project ideas
          - ‚úÖ Saved to learnings database

          ### üîÑ Impact on Development

          These learnings will influence:
          - üéØ Idea generation priorities and focus areas
          - üõ†Ô∏è Technology adoption decisions
          - üìà Feature implementation approaches
          - üéì Best practices and coding patterns
          - üîç Areas for deeper research and exploration

          ---

          *This learning was automatically collected by the Hacker News Learning workflow. The AI continuously learns from the tech community to stay current with industry trends and discussions.*" \
            --label "learning,automated,copilot" || echo "Issue creation skipped or failed"

      - name: Update learning index
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          from datetime import datetime, timezone
          
          # Create or update learning index
          index_file = 'learnings/index.json'
          
          if os.path.exists(index_file):
              with open(index_file, 'r') as f:
                  index = json.load(f)
          else:
              index = {
                  'total_learnings': 0,
                  'sources': {},
                  'last_updated': None,
                  'top_topics': {}
              }
          
          # Count learning files
          hn_files = [f for f in os.listdir('learnings') if f.startswith('hn_')]
          tldr_files = [f for f in os.listdir('learnings') if f.startswith('tldr_')]
          
          index['total_learnings'] = len(hn_files) + len(tldr_files)
          index['sources']['hacker_news'] = len(hn_files)
          index['sources']['tldr'] = len(tldr_files)
          index['last_updated'] = datetime.now(timezone.utc).isoformat()
          
          with open(index_file, 'w') as f:
              json.dump(index, f, indent=2)
          
          print(f"Updated learning index: {index['total_learnings']} total learnings")
          PYTHON_SCRIPT

      - name: Create PR for learnings
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Rebuild learnings book
          echo "Rebuilding learnings book..."
          python3 tools/build-learnings-book.py
          
          git add learnings/
          
          if git diff --staged --quiet; then
            echo "No new learnings to commit"
          else
            # Create a branch for this learning update
            branch_name="learning/hackernews-$(date +%Y%m%d-%H%M%S)"
            git checkout -b "${branch_name}"
            
            git commit -m "üî• Learn from Hacker News - $(date -u +%Y-%m-%d)"
            git push origin "${branch_name}"
            
            # Create PR
            gh pr create \
              --title "üî• Learning Update: Hacker News - $(date -u +%Y-%m-%d)" \
              --body "## Automated Learning Update from Hacker News
            
            **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
            **Stories Analyzed:** ${{ steps.fetch.outputs.learning_count }}
            **Topics Identified:** ${{ steps.fetch.outputs.topic_count }}
            **New Ideas Generated:** ${{ steps.fetch.outputs.idea_count }}
            
            ### Summary
            This PR adds new learnings collected from Hacker News trending discussions with full article content.
            
            **Sample Learning:**
            ${{ steps.fetch.outputs.sample_learning }}
            
            ### Changes
            - Updated learnings database with community insights
            - Extracted trending topics and patterns
            - Fetched and summarized full article content
            - Identified potential project ideas
            - Rebuilt learnings book with new insights
            
            ---
            *This PR was automatically created by the Hacker News Learning workflow and will be auto-merged.*" \
              --label "automated,learning,copilot" \
              --base main \
              --head "${branch_name}"
            
            echo "‚úÖ PR created successfully for Hacker News learnings"
          fi

      - name: Log learning activity
        run: |
          echo "Hacker News learning completed"
          echo "Stories analyzed: ${{ steps.fetch.outputs.learning_count }}"
          echo "Topics identified: ${{ steps.fetch.outputs.topic_count }}"
          echo "New ideas: ${{ steps.fetch.outputs.idea_count }}"
          echo "Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
