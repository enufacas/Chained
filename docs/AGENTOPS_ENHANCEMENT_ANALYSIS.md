# AgentOps Dashboard Enhancement Analysis

**Author:** @accelerate-master  
**Date:** 2025-11-17  
**Issue:** #[Issue Number]

## Executive Summary

This document analyzes what additional data can be extracted from GitHub Actions workflow runs for the AgentOps dashboard, evaluates the cost implications, and provides actionable recommendations for performance-efficient data extraction.

## Current State

The AgentOps dashboard currently extracts:

| Data Point | Source | API Cost |
|------------|--------|----------|
| Run metadata | Workflow runs endpoint | 1 call per workflow |
| Agent names | Display title + commit msg (regex) | Included above |
| Issue numbers | Run name + commit msg (regex) | Included above |
| PR numbers | pull_requests array + commit lookup | +0-1 calls per run |
| Duration | Calculated from timestamps | Included above |
| Status/Conclusion | Run metadata | Included above |

**Current API Usage:**
- 6 workflows monitored
- 50 runs fetched per workflow per sync
- Sync frequency: Every 2 hours
- **Total: ~72 API calls per day (1.44% of rate limit)**

## Available GitHub Actions API Endpoints

### 1. Workflow Runs API (Currently Used) ‚úÖ

```
GET /repos/{owner}/{repo}/actions/workflows/{workflow_id}/runs
```

**Already provides:**
- ‚úÖ `run_id`, `run_number`
- ‚úÖ `workflow_name`, `workflow_file`
- ‚úÖ `status`, `conclusion`
- ‚úÖ `created_at`, `updated_at`, `run_started_at`
- ‚úÖ `head_branch` ‚≠ê **Already available!**
- ‚úÖ `head_sha`
- ‚úÖ `event` (trigger type)
- ‚úÖ `triggering_actor`
- ‚úÖ `pull_requests` array ‚≠ê **Already available!**

**Key Finding:** Branch names are already in the metadata - no additional API calls needed!

### 2. Jobs API (High Value, Low Cost) ‚≠ê RECOMMENDED

```
GET /repos/{owner}/{repo}/actions/runs/{run_id}/jobs
```

**Provides:**
- List of all jobs in a workflow run
- Job-level status and conclusion
- `started_at`, `completed_at` per job
- **Steps array with:**
  - Step name
  - Step status and conclusion
  - Step number
  - `started_at`, `completed_at` per step

**Benefits:**
- üéØ Step-level timing data (identify slow steps)
- üéØ Failure isolation (which step failed)
- üéØ Performance profiling per workflow
- üéØ No log parsing required - structured JSON

**Cost:**
- 1 API call per run
- For 50 runs √ó 6 workflows = 300 calls per sync
- Daily: ~3,600 calls
- **Rate limit usage: ~7.2% per hour average**
- ‚úÖ **Risk: LOW** - well within the 5,000/hour limit

### 3. Logs API (High Cost) ‚ö†Ô∏è NOT RECOMMENDED FOR REGULAR USE

```
GET /repos/{owner}/{repo}/actions/runs/{run_id}/logs
```

**Returns:** ZIP file with all job logs

**Can extract:**
- Detailed error messages and stack traces
- PR numbers from log output
- Branch names from git commands
- Custom metrics printed by scripts
- Test results and coverage
- Resource usage from verbose output

**Drawbacks:**
- ‚ùå Large file sizes (10KB - 500KB per run)
- ‚ùå Requires ZIP extraction and text parsing
- ‚ùå High bandwidth usage
- ‚ùå Complex parsing logic needed

**Cost:**
- 1 API call per run + download time
- For 50 runs √ó 6 workflows = 300 calls per sync
- Daily: ~3,600 calls + significant bandwidth
- **Rate limit usage: ~7.2% per hour average**
- ‚ö†Ô∏è **Risk: MODERATE** - high quota usage with diminishing returns

### 4. Artifacts API (Moderate Cost)

```
GET /repos/{owner}/{repo}/actions/runs/{run_id}/artifacts
```

**Provides:**
- List of artifacts generated by run
- Artifact names and sizes
- Download URLs

**Cost:**
- 1 API call per run
- Moderate value for artifact tracking

## Cost-Benefit Analysis

### Option A: Add Jobs Data (RECOMMENDED ‚≠ê)

**Implementation:**
1. For each workflow run, call jobs endpoint
2. Extract step-level timing and status
3. Calculate step durations
4. Identify bottleneck steps
5. Store in agentops-runs.json

**Metrics to capture:**
- Total job count per run
- Step count per job
- Duration per step
- Failed step names
- Slowest steps per workflow

**Cost:**
- +300 API calls per sync
- Daily: ~3,600 calls
- Rate limit: 7.2% per hour
- **Status: ‚úÖ ACCEPTABLE**

**Benefits:**
- üöÄ Performance bottleneck identification
- üöÄ Failure isolation at step level
- üöÄ Optimization opportunities
- üöÄ No log parsing overhead

**ROI:** **HIGH** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

### Option B: Add Full Log Downloads (NOT RECOMMENDED ‚ùå)

**Cost:**
- +300 API calls per sync
- +Significant bandwidth (MB of data)
- +Processing time for ZIP extraction
- +Parsing complexity
- Daily: ~3,600 calls
- Rate limit: 7.2% per hour
- **Status: ‚ö†Ô∏è EXPENSIVE**

**Benefits:**
- Detailed error messages
- Custom metrics extraction

**Drawbacks:**
- üí∏ High rate limit usage
- üí∏ Bandwidth costs
- üí∏ Processing overhead
- üí∏ Parsing complexity

**ROI:** **LOW** ‚≠ê

### Option C: Selective Log Downloads (ACCEPTABLE üü°)

**Strategy:** Only download logs for failed runs

**Implementation:**
1. Filter runs where `conclusion == 'failure'`
2. Download logs only for these runs
3. Parse for error patterns

**Cost:**
- Assuming 15% failure rate: ~45 runs per sync
- Daily: ~540 calls
- Rate limit: 1.08% per hour
- **Status: ‚úÖ ACCEPTABLE**

**Benefits:**
- üéØ Targeted failure analysis
- üéØ Reasonable rate limit usage
- üéØ Actionable error insights

**ROI:** **MEDIUM** ‚≠ê‚≠ê‚≠ê

### Option D: On-Demand Deep Analysis (BEST FOR LOGS üåü)

**Strategy:** Add manual trigger for deep log analysis

**Implementation:**
1. Add `workflow_dispatch` input for `run_id`
2. User triggers when investigating specific failure
3. Download and parse logs for that run
4. Generate detailed report
5. No regular scheduled execution

**Cost:**
- User-controlled
- Minimal regular impact
- **Status: ‚úÖ OPTIMAL**

**Benefits:**
- üéØ Zero regular rate limit impact
- üéØ Deep analysis when needed
- üéØ User controls cost
- üéØ Best of both worlds

**ROI:** **HIGH** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

## Recommended Implementation Plan

### Phase 1: Zero-Cost Enhancements (Immediate) üéâ

**Extract existing data that's not currently displayed:**

1. **Branch Names**
   - Already in `head_branch` field
   - Add to dashboard table
   - Add branch filter
   - **Cost: $0** ‚úÖ

2. **PR Information**
   - Already in `pull_requests` array
   - Already being extracted!
   - Just needs better display
   - **Cost: $0** ‚úÖ

3. **Commit SHA Display**
   - Already captured (first 8 chars)
   - Link to commit on GitHub
   - **Cost: $0** ‚úÖ

**Total Phase 1 Cost: $0**  
**Impact: Immediate value from existing data**

### Phase 2: Jobs API Integration (High Value) ‚≠ê

**Add step-level performance data:**

1. **Modify agentops-data-sync.yml:**
   - Add jobs API call for each run
   - Extract step-level data
   - Store in enhanced data structure

2. **New Metrics:**
   ```json
   {
     "runs": [{
       "jobs": [{
         "id": 123,
         "name": "build",
         "duration_seconds": 45.2,
         "steps": [{
           "name": "Checkout",
           "conclusion": "success",
           "duration_seconds": 2.1
         }]
       }]
     }]
   }
   ```

3. **Dashboard Updates:**
   - Add "Step Performance" section
   - Show slowest steps per workflow
   - Display step failure details
   - Performance trend charts

**Cost: +300 API calls per sync (~3,600/day)**  
**Rate Limit: 7.2% per hour**  
**Risk: ‚úÖ LOW**  
**ROI: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê HIGH**

### Phase 3: On-Demand Log Analysis (Optional) üåü

**Add manual deep-dive capability:**

1. **New Workflow:** `agentops-log-analyzer.yml`
   - Input: `run_id` to analyze
   - Downloads logs for that run
   - Parses for:
     - Error messages
     - Agent output
     - Custom metrics
   - Generates detailed report
   - Posts as issue comment

2. **Trigger:**
   - Manual via workflow_dispatch
   - Optionally auto-trigger for critical failures

**Cost: User-controlled (0 regular cost)**  
**ROI: ‚≠ê‚≠ê‚≠ê‚≠ê HIGH (when needed)**

### Phase 4: Selective Failure Log Analysis (Future)

**Implement smart failure analysis:**

1. **Strategy:**
   - Automatically analyze failed runs
   - Extract error patterns
   - Build error knowledge base
   - Suggest fixes

2. **Cost Control:**
   - Analyze only failures (15% of runs)
   - Daily: ~540 API calls
   - Rate limit: 1.08% per hour

**Cost: +540 API calls per day**  
**ROI: ‚≠ê‚≠ê‚≠ê MEDIUM**

## Performance Optimization Strategies

### 1. Caching and Incremental Updates

**Strategy:** Only fetch new runs since last sync

```python
# Store last_run_id per workflow
# Only fetch runs after that ID
params = {
    'per_page': 50,
    'created': f'>={last_sync_time}'
}
```

**Benefit:** Reduces API calls when no new runs

### 2. Batch Processing

**Strategy:** Process multiple runs in parallel

```python
# Use concurrent requests with rate limiting
from concurrent.futures import ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=5) as executor:
    results = executor.map(fetch_jobs, run_ids)
```

**Benefit:** Faster sync times

### 3. Smart Sampling

**Strategy:** Full details for recent runs, summaries for older

```python
# Last 10 runs: full job details
# Runs 11-50: summary only
# Runs 50+: aggregated stats
```

**Benefit:** Balanced detail vs. cost

### 4. Error Pattern Caching

**Strategy:** Cache known error patterns, only fetch logs for new errors

```python
# Build error signature from:
# - Failed step name
# - Conclusion reason
# - Only download logs if signature is new
```

**Benefit:** Reduces log downloads significantly

## Implementation Checklist

### Phase 1: Zero-Cost Enhancements ‚úÖ

- [ ] Update agentops-data-sync.yml to expose head_branch
- [ ] Update agentops.html to display branch names
- [ ] Add branch filter dropdown
- [ ] Improve PR number display with better formatting
- [ ] Add commit SHA links
- [ ] Test on production data
- [ ] Update documentation

**Estimated Time:** 2-3 hours  
**API Cost:** $0  
**Risk:** None

### Phase 2: Jobs API Integration ‚≠ê

- [ ] Add jobs API call to sync workflow
- [ ] Design enhanced data structure
- [ ] Extract step-level timing
- [ ] Calculate step durations
- [ ] Identify failed steps
- [ ] Store job and step data
- [ ] Update dashboard with step performance section
- [ ] Add step-level filtering
- [ ] Create performance visualizations
- [ ] Test with various workflow types
- [ ] Monitor API rate limit usage
- [ ] Update documentation

**Estimated Time:** 4-6 hours  
**API Cost:** +3,600 calls/day  
**Risk:** Low

### Phase 3: On-Demand Analysis (Optional)

- [ ] Create agentops-log-analyzer.yml workflow
- [ ] Implement log download and extraction
- [ ] Add error pattern parsing
- [ ] Generate detailed report format
- [ ] Add workflow_dispatch inputs
- [ ] Test with failed runs
- [ ] Document usage
- [ ] Add to dashboard as "Analyze Run" button

**Estimated Time:** 3-4 hours  
**API Cost:** User-controlled  
**Risk:** None

## Rate Limit Protection

### Current Protections
- ‚úÖ Scheduled sync (not on-demand)
- ‚úÖ Cached data (static JSON)
- ‚úÖ Batch requests
- ‚úÖ Limited history (100 runs)

### Additional Safeguards for Enhanced Version

1. **Rate Limit Monitoring:**
   ```python
   # Check rate limit before each batch
   remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
   if remaining < 100:
       print("‚ö†Ô∏è  Approaching rate limit, pausing...")
       time.sleep(60)
   ```

2. **Exponential Backoff:**
   ```python
   # Retry with backoff on 429 errors
   @retry(wait=exponential(multiplier=1, min=2, max=60))
   def api_call():
       # API request
   ```

3. **Circuit Breaker:**
   ```python
   # Stop sync if rate limit critical
   if remaining < 50:
       raise RateLimitError("Stopping to preserve rate limit")
   ```

4. **Configurable Limits:**
   ```yaml
   inputs:
     max_runs_per_workflow:
       description: 'Limit runs to fetch'
       default: 50
       type: number
     enable_jobs_fetch:
       description: 'Fetch job details'
       default: true
       type: boolean
   ```

## Monitoring and Alerting

### Metrics to Track

1. **API Usage:**
   - Calls per sync
   - Rate limit remaining
   - Rate limit reset time
   - Failed API calls

2. **Data Quality:**
   - Runs with missing data
   - Jobs fetch success rate
   - Parse error rate

3. **Performance:**
   - Sync duration
   - Data file size
   - Dashboard load time

### Recommended Alerts

- ‚ö†Ô∏è  Rate limit below 1,000
- ‚ö†Ô∏è  Sync failure 3 times in a row
- ‚ö†Ô∏è  Sync duration > 5 minutes
- ‚ö†Ô∏è  Data file size > 5MB

## Conclusion

**@accelerate-master** recommends a phased approach:

1. **Phase 1 (Immediate):** Display existing data like branch names - **zero cost, immediate value**

2. **Phase 2 (High Priority):** Add jobs API integration - **manageable cost, high performance insights**

3. **Phase 3 (Optional):** On-demand log analysis - **user-controlled cost, deep debugging capability**

**Total Impact:**
- API calls: +3,600/day (well within limits)
- Rate limit: ~7.2% per hour (safe)
- New insights: Step-level performance, failure isolation, optimization opportunities
- ROI: High value for reasonable cost

The recommended implementation strikes an optimal balance between performance insights and API efficiency, following @accelerate-master's core principle: **maximum performance with minimal overhead**.

---

*Analysis completed by **@accelerate-master** - Thoughtful, deliberate, and direct.*
