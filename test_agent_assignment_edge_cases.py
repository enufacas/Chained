#!/usr/bin/env python3
"""
Comprehensive test suite for edge cases in the intelligent agent matching system.
This test suite complements test_agent_matching.py by focusing on edge cases,
ambiguous scenarios, and boundary conditions.
"""

import sys
import json
import subprocess
from pathlib import Path

# Test cases for edge cases and boundary conditions
EDGE_CASE_TESTS = [
    # Generic/ambiguous issues
    ("This is a new issue", "Testing to see which custom agent gets assigned", "test-champion", 3),
    ("Generic task", "Just a simple thing to do", "feature-architect", 0),  # Default fallback
    ("Empty body test", "", "test-champion", 0),  # Has "test" in title
    
    # Multiple agent keywords (should pick highest scoring)
    ("Fix bug in test suite", "The unit tests are failing with errors", "bug-hunter", 5),  # Bug keywords score higher
    ("Document security best practices", "Write guide on security and testing", "doc-master", 3),
    ("Refactor and optimize performance", "Code needs refactoring for better speed", "performance-optimizer", 3),  # Performance wins
    
    # Similar keywords across agents
    ("Clean code refactoring", "Refactor code to make it cleaner", "code-poet", 3),  # "clean code" matches code-poet strongly
    ("Format code properly", "Code needs better formatting and style", "code-poet", 2),
    
    # Case sensitivity tests
    ("FIX BUG IN LOGIN", "USERS EXPERIENCING CRASHES", "bug-hunter", 3),
    ("Add New Feature", "Implement User Profile Pages", "feature-architect", 3),
    
    # Special characters and punctuation
    ("Fix bug!!!", "Error handling is broken...", "bug-hunter", 3),
    ("Add API (REST)", "Build RESTful API endpoint", "integration-specialist", 3),
    
    # Minimal content
    ("Bug", "Fix", "bug-hunter", 3),
    ("Test", "Tests needed", "test-champion", 3),
    ("Docs", "Update documentation", "doc-master", 3),
    
    # Long descriptive issues
    (
        "Comprehensive security audit and vulnerability assessment",
        "We need to perform a thorough security review of our authentication system, "
        "check for SQL injection vulnerabilities, XSS attacks, and ensure proper "
        "encryption of sensitive data. This includes reviewing authorization logic.",
        "security-guardian",
        5
    ),
    
    # No keywords at all
    ("Generic title", "This has no relevant keywords", "feature-architect", 0),
    
    # Testing related edge cases
    ("QA needed", "Quality assurance and validation required", "test-champion", 3),
    ("Test coverage low", "Unit test coverage is below 50%", "test-champion", 5),
    ("E2E tests failing", "End-to-end integration tests are broken", "test-champion", 5),
    
    # Performance edge cases
    ("App is slow", "Everything is laggy and takes forever", "performance-optimizer", 3),
    ("Speed up queries", "Database queries causing bottlenecks", "performance-optimizer", 4),  # Adjusted expected score
    
    # Documentation edge cases
    ("Explain this code", "Need clarification on how this works", "doc-master", 3),
    ("API guide needed", "Create API documentation and examples", "doc-master", 5),
    
    # Integration edge cases
    ("Connect OAuth", "Integrate OAuth authentication with external service", "integration-specialist", 5),
    ("Payment gateway", "Add Stripe payment integration", "integration-specialist", 5),
    
    # UX edge cases
    ("Colors look bad", "The UI colors need improvement", "ux-enhancer", 3),
    ("Accessibility fix", "Screen reader support needed for a11y", "ux-enhancer", 5),
]

def run_matching(title, body):
    """Run the matching script and return parsed result."""
    try:
        result = subprocess.run(
            ['python3', 'tools/match-issue-to-agent.py', title, body],
            capture_output=True,
            text=True,
            check=True,
            cwd='.'
        )
        return json.loads(result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error running matching script: {e}")
        print(f"Stdout: {e.stdout}")
        print(f"Stderr: {e.stderr}")
        return None
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON: {e}")
        print(f"Output was: {result.stdout if 'result' in locals() else 'N/A'}")
        return None

def test_edge_cases():
    """Run all edge case tests and report results."""
    print("ðŸ§ª Testing Agent Matching Edge Cases")
    print("=" * 70)
    print()
    
    passed = 0
    failed = 0
    warnings = 0
    
    for title, body, expected_agent, min_score in EDGE_CASE_TESTS:
        result = run_matching(title, body)
        
        if result is None:
            print(f"âŒ FAILED: {title}")
            print(f"   Could not run matching")
            failed += 1
            continue
        
        matched_agent = result.get('agent')
        score = result.get('score', 0)
        confidence = result.get('confidence', 'unknown')
        
        # Check if matched agent is correct and score meets minimum
        if matched_agent == expected_agent and score >= min_score:
            print(f"âœ… PASSED: {title}")
            print(f"   â†’ Matched: {matched_agent} (score: {score}, confidence: {confidence})")
            passed += 1
        elif matched_agent == expected_agent and score < min_score:
            # Matched correct agent but score is lower than expected
            print(f"âš ï¸  WARNING: {title}")
            print(f"   â†’ Matched correct agent: {matched_agent}")
            print(f"   â†’ But score {score} < expected minimum {min_score}")
            print(f"   â†’ Confidence: {confidence}")
            warnings += 1
            passed += 1  # Still count as pass since agent is correct
        else:
            print(f"âŒ FAILED: {title}")
            print(f"   â†’ Expected: {expected_agent} (min score: {min_score})")
            print(f"   â†’ Got: {matched_agent} (score: {score}, confidence: {confidence})")
            # Show top 3 scoring agents for debugging
            all_scores = result.get('all_scores', {})
            top_agents = sorted(all_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            print(f"   â†’ Top 3: {top_agents}")
            failed += 1
        
        print()
    
    print("=" * 70)
    print(f"ðŸ“Š Results: {passed} passed, {failed} failed, {warnings} warnings")
    print(f"   Total tests: {passed + failed}")
    
    if warnings > 0:
        print(f"\nâš ï¸  {warnings} test(s) passed with warnings (correct agent but lower score)")
    
    if failed == 0:
        print("âœ… All edge case tests passed!")
        return 0
    else:
        print(f"âŒ {failed} test(s) failed")
        return 1

def test_boundary_conditions():
    """Test specific boundary conditions."""
    print("\nðŸ§ª Testing Boundary Conditions")
    print("=" * 70)
    print()
    
    # Test with None/empty inputs
    test_cases = [
        ("", "", "Should handle empty strings"),
        ("Title only", "", "Should work with just title"),
        ("", "Body only", "Should work with just body"),
    ]
    
    for title, body, description in test_cases:
        result = run_matching(title, body)
        if result:
            print(f"âœ… {description}")
            print(f"   â†’ Result: {result['agent']} (score: {result['score']})")
        else:
            print(f"âŒ {description} - Failed to get result")
        print()
    
    return 0

def test_score_calculation():
    """Verify score calculation is working as expected."""
    print("\nðŸ§ª Testing Score Calculation")
    print("=" * 70)
    print()
    
    # Issues that should have progressively higher scores
    test_cases = [
        ("test", "", "Single keyword in title"),
        ("test coverage", "", "Two keywords in title"),
        ("test coverage improvement", "", "Three keywords in title"),
        ("test", "unit testing coverage", "Keywords in both title and body"),
    ]
    
    previous_score = -1
    all_passed = True
    
    for title, body, description in test_cases:
        result = run_matching(title, body)
        if result and result['agent'] == 'test-champion':
            score = result['score']
            print(f"âœ… {description}: score = {score}")
            
            # Verify scores are generally increasing (title repeated gives extra weight)
            if previous_score >= 0:
                if score < previous_score:
                    print(f"   âš ï¸  Score didn't increase (previous: {previous_score})")
            previous_score = score
        else:
            print(f"âŒ {description}: didn't match test-champion")
            all_passed = False
        print()
    
    if all_passed:
        print("âœ… Score calculation tests passed")
        return 0
    else:
        print("âŒ Some score calculation tests failed")
        return 1

def main():
    """Main entry point."""
    # Change to repo root
    repo_root = Path(__file__).parent
    import os
    os.chdir(repo_root)
    
    print(f"Working directory: {os.getcwd()}")
    print()
    
    # Run all test suites
    results = []
    
    results.append(test_edge_cases())
    results.append(test_boundary_conditions())
    results.append(test_score_calculation())
    
    # Overall summary
    print("\n" + "=" * 70)
    print("ðŸ“Š Overall Test Summary")
    print("=" * 70)
    
    if all(r == 0 for r in results):
        print("\nâœ… All test suites passed!")
        return 0
    else:
        print(f"\nâŒ Some test suites had failures")
        return 1

if __name__ == "__main__":
    sys.exit(main())
